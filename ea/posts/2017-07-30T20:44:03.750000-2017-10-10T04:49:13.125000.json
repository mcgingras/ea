[{"_id": "jYT6c8ByLfDpYtwE9", "title": "Why Charities Usually Don't Differ Astronomically in Expected Cost-Effectiveness", "postedAt": "2017-09-16T10:40:56.701Z", "htmlBody": "<p><em>Written by Brian Tomasik. Published by the EA Forum team as a <a href=\"https://forum.effectivealtruism.org/tag/classic-repost\">classic repost</a>.</em></p>\n<h2>Summary</h2>\n<p>I think some in the effective-altruism movement overestimate the extent to which charities differ in their expected marginal cost-effectiveness. This piece suggests a few reasons why we shouldn't expect most charities to differ by more than hundreds of times. In fact, I suspect many charities differ by at most ~10 to ~100 times, and within a given field, the multipliers are probably less than a factor of ~5. These multipliers may be positive or negative, i.e., some charities have negative expected net impact. I'm not claiming that charities don't differ significantly, nor that we shouldn't spend substantial resources finding good charities; indeed, I think both of those statements are true. However, I do hope to challenge black/white distinctions about effective/ineffective charities, so that effective altruists will see greater value in what many other people are doing in numerous places throughout society.</p>\n<h2>Introduction</h2>\n<blockquote>\n<p><em>\"It is very easy to overestimate the importance of our own achievements in comparison with what we owe others.\"</em></p>\n</blockquote>\n<blockquote>\n<p>\u2014 <a href=\"http://www.brainyquote.com/quotes/quotes/d/dietrichbo387413.html\">attributed</a> to Dietrich Bonhoeffer</p>\n</blockquote>\n<p>Sometimes the effective-altruist movement claims that \"charities differ in cost-effectiveness by thousands of times.\" While this may partly be a marketing pitch, it also seems to reflect some underlying genuine sentiments. Occasionally there are even claims to the effect that \"shaping the far future is 10^30 times more important than working on present-day issues,\" based on a naive comparison of the number of lives that exist now to the number that might exist in the future.</p>\n<p>I think charities do differ a lot in expected effectiveness. Some might be 5, 10, maybe even 100 times more valuable than others. Some are negative in value by similar amounts. But when we start getting into claimed differences of thousands of times, especially within a given charitable cause area, I become more skeptical. And differences of 10^30 are almost impossible, because everything we do now may affect the whole far future and therefore has nontrivial expected impact on vast numbers of lives.</p>\n<p>It would require razor-thin exactness to keep the expected impact on the future of one set of actions 10^30 times lower than the expected impact of some other set of actions.</p>\n<p>I'll elaborate a few weak arguments why our expectations for charity cost-effectiveness should not diverge by many orders of magnitude. I don't claim any one of these is a fully general argument. But I do see a macro-level trend that, regardless of which particular case you look at, you can come up with reasons why a given charity isn't more than, say, 10 or 100 times better than many other charities. Among charities in a similar field, I would expect the differences to be even lower\u2014generally not more than a factor of 10.</p>\n<p>Note that these are arguments about ex ante expected value, not necessarily actual impact. For example, if charities bought lottery tickets, there would indeed be a million-fold difference in effectiveness between the winners and losers, but this is not something we can figure out ahead of time. Similarly, there may be butterfly-effect situations where just a small difference in inputs produces an enormous difference in outputs, such as in the proverb \"For Want of a Nail\". But such situations are extremely rare and are often impossible to predict ex ante.</p>\n<p>By \"expected value\" I mean \"the expected value estimate that someone would arrive at after many years of studying the charity and its impacts\" rather than any old expected value that a novice might have. While it's sometimes sensible to estimate expected values of 0 for something that you know little about on grounds of a principle of indifference, it's exceedingly unlikely that an expected value will remain exactly 0 after studying the issue for a long time, because there will always be lots of considerations that break symmetry. For example, if you imagine building a spreadsheet to estimate the expected value of a charity as a sum of a bunch of considerations, then any single consideration whose impact is not completely symmetric around 0 will give rise to a nonzero total expected value, except in the extremely unlikely event that two or more different considerations exactly cancel.</p>\n<p>I'm also focusing on the marginal impact of your donating additional dollars, relative to the counterfactual in which you personally don't donate those dollars, which is what's relevant for your decisions. There are of course some crucially important undertakings (say, keeping the lights on in the White House) that might indeed be thousands of times more valuable per dollar than most giving opportunities we actually face on the margin, but these are already being paid for or would otherwise be funded by others without our intervention.</p>\n<p>Of course, there are some extreme exceptions. A charity that secretly burns your money is not competitively effective with almost anything else. Actually, if news of this charity got out, it might cause harm to philanthropy at large and therefore would be vastly negative in value. Maybe donating to painting galleries or crowd-funding altruistically neutral films might be comparable in impact to not giving at all. And even money spent on different luxuries for yourself may have appreciably different effects that are within a few orders of magnitude of the effect of a highly effective charity donation.</p>\n<p>What I'm talking about in this piece are charities that most people reasonably consider to be potentially important for the world\u2014say, the best 50% of all registered 501(c)(3) organizations. Of course, there are also many uses of money that do substantial good that do not involve donating to nonprofits.</p>\n<p>Finally, note that this discussion assumes valuation within a particular moral framework, presumably a not-too-weird total consequentialism of some sort. Comparing across moral frameworks requires an exchange rate or other harmonization procedure, which is not my concern here.</p>\n<p>With these qualifications out of the way, I'll dive into a few specific arguments.</p>\n<h2>Arguments for the claim</h2>\n<h3>Argument 1: Many types of flow-through effects</h3>\n<p>By \"<a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>\" I mean indirect effects of a charity's work. These could include the spill-over implications on the far future of the project in which the charity engages, as well as other side effects of the charity's operations that weren't being directly optimized for.</p>\n<p>Charities have multiple flow through effects. For example, with an international-health organization, some relevant flow-through impacts include</p>\n<ul>\n<li>\n<p>lives saved</p>\n</li>\n<li>\n<p>contribution to health technology</p>\n</li>\n<li>\n<p>contribution to transparency, measurement, and research in the field</p>\n</li>\n<li>\n<p>inspiring developed-world donors of the importance of global health, thereby increasing donations and compassion in the long run</p>\n</li>\n<li>\n<p>writing newsletters, a website, and editorials in various magazines</p>\n</li>\n<li>\n<p>providing training for staff of the organization so that they can be empowered to do other things in the future<sup class=\"footnote-ref\"><a href=\"#fn-kJCAa8CnPfg8AmfcB-1\" id=\"fnref-kJCAa8CnPfg8AmfcB-1\">[1]</a></sup></p>\n</li>\n<li>\n<p>etc.</p>\n</li>\n</ul>\n<p>For papers written about dung beetles, effects include</p>\n<ul>\n<li>more knowledge of dung beetles</li>\n<li>better understanding of ecology and evolution in general, with some possible spillover applications to game theory, economics, etc.</li>\n<li>better understanding of behavior, neuroscience, and nature's algorithms</li>\n<li>development of scientific methodology (statistics, data sampling, software, communication tools, etc.) with applications to other fields</li>\n<li>training for young scientists</li>\n</ul>\n<p>etc.</p>\n<p>Even if a charity is, say, 10 times better along one of these dimensions, it's unlikely to be 10 times better along all of them at once. As a result, a charity that seems 10 times as cost-effective on its main metric will generally be less than 10 times as effective all things considered.</p>\n<p>Of course, probably some charities are better along all dimensions at once, just as some people may be smarter across all major domains of intelligence than others, but it's unlikely that the degree of outstandingness is similarly high across all dimensions.</p>\n<p>To illustrate, suppose we're comparing charity A and charity B. In terms of the most salient impact of their work, charity A has a cost-effectiveness per dollar of 1000, while charity B's cost-effectiveness per dollar is only 1. So it looks like charity A is 1000 times more cost-effective than charity B. However, along another dimension of altruistic impact, suppose that charity A is 10, while charity B is 0.5. On a third dimension, charity A is 40, and charity B is 6.5. On a fourth dimension, charity A is -20, and charity B is -1. All told, the ratio of charity A to charity B is (1000 + 10 + 40 - 20) / (1 + 0.5 + 6.5 - 1) = 147.</p>\n<p>This point would not be relevant if only one or maybe two of the flow-through effects completely dominates all the others. That seems unlikely to me, though it's more plausible for lives saved than for dung-beetle papers, since for dung-beetle papers, most of the flow-through effects come from the indirect contributions toward scientific training, methodology, and transferable insights. (Of course, these effects are not necessarily positive. Faster science <a href=\"http://utilitarian-essays.com/differential-intellectual-progress.html\">might</a> pose more of a risk than a benefit to the future.)</p>\n<p>In any event, my claim is only that charities are unlikely to differ by many orders of magnitude; they might still differ by one or two, and of course, they may also have net negative impacts. Even for health charities, I think it's understood that they don't differ by more than a few orders of magnitude, and indeed, if you pick a random developing-world health charity, it's unlikely to be even one or two orders of magnitude worse than a GiveWell-recommended health charity, even just on the one dimension that you're focusing on.</p>\n<p>Health charities between the developed world and the developing world may differ by more than 100 times in direct impact as measured by lives saved or DALYs averted, but the ultimate impacts will differ less than this, because (a) developed-world people contribute more to world GDP on average and (b) the memes of developed-world society have more impact than those in the developing world, inasmuch as the developed world exports a lot of culture, and artificial general intelligence will probably be shaped mostly by developed-world individuals. Note that I'm personally <a href=\"http://utilitarian-essays.com/differential-intellectual-progress.html#economic-growth\">uncertain</a> of the sign of GDP growth on the far future, and the sign of saving developed-world lives on society's culture isn't completely obvious either. But whether positive or negative, these impacts are more positive or negative per person in the developed world. That said, I still think it's better to support developing-world charities, in part because this helps expand people's moral circle of compassion. I'm just pointing out that the gap is not as stark as it may seem when examining only direct effects on individual lives in the short run.</p>\n<h3>Argument 2: Replaceability of donations</h3>\n<p>There's another reason we should expect the cost-effectiveness of charities to even itself out over time. If a charity is really impressive, then unless it's underfunded due to having weird values or otherwise failing to properly make itself known, it should either already have funding from others or should be in a position to get funding from others without your help. Just as the question in career choice is not \"Where can I make the biggest impact?\" but \"Where can I make the biggest impact that wouldn't have happened without me?\" so also in philanthropy, it's relevant whether other people would have donated to the funding gap that you're intending to fill. The more effective the charity is, the more likely your donation would have been eventually replaced.</p>\n<p>I think people's effectiveness can vary quite significantly. Some academics are hundreds of times more important than others in their impact on a field. Some startup founders appear to have <a href=\"http://80000hours.org/blog/23-entrepreneurship-a-game-of-poker-not-roulette\">vastly greater skill</a> than average. But what I'm discussing in this piece is the value of marginal dollars. A brilliant startup founder should have no trouble attracting VC money, nor should an outstanding academic have trouble securing grants. Differences in outstandingness tend to be accompanied by differences in financial support. The value of additional money to a brilliant person is probably much lower, because otherwise someone would have given it already.</p>\n<p><strong>Engineering vs. market efficiency</strong></p>\n<p>Naively one might think of cost-effective charity like a problem of engineering efficiency: Charity A is ten times more effective than charity B, so I should fund charity A and get a 10X multiplier on my impact. This may sometimes be true, especially when choosing a cause to focus on. However, to some extent, the process of charity selection more resembles how banks and hedge funds search for a company that's temporarily undervalued so as to make some profits before its price returns to equilibrium. If a highly successful charity is underfunded, we shouldn't necessarily expect it to stay that way forever, even without our help. Of course, charity markets aren't nearly as <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">efficient</a> as financial markets, so the proper characterization of searching for cost-effective charities is some mix between the permanent engineering free lunch and the competitive search for a vanishing moment of under-valuation.</p>\n<p>The engineering free lunch is more likely to exist in cases of divergent values. For instance, some people may have an emotional connection to an art gallery or religious cathedral and insist on donating to that rather than, say, research on world peace. Unlike in the case of financial markets, efficiency will not necessarily close this gap, because there's not a common unit of \"good\" shared by all donors in the way that there is a common unit of value in financial markets. That said, if there are some cross-cause donors who pick charities based on room for more funding, then donation levels may become more insensitive to cause-specific donors, as Carl Shulman explains in \"<a href=\"http://reflectivedisequilibrium.blogspot.com/2014/01/its-harder-to-favor-specific-cause-in.html\">It's harder to favor a specific cause in more efficient charitable markets.</a>\"</p>\n<p><strong>Returns look high before big players enter</strong></p>\n<p>Consider the cause of figuring out technical details for how to control advanced artificial general intelligence (AGI). In 2014, I'd guess this work has maybe ~$1 million in funding per year, give or take. Because very few people are exploring the question, the leverage of such research appears extremely high. So it's tempting to say that the value of supporting this research is immense. But consider that if the problem is real, it seems likely that the idea will accumulate momentum and would have done so to some degree even in the absence of the current efforts. Already in 2014 there seems to be an increasing trend of mainstream academics and public figures taking AGI control more seriously. In the long run, it's plausible that AGI safety will become a recognized subfield of computer science and public discourse. If that happens, there will be much more investigation into these topics, which means that work done now will comprise a tiny portion of the total. (I originally wrote this paragraph in mid-2014. As of late 2017, this prediction seems to have largely come true!)</p>\n<p>Consider the following graph for the returns from investment in the cause of AGI safety. Right now, because so little is known, the returns appear huge: This is the early, steep part of the curve. But if significant funding and expertise converge on the issue later, then the marginal value of the work now is much smaller, since it only augments what would have happened anyway.</p>\n<p><img src=\"https://reducing-suffering.org/wp-content/uploads/2014/12/value-if-project-is-eventually-funded.png\" alt=\"\"></p>\n<p>Paul Christiano <a href=\"http://www.effective-altruism.com/ea/62/paul_christiano_on_cause_prioritization/\">makes a similar point</a> in the language of rates of return:</p>\n<blockquote>\n<p><em>A mistake I sometimes see people make is using the initial rates of return on an investment to judge its urgency. But those returns last for a brief period before spreading out into the broader world, so you should really think of the investment as giving you a fixed multiplier on your dollar before spreading out and having a long-term returns that go like growth rates.</em></p>\n</blockquote>\n<p>See also my \"<a href=\"https://reducing-suffering.org/the-haste-consideration-revisited/\">The Haste Consideration, Revisited</a>\". Because multiplication is commutative, in oversimplified models, it shouldn't matter when the periods of high growth rates come. For instance, suppose the typical growth rate is 3%, but when the movement hits a special period, growth is 30%. 1.3 * 1.3 * 1.03 * 1.03 = 1.03 * 1.03 * 1.3 * 1.3. Of course, in practice, the set of growth rates will vary based on the ordering of events.</p>\n<p>The picture I painted here assumed the topic would go mainstream at some point. But arguably the most important reason to work on AGI safety now is to set the stage so that the topic can go mainstream later. Indeed, a lot of the attention the topic has received of late has been due to efforts of the \"early adopters\" of the cause. Making sure there will be big funding later on seems arguably the most important consequence of early work. This consideration argues why early work is indeed highly valuable, even though the value is mainly indirect.</p>\n<h3>Argument 3: Cross-fertilization</h3>\n<p>Thought communities are connected. Ideas like effective altruism that persuade people on one dimension\u2014say, the urgency of global poverty\u2014also tend to persuade people on other dimensions\u2014such as the importance of shaping the far future to avoid allowing <a href=\"http://www.utilitarian-essays.com/astronomical-suffering.html\">vast amounts</a> of suffering to occur. Slippery slopes are not really logical fallacies but are often genuine and important. Encouraging people to take the <a href=\"https://en.wikipedia.org/wiki/Foot-in-the-door_technique\">first step</a> toward caring about animals by not eating cows and chickens might later lead them to see the awfulness of predation in the wild, and they might eventually also begin to take the possibility of insect suffering seriously. Just as no man is an island, no idea or movement is either.</p>\n<p>Suggesting that one charity is astronomically more important than another assumes a model in which cross-pollination effects are negligible. But as quantum computer scientists know, it's extremely hard to keep a state from entangling with its environment. Likewise with social networks: If you bring people in to cause W, some fraction will trickle from there to causes X, Y, and Z that are connected to W. Even if you think W is useless by itself, if 5% of the people you bring to cause W move to cause X, then adding new people to cause W should be about 5% as good as adding them to cause X. Of course, people may flow the other way, from X back to W, and maybe promoting cause W too successfully would drag people away from X. Regardless, it's unlikely all these effects will exactly cancel, and in the end, working on cause W should be at least an appreciable fraction of 5% as important (in either a positive or negative way) as X itself, even ignoring any direct effects of W. (See also \"<a href=\"https://reducing-suffering.org/why-charities-dont-differ-astronomically-in-cost-effectiveness/#distraction\">Appendix: Concerns about distracting people</a>.\")</p>\n<p>As an example, consider a high-school club devoted to international peace. You might say those students won't accomplish very much directly, and that's true. They won't produce research papers or host a peace-building conference. But they will inspire their members and possibly their fellow students to become more interested in the cause, and perhaps some of them will go on to pursue careers in this field, drawing some inspiration from that formative experience. Of course, some high-school clubs will have negative impacts in the same way, by drawing people's attention from more important to less important or actively harmful pursuits. But regardless of the sign, the magnitude of impact per dollar should not differ hundreds of times from that of, e.g., a professional institute for peace studies.</p>\n<p><strong>Example: Veg outreach vs. welfare reforms</strong></p>\n<p>Cross-fertilization also matters for the projects worked on as well as the people working on them. Suppose you believe that the dominating impact of animal charity is encouraging people to <a href=\"http://www.utilitarian-essays.com/caring-about-animal-suffering.html\">care about animal suffering</a> in general, with implications for the trajectory of the future. Naively you might then think that veg outreach is vastly more important than, say, animal-welfare reforms, because veg outreach is spreading concern, while welfare reforms are just helping present-day animals out of the sight of most people.</p>\n<p>But this isn't true. For one thing, legislative reforms\u2014especially ballot initiatives like <a href=\"https://en.wikipedia.org/wiki/California_Proposition_2_(2008)\">Proposition 2</a>\u2014involve widespread outreach, and many people learn about animal cruelty via such petitions and letters to Congress. Moreover, laws are a useful memetic tool, because activists can say things like, \"If dog owners treated their dogs the way hog farmers treat their pigs, the dog owners would be guilty of violating animal-cruelty laws,\" and so on. Given that beliefs often follow actions rather than preceding them, having a society where animals are implicitly valued in legislation or industry welfare standards can make a subtle difference to vast numbers of people. Now, this isn't to claim that laws are necessarily on the same order of magnitude for antispeciesism advocacy as direct veg outreach, but they might be\u2014indeed, it's possible they're better. Of course, veg outreach can also inspire people to work on laws, showing that the cross-pollination is bidirectional. The broader point is, until much more analysis is done, I think we should regard the two as being roughly in the same ballpark of effectiveness, especially when we consider model uncertainty in our assessments.</p>\n<p>And as mentioned earlier, there's crossover of people too. By doing welfare advocacy, you might bring in new activists who then move to veg outreach. Of course, the reverse might happen as well: Activists start out in veg outreach and move to welfare. (I personally started out with a focus on veg outreach but may incline more toward welfare because I think animal welfarism encourages much more rational thinking about animals and is potentially more likely to recognize the importance of averting wild-animal suffering. In contrast, many veg*ans <a href=\"http://www.utilitarian-essays.com/animal-rights-wilderness-preservation.html\">ideologically support wilderness</a>.)</p>\n<p><strong>Fungibility of good</strong></p>\n<p>Holden Karnofsky <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">coined</a> the term \"fungibility of good\" to describe the situation in which solving some problems opens up resources to focus on other problems. For instance, maybe by addressing the worst abuses of lab animals, more people are then able to focus on the welfare of (far more numerous) farm animals. So even if you think helping lab animals is negligibly valuable compared with helping farm animals in terms of direct impact, helping lab animals may still conduce to other people focusing on farm animals.</p>\n<p>That said, the reverse dynamic is also possible: The more people focus on lab animals, the more attention that cause gets, and the more further funding it can attract\u2014stealing funding from farm-animal work. Therefore, this point is only tentative.</p>\n<h3>Argument 4: Your idea is probably easy to invent or misguided</h3>\n<p>Suppose you find a charity that you think has an astoundingly important idea. If this charity isn't funded, it may dwindle and die, and the idea may not gain the audience that it deserves. Isn't funding this charity then overwhelmingly cost-effective?</p>\n<p>The charity does sound promising, yes, but here are two possibilities that rein in expectations:</p>\n<p>**Epistemic modesty. **If the idea hasn't been proposed before, maybe there's a good reason for that? Maybe it's a reason you haven't thought of yet? If something seems too good to be true, perhaps your analysis is flawed\u2014if not completely, then at least enough to downgrade the seeming astronomical importance. When we conclude that one cause is astronomically more important than another, we typically do so within the framework of a model or perhaps a few models. But we should also account for model uncertainty.</p>\n<p><strong>Rare breakthroughs are rare.</strong> Some new ideas are relatively easy to invent, and others are hard. If you invent what you think is a new idea, chances are it was an easy idea, because it's much more rare for people to invent hard ideas. The fact that you thought of it makes it more likely that someone else would also have thought of it. Thus, the value of your particular discovery is probably lower than it might seem. Likewise with an organization to promote the idea: Even if your organization fails, perhaps someone else will re-invent the idea later and spread it then. When an idea or cause will be invented multiple times, our impact comes from just inventing it sooner or at more critical periods in humanity's development. It's less probable we'd be the only ones to ever invent it.</p>\n<p>These mediocritarian considerations are weakened if you have a track record of genius. Albert Einstein or Srinivasa Ramanujan\u2014after seeing that they had made several groundbreaking discoveries\u2014should rightly have assessed themselves as worth hundreds of ordinary scientists. Minor geniuses can also rightly assess themselves as having more altruistic potential than an average person.</p>\n<p>In general, though, people in other communities than our own are not dumb. Often traditional norms and ways of doing business have wisdom, perhaps implicit or only tribally known, that's hard to see from the outside. In most cases where someone thinks he can do vastly better than others by applying some new technique, that person is mistaken. Of course, we can try, just like some startup founders can shoot to become the next Facebook. But that doesn't mean we're likely to succeed, and our estimates should reflect that fact.</p>\n<h3>Argument 5: Logical action correlations</h3>\n<p>Typically claims for the astronomical dominance of some charities over others rely on a vision of the universe in which the colonization of our supercluster depends upon the trajectory of Earth-originating intelligence. It would seem that the only astronomical impacts of our actions would come through the \"funnel\" of influencing the nature of Earth-based superintelligence and space colonization, and actions not affecting this funnel don't matter in relative terms.</p>\n<p>As I've pointed out already, any action we take does affect this funnel in some way or other via flow-through effects, cross-fertilization, etc. But another factor deserves consideration as well: Given that some algorithms and choices are shared between our brains and those of other people, including large post-human populations, our choices <a href=\"https://en.wikipedia.org/wiki/Superrationality\">have logical implications</a> for the choices of other people, including at least a subset of post-humans to a greater or lesser degree.</p>\n<p>Following is an unrealistic example only for purposes of illustration. Suppose you're deciding between two actions:</p>\n<p>work to shape the funnel of the takeoff of Earth-originating intelligence in order to reduce expected future suffering by 10^-20 percent</p>\n<p>help prevent suffering in the short term, which itself has negligible impact on total suffering, but which logically implies a tiny difference in the decision algorithms of all ~10^40 future people, implying a total decrease in suffering of 10^-19 percent.</p>\n<p>Here, the short-term action that appeared astronomically less valuable when only looking at short-term impact (ignoring flow-through effects and such) ended up being more valuable because of its logical implications about decision algorithms in general.</p>\n<p>As I said, this example was just to clarify the idea. It's not obvious that reducing short-term suffering actually does imply better logical consequences for the actions of others; maybe it implies worse consequences. In general, the project of computing the logical implications of various choices seems to be a horrendously complicated and not very well studied. But by adding yet more variance to estimation of the cost-effectiveness of different choices, this consideration reduces further the plausibility that a random pair of charities differs astronomically.</p>\n<h2>Fundraising and meta-charities</h2>\n<p>In addition to donating to a charity directly, another option is to support fundraisers or meta-charities that aim to funnel more donations toward the best regular charities.</p>\n<p>Fundraising can sometimes be quite effective. For instance, every ~<a href=\"http://www.affinityresources.com/pgs/articles/fundraising_costs.html\">$0.20</a> spent on it might yield $1 of donations. However, even taken at face value, this is only a 5X multiplier on the value of dollars spent, and the figure shouldn't be taken at face value. Fundraising only works if the rest of the organization is doing enough good work to motivate people to donate. The publicity that a charity generates by its campaigns is an important component of attracting funds and maintaining existing donors, and that publicity requires direct, in-the-trenches work on the charity's programs. So it's not as though you can just continue adding to the charity's fundraising budget while keeping everything else constant and maintain the 5X return on investment. Marginal fundraising dollars will be less and less effective, and at some point, fundraising might cause net harm when it starts to spam donors too much.</p>\n<p>Organizations like Giving What We Can (GWWC) share similarities with fundraising but also differ. GWWC is independent of the charities to which it encourages donations, which may reduce donor backlash against excessive fundraising. It also encourages more total giving, which may create donations that wouldn't have happened anyway. In contrast, fundraising for charity X might largely steal donations from charities Y and Z (though how much this is true would require research). GWWC <a href=\"https://www.givingwhatwecan.org/impact#realistic\">estimates</a> that based on pledged future donations and consideration of counterfactuals, it will generate ~60 times as many dollars to top charities as it spends on its own operations. I tinkered with GWWC's calculation to reduce the optimism in some of the estimates, and I got an ending figure of about ~20 times rather than ~60. This is still reasonably impressive. However:</p>\n<ol>\n<li>The calculation uses <a href=\"http://blog.givewell.org/2014/06/10/sequence-thinking-vs-cluster-thinking/\">sequence thinking</a> and should be reined in based on priors against such high multipliers.</li>\n<li>My impression is that GWWC has been one of the more successful charities, so this figure has a bit of selection bias; other efforts that might have looked similar to GWWC's ex ante probably didn't take off as well. There are plenty examples of business startups that surpass most people's expectations for growth by hundreds of times, but these are the exceptions rather than the rules. The ex ante expected value of a given startup is high but not astronomically high.</li>\n<li>Because these figures are pretty impressive, and because GWWC has so many supporters, I expect GWWC will be able to convince a reasonable number of donors to keep funding it, which means your particular donations to GWWC might not make a big difference in a counterfactual sense as to whether GWWC's work continues.</li>\n</ol>\n<p>Beyond the specific arguments that I put forward, part of my skepticism just comes from claims of astronomical cost-effectiveness \"not passing the sniff test\" given how competent lots of non-GWWC charity workers are and how easy it is to come up with a charity like GWWC. (Tithing and giving pledges are nothing new.) If GWWC really has stumbled on an amazingly effective new approach, eventually that approach should be replicated until marginal returns diminish, unless one thinks that GWWC's discovery will languish because outsiders irrationally won't see how amazing it is.</p>\n<p>Charities that claim extraordinary cost-effectiveness based on unusual axiological assumptions, such as consideration of astronomical outcomes in the far future or the neglected <a href=\"https://reducing-suffering.org/the-importance-of-insect-suffering/\">importance of insect suffering</a>, seem slightly more plausible, since many fewer people are devoting their lives to such issues. In contrast, probably thousands of smart people outside the effective-altruism community are already promoting and raising money for effective developing-world charities.</p>\n<p>In general, I think it's plausible that meta-charities with sound business models will tend to have more impact\u2014maybe several times more impact\u2014than the regular charities they aim to support, but I remain skeptical that a good meta-charity for a cause is more than ~5 times more cost-effective, in terms of marginal ex ante expectation relative to the counterfactual where you personally don't donate, than a good regular charity for that cause once all factors are considered.</p>\n<h2>The distribution of effectiveness</h2>\n<p>When talking about the impact of a charity or other use of money, we need a reference point: impact relative to what? Let's say the reference point is that you burn the money that you would have donated (in a hypothetical world where doing so was not illegal). This of course has some effect in its own right, because it <a href=\"https://en.wikipedia.org/wiki/Money_burning#Macroeconomic_effect\">causes deflation</a> and makes everyone else's money worth slightly more. However, we have to set a zero point somewhere.</p>\n<h3>All spending has impact</h3>\n<p>Now, relative to this reference point, almost everything you spend money on will have a positive or negative effect. It's hard to avoid having an impact on other things even if you try. Spending money is like pushing on how society apportions its resources. The world has labor and capital that can be applied to one task or another (charity work, building mansions, playing video games, etc.), and any way you spend money amounts to pushing more labor/capital toward certain uses and away from others. Money doesn't create productive capacity; it just moves it. And when we move resources in one region, \"<a href=\"https://en.wikiquote.org/wiki/John_Muir\">we find it hitched to everything else</a>\" in the economy. I picture spending money like pulling up a piece of dough in one spot: It increases that spot and also tugs (either positively or negatively) on other dough, most strongly the other dough in its vicinity.</p>\n<p>Even different non-charitable uses of money may have appreciably different impacts. Buying an expensive watch leads people to spend more time on watch design, construction, and marketing. Buying a novel leads to slightly more fiction writing. Buying a cup of coffee leads to more coffee cultivation and more Starbucks workers. The recipients of your money may go on to further spend it on different kinds of things, and those effects may be relevant as well.</p>\n<p>Buying a self-published philosophy book for your own personal enjoyment isn't typically seen as charity, but paying someone to write about philosophy is presumably more helpful to society than paying someone to spend 400 hours carefully crafting the $150,000 <a href=\"http://www.therichest.com/luxury/most-expensive/the-10-most-expensive-watches-available/\">Parmigiani Kalpa XL Tourbillon watch</a>, only to serve the purpose of conspicuous consumption in a status arms race.</p>\n<p>In general, there's no hard distinction between charity vs. other types of consumption and investment. There's just spending money and encouraging some activities more than others, with society classifying some of those activities as \"charity.\" The ethical-consumerism movement recognizes the impact of dollars outside of charitable spending. For instance, vegetarianism is premised on the idea that buying meat leads to more animal farming. (There's an ethical-investment movement too, though it's less clear how much impact this can have in an efficient capital market.)</p>\n<p>One way to state the argument in this piece is as follows. Suppose you spend $5 million on a seemingly worthless charity. That's $5 million of spending that occurs within the economy, and such spending must have side effects (both good and bad) on other things that happen in the economy. It would seem to be a miracle if a whole $5 million of economic activity had a net impact on the altruistic things you care about smaller than, e.g., donating just $1 to the most cost-effective charity you can think of.</p>\n<h3>Case study: Homeopaths Without Borders</h3>\n<p>What about <a href=\"http://homeopathswithoutborders-na.org/\">Homeopaths Without Borders</a> (HWB)? Aren't they a textbook case of a charity with literally zero impact? Well, the medicine they provide does have no mechanistic effect on patient health, but that's not the end of the story. Consider some additional impacts of HWB:</p>\n<ul>\n<li>Providing homeopathic treatment itself. [May have either negative or positive consequences. For instance, if it leads people not to seek other treatment, it's harmful. If it has helpful placebo effect and makes people feel cared for, it's potentially positive. Maybe HWB also brings additional non-medical services?]</li>\n<li>Allowing developed-world people to experience developing-world conditions, potentially leading to actually useful altruism of other sorts later. [Somewhat positive.]</li>\n<li>Setting up offices and homeopathic schools in poor communities. [Sign isn't clear.]</li>\n<li>Promoting the idea of homeopathy, both in the countries where HWB works and in countries where they attract donors. [Moderately bad.]</li>\n<li>Drawing away well-meaning volunteers who could be doing actually helpful work for poor people if they hadn't been caught by HWB's marketing materials. [Very bad.]</li>\n<li>Donation drives encourage well-meaning people to donate to HWB instead of legitimate international-health charities. [Very bad.]</li>\n</ul>\n<p>Consider just the issue of drawing away donors from legitimate groups. Suppose that 5% of people from whom HWB raises money would have given to a legitimate international-health charity like Oxfam or UNICEF if they hadn't given to HWB. Also assume that HWB spends marginal donations on fundraising at the same rate as it spends its whole budget on fundraising. Then donating D dollars to HWB is, on this dimension alone, 5% as bad as donating D dollars to Oxfam or UNICEF is good because HWB steals 5% of D dollars from those kinds of charities.<sup class=\"footnote-ref\"><a href=\"#fn-kJCAa8CnPfg8AmfcB-2\" id=\"fnref-kJCAa8CnPfg8AmfcB-2\">[2]</a></sup> If marginal HWB donations go relatively less to fundraising, or if marginal fundraising is less effective per dollar, then the harm drops to less than 5%, but probably not that much less. And when you combine in all the other impacts that HWB has, the negative consequences could be greater.</p>\n<p>That said, it's possible the cross-fertilization effect goes the other way. Maybe HWB attracts primarily homeopathy disciples and encourages them to take interest in international health. Then, if 5% of them eventually learn that homeopathy is medically useless, they might remain in the field of international health and do more productive work. In this model, HWB would have a positive effect of channeling counterfactual homeopathy people and dollars toward programs that have spillover effects of creating a few serious future international-health professionals.</p>\n<p>So theoretically the effect could go either way, and we'd need more empirical research to determine that. However, I find it extremely unlikely that these effects are all going to magically cancel out. There are just too many factors at play. And while opposing considerations may cancel out to some extent, more considerations still means higher variance. For example, suppose HWB's total expected effect is a sum of N independent components C1 to CN. Based on past experience, but before doing any research on HWB itself, we approximate these components as coming from a distribution with mean zero and standard deviation \u03c3. Then a priori, \u03a3i Ci has mean zero and standard deviation \u03c3 * sqrt(N). In other words, the cumulative effect tends to be farther from zero than any individual effect, although not N times farther.</p>\n<h3>A sample cost-effectiveness distribution</h3>\n<p>We might think of the distribution of impact of various uses of money as is shown in the following figure, where colored dots represent a way to spend money, and collectively they illustrate the shape of the frequency distribution for uses of money that we actually observe being done. The vertical location of a dot doesn't mean anything; the dots just vary on the vertical dimension to give them space so that they won't be crowded and so that they collectively trace out a bell-shaped curve.</p>\n<p><img src=\"https://reducing-suffering.org/wp-content/uploads/2014/09/distribution.jpg\" alt=\"\"></p>\n<p>I don't claim to know exactly the shape or spread of the curve, nor would I necessarily stand by my example charity assessments if I thought about the issues more. This is just an off-the-cuff illustration of what I have in mind.</p>\n<p>Burning money should be the weighted center of the distribution of presently undertaken monetary expenditures, because deflation helps all other uses of money in proportion to their volume. Also note that it's not always true that charities are within a few hundreds of times the expected effectiveness of one another. A rare few lie in the tails, at least for a brief period, just like a few rare investments really can dramatically outperform the market temporarily. Or, if we compare some typical charity X against some charity Y that happens to fall exceedingly close to the zero point, the ratio of cost-effectiveness for X to Y will indeed be large. But this is an exception and it's not what I mean when I say that most charities don't differ by more than tens to hundreds of times.</p>\n<h2>Effectiveness entropy</h2>\n<p>Some of the above arguments boil down to pointing out that the variance of our cost-effectiveness estimates tend to increase as we consider more factors and possibilities. For instance, consider the argument about flow-through effects. Prior to incorporating flow-through effects, we might compute</p>\n<p><em>effectiveness of charity = years of direct animal suffering prevented per dollar,</em></p>\n<p>or, to abbreviate:</p>\n<p><em>charity = direct_suff_prev.</em></p>\n<p>This estimate has some variance:</p>\n<p><em>Var(charity) = Var(direct_suff_prev).</em></p>\n<p>Now suppose we consider N flow-through effects of the charity (FTE1, ..., FTEN), all of which happen to be probabilistically independent of each other. Then</p>\n<p><em>charity = direct_suff_prev + FTE1 + ... + FTEN.</em></p>\n<p>Since the effects are independent, the variance of the sum <a href=\"https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)\">is the sum of the variances</a>:</p>\n<p><em>Var(charity) = Var(direct_suff_prev) + Var(FTE1) + ... + Var(FTEN).</em></p>\n<p>In other words, the variance in our effectiveness estimate increases dramatically as more considerations are incorporated. Other arguments discussed above, like cross-fertilization and consideration of action correlations, also add complexity and hence variance to our estimates.</p>\n<p>This is relevant because as the variance of our subjective probability distribution for the net impact of each charity grows, the (absolute value of the) ratio of the actual cost-effectiveness values of the two charities will probably shrink:</p>\n<p><img src=\"https://reducing-suffering.org/wp-content/uploads/2015/01/effectiveness-entropy.png\" alt=\"\"></p>\n<p>(Why am I talking about actual cost-effectiveness values if, as the title of this piece says, my argument refers to expected cost-effectiveness? See the next subsection.)</p>\n<p>This phenomenon is similar to the way in which those who think a policy question is obvious and purely one-sided tend to know least about the topic. The more you learn, the more you can see subtle arguments on different sides that you hadn't acknowledged before.</p>\n<p>We can think of this as \"entropy\" in cost-effectiveness. In a similar way as it's rare in a chemical solution to have ions of type A cluster together in a clump separated from ions of type B, it's rare to see charities whose efforts have so little uncertainty that we can be confident that, upon further analysis, the effectiveness estimates will remain very far apart. Rather, the more uncertainty we have, the more the charities tend to blur together, like different types of ions in a solution. (The analogy is not perfect but only illustrative. Diffusion should almost perfectly mix ions in a solution, while real-world charities do retain differences in expected cost-effectiveness.)</p>\n<h3>Objection: Expected value is all that matters</h3>\n<p>In my diagram above, I kept the expected values of charities A and B the same, only increasing the variance of our subjective probability distributions for their effectiveness. But if we're forced to make a choice now, we should <a href=\"https://reducing-suffering.org/why-maximize-expected-value/\">maximize expected value</a>, regardless of variance. So don't the expected values still differ astronomically?</p>\n<p>Yes, but variance is important. For one thing, the absolute ratio of actual effectiveness is probably lower than the difference in expected values. Of course, you can never observe the actual effectiveness, since this requires omniscience. But you can reduce the variance in your estimates by studying the charities further and refining your analyses of them. Studying further leads to wiggling the expected-value estimates around a bit and reducing the variances of the charities' probability distributions. This is like a less pronounced version of drawing a single point from the distributions. In particular, charities that started out at almost zero in expected value will tend to move left or right somewhat, which significantly reduces the absolute value of effectiveness ratios.</p>\n<p>If you're forced to make an irrevocable tradeoff between charities now, and if you've done your Bayesian calculations perfectly, then yes: trade off the charities based on expected values without concern for variance. But if you or others will have time to study the issues further and refine the effectiveness estimates, you can predict that the absolute ratio of expected values will probably go down rather than up on further investigation.</p>\n<p>At this point, you might be wondering: If we expect the absolute effectiveness ratio to decrease with further investigation, why not get ahead of the curve and reduce our tradeoff ratio between the charities ahead of time? The following example illustrates the answer:</p>\n<p><em><strong>Example:</strong> Suppose charity X has expected value of literally 0. Charity Y has expected value of 1000. If you had to donate money right now, you'd favor Y over X to an infinite degree. If instead you waited and learned more, X would almost certainly move away from zero in expected value, but if its effectiveness probability distribution is symmetric, it's just as likely to turn out to be negative as to be positive, so donating to X is just as likely to cause harm as benefit. This is why you shouldn't jump the gun and be more willing to donate to X ahead of time.</em></p>\n<p>If charities could only have positive effects, then an increase in variance would automatically imply an increase in expected value. But in that case, if both charities had large variances, the expected values themselves wouldn't have differed astronomically to begin with.</p>\n<p>As a footnote, I should add that there are some cases where increased variance probably increases the absolute ratio of effectiveness. For instance, if two charities start out with the same expected values (or if one charity has the negative of another's expected value), then further investigation has to accentuate the multiplicative ratio between the charities (since the absolute value of the ratio of the charity with the bigger-magnitude expected value to the charity with the smaller-magnitude expected value can't be less than 1).</p>\n<h2>Why does this question matter?</h2>\n<p>Maybe it's true that charities don't necessarily differ drastically in expected cost-effectiveness. Why is this relevant? We should still pick the best charities when donating, regardless of how much worse the other options are, right?</p>\n<p>Yes, that's right, but here are a few of many examples showing why this discussion is important.</p>\n<ol>\n<li><strong>Not appearing naive.</strong> If we talk about differences among charities that are thousands of times, sophisticated observers may consider us young and foolish at best, or arrogant and deceptive at worst.</li>\n<li><strong>Competing for donors</strong>. Suppose that for the same amount of effort, you can either (A) raise $500 from totally new donors who wouldn't have otherwise donated or (B) raise $600 from existing donors to so-called less effective charities. If you really think your charity is vastly better, you'll choose option (B). If you recognize that the charities are closer in effectiveness than they seem, option (A) appears more prudent. Of course, one could also make game-theoretic or rule-utilitarian arguments not to steal other people's donors, although some level of competition can be healthy to keep charities honest.</li>\n<li><strong>Movement building vs. direct value creation.</strong> A generalization of the point about competing for donors is that when we recognize the value of charity work outside our sphere of involvement, the value of spreading effective altruism is slightly downshifted compared with the value of doing useful altruism work directly, because spreading effective altruism sometimes takes away resources from existing so-called \"non-effective\" altruist projects. The downshift in emphasis on movement building may be small. For one thing, many new effective altruists may not have done any altruism before and thus don't have as much opportunity cost. In addition, even if differences in effectiveness are only, say, 3 times, between what the activists are now doing versus what they had been doing, that's still 2/3 as good per activist as if they had previously been doing nothing.</li>\n<li><strong>Hesitant donors.</strong> A mainstream donor approaches you asking for a suggestion on where to give. Because she comes from a relatively conventional background, she'd be weirded out by the charity that you personally think is most effective. If you believe your favorite charity really is vastly better than alternatives, you might take a gamble and suggest it anyway, knowing she'll probably reject it. If you think other, more normal-seeming alternatives are not dramatically worse, you might recommend them instead, with a higher chance they'll be chosen.</li>\n<li><strong>Your relative strengths.</strong> Suppose you're passionate about field A and have unusual talents in that domain. However, a naive cost-effectiveness estimate says that field B is vastly more important than field A. You resign yourself to grudgingly working in field B instead. But if your estimate of the relative difference is more refined, you'll see that A and B are closer than they seem, and combined with your productivity multiplier for working in field A, it may end up being better for you to pursue field A after all. Holden Karnofsky made a similar point in the end of an <a href=\"http://80000hours.org/blog/172-interview-with-holden-karnofsky-co-founder-of-givewell\">interview</a> with 80,000 Hours. (Of course, it's also <a href=\"http://80000hours.org/blog/63-do-what-you-re-passionate-about-part-2\">important not to assume</a> prematurely that you know what field you'd enjoy most.)</li>\n<li><strong>General insight.</strong> Having a more accurate picture of how various activities produce value is broadly important. One could similarly ask, Why should I read a proof of this theorem if I already know the theorem is true? It's because reading the proof enhances your understanding of the material as a whole and makes you better able to draw connections and produce further contributions.</li>\n</ol>\n<h2>Acknowledgments</h2>\n<p>This piece was inspired by a <a href=\"http://blog.jessriedel.com/2013/12/09/impact-discrepancies-persist-under-uncertainty/\">blog post</a> from Jess Riedel. I clarified the discussion further in response to <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/617894611600233/?comment_id=618581201531574&amp;offset=0&amp;total_comments=10\">feedback</a> from Ben Kuhn and Ben West, and a <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/623457984377229/\">later round</a> from Darren McKee, Ben Kuhn, and several others.</p>\n<h2>See also</h2>\n<p>An <a href=\"http://measuringshadowsblog.blogspot.com/2015/08/multiplicative-factors-in-games-and.html\">argument</a> for apportioning resources somewhat evenly among causes.</p>\n<h2>Appendix: Concerns about distracting people</h2>\n<p>Suppose you think cause X is vastly more intrinsically important than cause W. Some fraction of people working on X might stray and work on W if you talk too much about cause W. This might lead you to be nervous about distracting X people with W. On the other hand, by being involved with W, you can bring W people toward cause X. Thus, the flow can go both ways, and being involved with W isn't obviously a bad idea on balance.</p>\n<p>One way to attempt to deal with this is to socialize more with the W supporters, talking openly about X with them, while not mentioning W when socializing with the X supporters. However, sometimes the bidirectionality is harder to evade. One example is this website: Because I discuss many issues at once, some supporters of the more important topics will get distracted by less important ones, while some supporters of the less important topics will through this site find the more important ones. For web traffic, this may be basically unavoidable. For directed traffic, I suppose I could aim to promote my site more among supporters of the less important causes.</p>\n<p>Ideally one would hope that the more important cause would be \"stickier\" in terms of better retaining the people who had found it, while members of the less important cause would be more likely to gravitate to the more important cause if they knew about it. In practice I'm not sure this is the case. I think a lot of how people choose causes is based on what other people are doing rather than a pure calculation. To a large extent this makes sense, because what others believe is important evidence for what we should believe, but this can also lead to information cascades.</p>\n<p>There are also more complicated possibilities. For example, maybe there are supporters of cause V whom your website convinces to support cause W, which tends to be a stepping stone on the way to cause X. Then if you get V readers who move to W, and not a lot of W readers who move back to V, this could be a good tradeoff. And so on.</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-kJCAa8CnPfg8AmfcB-1\" class=\"footnote-item\"><p>Of course, other jobs where people would work would also provide training, so training by any particular charity may not be a counterfactual benefit. Or it might be if the counterfactual jobs would be in, say, shoe shining. In general, a flow-through analysis should consider replaceability questions. If people weren't working for the health charity or writing beetle papers, maybe they would be doing something with even higher impact, so funding health charities or beetle papers pulls them away from better things. Of course, it might also pull them away from worse things. Either way, the effect on labor pools is an important consideration. If you find a charity with really high marginal impact because its employees are so effectiveness-minded, then chances are they would also be doing something really effective if they weren't working for this organization, so the impact of your dollars may be less than it seems. <a href=\"#fnref-kJCAa8CnPfg8AmfcB-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-kJCAa8CnPfg8AmfcB-2\" class=\"footnote-item\"><p>In case this isn't clear, here's the reasoning in more detail. Say HWB's annual budget is B dollars. Some fraction f of it is spent on fundraising, and suppose that any additional donation is also devoted to fundraising in the proportion f. Each year, f * B fundraising dollars yield another budget of size B, so the multiplier of \"dollars raised\" to \"dollars spent on fundraising\" is 1/f, assuming unrealistically that each fundraising dollar contributes the same increase in donations. Suppose you donate D dollars to HWB. f * D dollars go to fundraising, and this yields f * D * (1/f) = D dollars to HWB from its future fundraising efforts. Because 5% of these D dollars raised would have gone to charities like Oxfam, your D-dollar donation is taking away 0.05 * D dollars from charities like Oxfam. In practice, the amount taken from Oxfam-like charities may be lower if marginal fundraising dollars don't have constant fundraising returns or if marginal donated dollars go to fundraising with a proportion less than f.\nAlso keep in mind that the division between \"fundraising dollars\" and \"program dollars\" is somewhat artificial. In practice, all of a charity's activities contribute toward fundraising because the more it does, the more impressive it looks, the more media mentions and website references it gets, and so forth. This suggests that even if HWB explicitly spent no marginal dollars on fundraising-categorized expenses, it would still expand its donations just by being able to have more bodies doing more work. <a href=\"#fnref-kJCAa8CnPfg8AmfcB-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Brian_Tomasik"}}, {"_id": "Q3Y4kMCoofMypsiFt", "title": "New Report on the Welfare Differences Between Cage and Cage-Free Housing", "postedAt": "2017-09-14T10:28:26.637Z", "htmlBody": "<p>Over the last two years, animal welfare organizations <a href=\"http://www.openphilanthropy.org/blog/why-are-us-corporate-cage-free-campaigns-succeeding\">successfully secured</a> pledges from major restaurants and grocers to eliminate battery cages from their supply chains, which are collectively expected to bring cage-free housing from ~13% of the domestic egg supply to ~70% when fully implemented. We have been the largest funder of these campaigns.</p><p>In our blog post announcing our support for these campaigns, we claimed that <a href=\"http://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms#Battery_cages_cause_severe_suffering_and_cage-free_systems_are_much_better\">cage-free systems were much better than battery cages</a> for hen welfare, based on initial research conducted by <a href=\"http://www.openphilanthropy.org/about/team/lewis-bollard\">Lewis Bollard</a>, our program officer for farm animal welfare. Lewis briefly argued against a <a href=\"https://docs.google.com/document/d/1mEoaEm9xRuVkX3nWggRPz700RU7_Z_QgRuEtQMWiMbM/edit\">memorandum</a> by animal rights group <a href=\"http://www.directactioneverywhere.com/\">Direct Action Everywhere</a> (DxE) which claimed the opposite. This disagreement was further explored in a series of <a href=\"http://www.openphilanthropy.org/blog/initial-grants-support-corporate-cage-free-reforms#comment-40\">comments</a> written by Lewis and Wayne Hsiung of DxE.</p><p>We left that discussion unsatisfied with our knowledge about the evidence on hen welfare in different housing systems, and I (Ajeya Cotra) conducted a more thorough investigation \u2013 the full report can be found <a href=\"http://www.openphilanthropy.org/how-will-hen-welfare-be-impacted-transition-cage-free-housing\">here</a>. My findings were, in short:</p><ul><li>Cage-free housing will provide hens with increased ability to perform behaviors like stretching and walking, laying eggs in nestboxes, and perching at night. Our conversations with animal welfare scientists suggested that these behaviors are important to hen welfare, and in the handful of specific experiments we reviewed, hens were willing to pay substantial-seeming costs for the opportunity to perform these behaviors. However, our initial look at these experiments was limited in depth and we aren\u2019t confident about their quality.</li><li>It seems likely that there will be a \u201ctransition cost\u201d of increased mortality when U.S. farms initially transition into cage-free housing. Researchers told us that these systems are more difficult to maintain well, and our understanding is that mortality from disease and injury will likely be abnormally high while farmers learn to manage the new systems. Other things being equal, farmers have substantial incentive to reduce high mortality rates, because mortality reduces the cost-efficiency of egg production. However, we are unsure how far this effect will push, or how long that process may take.</li><li>Post-transition, mortality rates will likely continue to be more variable in cage-free systems than in battery cages. Our best guess is that mortality in aviaries will be roughly similar to cages in the long run, but this read of the evidence depends on particular judgment calls described in more detail <a href=\"http://www.openphilanthropy.org/how-will-hen-welfare-be-impacted-transition-cage-free-housing#Pooling_and_analysis_of_the_two_datasets\">here</a>. In particular, mortality in aviaries would look significantly worse if we had chosen to give extra weight to a short-term U.S. experimental study <a href=\"https://academic.oup.com/ps/article/94/3/485/1523002/Impact-of-commercial-housing-systems-and-nutrient\">Karcher et al 2014</a> (which we expect to be less representative of long-run outcomes than <a href=\"https://www.aphis.usda.gov/animal_health/nahms/poultry/downloads/layers2013/Layers2013_dr_PartI.pdf\">this 2013 survey of U.S. farms</a> and a <a href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0146394\">this meta-analysis from the U.K.</a>, which gathered data from cage-free systems that had been in place for a longer period of time).</li><li>The literature doesn\u2019t seem to offer any conclusive method to morally weigh changes in behavioral prospects and changes in disease or injury risk in a unified quantitative framework. Animal welfare scientists from the U.K. and Canada expressed the intuition that cage-free housing was substantially better for hen welfare all things considered, while scientists from the U.S. tended to believe furnished (but not battery) cages were best for animal welfare. Our impression is that compared with U.K. and Canadian scientists, U.S. animal welfare scientists tend to emphasize \u201cwelfare\u201d criteria that align with production metrics over hen subjective well-being, and are more often funded by industry interests.</li></ul><p>In light of this new investigation, we believe we were overconfident in our unqualified initial statement that \u201cCage-free systems\u2026 [are] much better than cages.\u201d In particular, we had put substantial weight on a pre-existing <a href=\"http://www.sciencedirect.com/science/article/pii/S1573521406800199\">quantitative assessment</a> that ranked hen housing systems on a 0-10 welfare scale; we are now much more skeptical of that paper\u2019s methodology. Additionally, we now think it is likely that mortality rates in aviaries will be higher than in cages in the years immediately after farmers transition to cage-free production, though we expect this initially-high mortality to decline significantly as farmers follow profit incentives to improve management practices and reduce mortality to roughly the levels currently seen in commercial cage-free farms in the US and the UK. We continue to believe our grants to accelerate the adoption of cage-free systems were net-beneficial for layer hens, but we feel we made a mistake by not conducting a more thorough review of the research on this topic earlier.</p><p>In addition, it seems clear to us that cage-free systems have much higher welfare <i>potential</i> than battery cage systems \u2013 that is, the theoretical highest-welfare hen housing system would not contain cages. Though we stand by our bottom line, we appreciate the pushback from Direct Action Everywhere for prompting us to look more closely at the evidence base.</p><p>For more details, read the full report <a href=\"http://www.openphilanthropy.org/how-will-hen-welfare-be-impacted-transition-cage-free-housing\">here.</a></p>", "user": {"username": "Ajeya"}}, {"_id": "DmF8rPaiWt3FpRqJz", "title": "Claire Zabel: Biosecurity as an EA cause area", "postedAt": "2017-08-11T15:31:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=7OzaTw7rGXE\"><div><iframe src=\"https://www.youtube.com/embed/7OzaTw7rGXE\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this 2017 talk, the Open Philanthropy Project's </i><a href=\"https://www.openphilanthropy.org/about/team/claire-zabel\"><i>Claire Zabel</i></a><i> talks about their work to mitigate Global Catastrophic Biological Risks. She also discusses what effective altruists can do to help, as well as differences between biological risks and risks from advanced AI. At the very end you will find an added section on what you can do to&nbsp;help.</i></p><p><i>The below transcript is lightly edited for&nbsp;readability.</i></p><h1>The Talk</h1><p>Today I'm going to talk to you about biosecurity as a cause area and how the Open Philanthropy Project is thinking about it. I think that this is a cause area that EAs have known about for a while, but haven't dug into as deeply as some of the other things that have been talked about at this event - like AI and farm animal welfare and global poverty - but I think it's important. I think it's an area where EAs have a chance to make a huge difference, especially EAs with a slightly different set of skills and interests than those required by some of the other cause areas. I would love to see more EAs engaging with&nbsp;biosecurity.</p><p>When I say biosecurity, I want to make sure we're clear on the problem that I'm talking about. I'm focusing on what we're calling Global Catastrophic Biological Risks at the Open Philanthropy Project. I'm going to talk to you about how we see that risk and where we see that risk - where we think it might be coming from. I'm going to talk to you about how I think EAs can make a difference in it. Then I want to note that I'm not really focusing too much on the specific work that we've done and that others have done. I thought it would be more interesting for people to get a sense of what this area is like and the strategic landscape as we see it before getting into the details of specific organizations and people, so hopefully that's helpful for&nbsp;everyone.</p><p>I also want to note quickly that I think this is an area where a lot less thinking has been done for a much shorter period of time, so to a greater extent everything should be viewed as somewhat preliminary and uncertain. We might be changing our minds in the near&nbsp;future.</p><p>The cause for concern when we think about global catastrophic biological risks is something that could threaten the long term flourishing of human civilization, that could impair our ability to have a really long, really big future full of joy and flourishing for many different sentient beings. That's kind of different from what you might think about biological risks that most people talk about, which are often things like Ebola or Zika. Ebola or Zika are unbelievably tragic for the people afflicted by them, but it doesn't seem like the evidence suggests that they have a realistic chance of causing international civilizational collapse and threatening our long-term&nbsp;future.</p><p>To take this further, we predict that we would need a really extremely big biological catastrophe to threaten the long-term future. We're really thinking about something that kills or severely impairs a greater proportion of the entire human civilization than what happened in either of the world wars or in the 1918 flu pandemic. That kind of implies that we're thinking about fatalities that could range into the hundreds of millions or even the billions. There's a lot of really amazing work that could go into preventing smaller risks, but that's not really what we've been focusing on so far. It's not what I anticipate us focusing on in the future. Overall, we're currently ranking the prevention of global catastrophic biological risks as a high priority, although I think it's somewhat uncertain. I think it's high priority to figure out more and then we might readjust our beliefs about how much we should prioritize&nbsp;it.</p><p>So what are these risks even like? We think the biggest risks are from biological agents that can be easily transmitted that can be released in one area and spread, as opposed to something like Anthrax, which is very terrible in the space that it's released, but it's hard to imagine it really coming to afflict a large proportion human&nbsp;civilization.</p><p>Then within the space of infectious diseases, we're thinking about whether the most risky type of attack would be something that happened naturally that just came out of an animal reservoir, or something that was deliberately done by people with the intention of causing this kind of destruction. Or it might be the middle ground of something that might have been accidentally released from a laboratory where people were doing&nbsp;research.</p><p>Our best guess right now is that deliberate biological attacks are the biggest risk. Accidental risk somewhere in the middle, and natural risk is low. I want to explain why that is because I think a lot of people would disagree with that. Some of the reasons I'm skeptical of natural risks are that first of all, they've never really happened before. Humans have obviously never been caused to go extinct by a natural risk, otherwise we would not be here talking. It doesn't seem like human civilization has come close to the brink of collapse because of a natural risk, especially in the recent&nbsp;past.</p><p>You can argue about some things like the Black Death, which certainly caused very severe effects on civilization in certain areas in the past. But this implies a fairly low base rate. We should think in any given decade, there's a relatively low chance of some disease just emerging that could have such a devastating impact. Similarly, it seems like it rarely happens with nonhuman animals that a pathogen emerges that causes them to go extinct. I know there's one confirmed case in mammals. I don't know of any others. This scarcity of cases also implies that this isn't something that happens very frequently, so in any given decade, we should probably start with a prior that there's a low probability of a catastrophically bad natural pathogen&nbsp;occurring.</p><p>Also, we're in a much better situation than we were in the past and than animals are in some ways, because we have advanced biomedical capabilities. We can use these to create vaccines and therapeutics and address a lot of risks from different pathogens that we could&nbsp;face.</p><p>Then finally, on kind of a different vein, people have argued that there's some selection pressure against a naturally emerging highly virulent pathogen because when pathogens are highly virulent, often their hosts die quickly and they try to rest before they die and they're not out in society spreading it the way you might spread the cold, if you go to work when you have the&nbsp;cold.</p><p>Now, before you become totally convinced about that, I think that there's some good countervailing considerations to consider about humanity, that make it more likely that a natural risk could occur now than in the past. For example, humanity is much more globalized, so it might be the case that in the past there were things that were potentially deadly for human civilization, but humans were so isolated it didn't really spread and it wasn't a huge deal. Now everything could spread pretty much around the&nbsp;globe.</p><p>Also, civilization might be more fragile than it used to be. It's hard to know, but it might be the case that we're very interdependent. We really depend on different parts of the world to produce different goods and perhaps a local collapse could have implications for the rest of the globe that we don't yet&nbsp;understand.</p><p>Then there's another argument one can usually bring up, which is if you're so worried about accidental or engineered deliberate attacks, there's also not very much evidence of those being a big deal. I would agree with this argument. There haven't been very many deliberate biological weapon attacks in recent times. There's not a strong precedent. Nonetheless, our best guess right now is that natural risks are pretty unlikely to derail human&nbsp;civilization.</p><p>When we think in more detail about where catastrophic biological attack risks come from, we can consider the different potential actors. I don't think that we've really come to a really strong view on this. I do want to explain the different potential sources. Some possible sources could be different states. For example, in bio-weapons programs, states could develop pathogens as weapons that have the potential to be destructive. Small groups, such as terrorists or other extremists might be interested in developing these sorts of capabilities. Individuals who have an interest, people working in various sorts of labs: in academia, in the government and on their own. There are DIY biohacker communities that do different sorts of biological experimentation. Those are the different groups that might contribute to catastrophic biological&nbsp;risk.</p><p>There are different kinds of pathogens and I think here - our thinking is even more preliminary - we're especially worried about viral pathogens, because there's proven potential for high transmissibilities and lethality among viruses. They can move really fast. They can spread really fast. We have fewer effective countermeasures against them. We don't have very good broad spectrum antivirals that are efficacious and that means that if we had a novel viral pathogen, it's not the case that we have a huge portfolio of tools that we can expect to be really helpful against&nbsp;it.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=1800&amp;q=70 1800w\"></figure><p>I've created this small chart that I think can help illustrate how we divide up these risks. On the top there's a dichotomy of whether the pathogen is more natural or more engineered and then on the vertical axis a dichotomy of whether it emerged naturally (or accidentally) or was a deliberate release. The reason I'm flagging these quadrants is because I think there are two different ways to increase the destructiveness of an attack. One is to engineer the pathogen really highly, and the other is to optimize the actual attack type. For example, if you released a pathogen at a major airport, you would expect it to spread more quickly than if you released it in a rural village. Those are two different ways in which you can become more destructive, if you're interested in doing that. Hopefully you're not. My current guess is that there's a lot more optimization space in engineering the actual pathogen than in the release type. There seems to be a bigger range, but we're not super confident about&nbsp;that.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=1800&amp;q=70 1800w\"></figure><p>Here's where the risk we see is coming from. There are advances in gene editing technology, which is a really major source of risk. I think that they've created a lot more room to tinker with biology in general to both lower resources and lower levels of knowledge required and to a greater overall degree, create novel pathogens that are different from what exists in nature, that you can understand how they work. This has amazing potential to do a lot of good but it also has potential to be misused. It's becoming a lot cheaper to synthesize DNA and RNA, to get genetic material for different pathogens. This means that these capabilities are becoming more widely available, just because they're cheaper. Regulating them and verifying buyers is becoming a bigger proportion of the costs, which means companies are more and more incentivized to stop regulating sales and verifying&nbsp;buyers.</p><p>Biotech capabilities are becoming more available around the world. They're spreading to different areas, to new labs. Again, this is mostly a sign of progress. People are having access to technology, places in Asia and around the world are having large groups of very talented scientists and that's really great for the most part, but it means there are more potential sources of risk than there were in the&nbsp;past.</p><p>Then finally, all of those things are happening much faster than governments can possibly hope to keep up and than norms can evolve, so that leads you to the situation where the technology has outpaced our society and our means of dealing with risk, and that increases the level of&nbsp;danger.</p><p>Now I'll contrast and compare biosecurity with AI alignment, because I think AI alignment is something people are much more familiar with. It might be helpful to draw attention to the differences, for getting people up to speed. I think that overall, there's a smaller risk of a far future negative trajectory change from biosecurity. Overall it seems like a smaller risk to&nbsp;me.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=1800&amp;q=70 1800w\"></figure><p>With addressing biosecurity risk, there are fewer potential upsides. With an AI, you can imagine that if it develops really well, it has this amazing potential to increase human capabilities and cause human flourishing. With biosecurity, we're basically hoping that just nothing happens. The best outcome is just nothing. No attacks occur. No one dies. Society progresses. In the case of AI alignment, maybe somebody develops an aligned AI, which would be great. But for biosecurity, we're really about preventing downside risks. More of the risk here comes from people with actively bad intentions as opposed to people with good intentions or people who are just interested in the research, especially if you believe and you agree with me that deliberate attacks are the most likely source of&nbsp;concern.</p><p>In biosecurity more than AI, I think there are many more relevant actors on both sides, as opposed to there being a few labs with a lot of capabilities in AI. It could be the case that we end up with a situation in biosecurity where there are millions of people that are capable of doing something that would be pretty destructive. And also, we can unilaterally develop counter measures against their attacks. There's less connection between the sources of the risk and the sources of the risk reduction. They're more divorced from one another. There's more possible actors on the sides of attack and&nbsp;defense.</p><p>I think that the way that The Open Philanthropy Project is seeing this field right now is somewhat different from how most people are seeing it. Most of the discussion in the field of biosecurity is focused on much smaller risks than the ones that we're worried about. I think discussion of things with greater than one million fatalities was kind of taboo up until very recently. It's been difficult for us to find people that are interested in working on that kind of thing. I think that part of the reason for that, is that it's been really hard to get funding in the space, so people want to make sure their work seems really relevant. And since small attacks and small outbreaks are more common, a good way to make your work more relevant is to focus on&nbsp;those.</p><p>There's ongoing debate in the field about whether natural, deliberate or accidental releases are the biggest risks. I don't think people are synced up on what the answer to that question is. I don't think everyone agrees with us that deliberate is mostly the thing to worry about. Then people are really trying to walk this tightrope of regulating risky research while not regulating productive research, maintaining national competitiveness, and encouraging productive biotech&nbsp;R&amp;D.</p><p>Given all of that, we have some goals in this space. They're kind of early goals. They won't be sufficient on their own. They're mostly examples, but I think they could get us pretty far. The first thing is we just really need to understand the relevant risks in particular. I'm keeping it very high level for now, because there's not a lot of time, and partly because I think that talking about some of these risks publicly is not a productive thing to do, and also because we're pretty uncertain about them. I think it would be really helpful to have some people dig into the individual risks. Think about what one would need to do in order to pull off a really catastrophic bio attack. How far out is that being a possibility? What sorts of technological advancements would need to occur? What sorts of resources would one need to be able to access in order to do that? If we can answer these questions, we can have a sense of how big catastrophic biosecurity risks are and how many actors we need to be worried&nbsp;about.</p><p>Understanding particular risks will help us prioritize things we can do to develop counter measures. We want to support people in organizations that increase the field's ability to respond to global catastrophic biological risks. The reason for that is that right now the field of biosecurity has lacked funding for a long time. A lot of people have left the field. Young people are having a very difficult time going into the field. Hopefully that's changing, but it's still a pretty dire situation, in my view. We want to make sure that the field ends up high quality with lots of researchers that care about the same risks we care about, so people that show signs of maybe moving in that direction, we're very enthusiastic about supporting, in&nbsp;general.</p><p>Then finally, we want to develop medical counter measures for the things that we're worried about. We've started having our science advisors look into this. We have some ideas about what the worst risks are and if we can develop counter measures in advance and stockpile those, I think we would be much better prepared to address risks when they come&nbsp;up.</p><p>Finally, I want to talk to you a little bit about what I think EAs can do to help. I see a lot of potential value in bringing parts of the EA perspective to the field. Right now there aren't a lot of EAs in biosecurity and I think that the EA perspective is kind of special and has something special to offer people. I think some of the really great things about it are, first of all, the familiarity with the idea of astronomical waste and the value of the far future. That seems like it's somewhat hard to understand. It's a bit weird and counterintuitive and philosophical, but a lot of EAs find it compelling. A lot of other people find it wacky or haven't really heard about it. I think having more concern about that pool of value and those people in the future who can't really speak for themselves could do the field of biosecurity a lot of&nbsp;good.</p><p>Another thing that I think is amazing about the EA perspective, is comfort with explicit prioritization, the ability to say, \"We really need to do X, Y, and Z. A, B, and C are lower priority. They'll help us less. They're less tractable. They're more crowded. We should start with these other things.\" I think right now, the field doesn't have a clear view about that. There's not a very well thought out and developed road map to addressing these concerns. I think EAs would be good at helping with&nbsp;that.</p><p>Finally, I think a lot of EAs have a skepticism with established methods and expertise. That's great because I think that's necessary actually in almost every field. Especially in fields that involve a complicated interplay of natural science and social science. I think that there's a lot of room for things to be skewed in certain directions. I haven't seen too much harmful skew, but guarding against it would be really&nbsp;helpful.</p><p>There's some work going on at the Future of Humanity Institute that we're very excited about. It seems like there's a lot of low hanging fruit right now. There are a lot of projects that I think an EA could take on and they'd be pretty likely to make progress. I think biosecurity progress is more of a matter of pulling information together and analyzing it, and less based only in pure&nbsp;insight.</p><p>I think that you should consider going into biosecurity if you are an EA concerned with the far future, who wants to make sure that we all get to enjoy our amazing cosmic endowment, and if you think that you might be a good fit for work in policy or in the biomedical&nbsp;sciences.</p><p>This is an area where I think that a lot of safety might come from people not overhyping certain sorts of possibilities as they emerge, at least until we develop counter measures. It's important to have people that feel comfortable and are okay with the idea of doing a lot of work and then not sharing it very widely and actually not making it totally open, because that could actually be counterproductive and increase risk. That's what I hope that people will be willing to do. I hope that we find some EAs who want to move into this field. If you feel like you're interested in moving into this field, I would encourage you to reach out to me or grab me sometime at this conference and talk about both what you'd like to do and what might be stopping you from doing&nbsp;it.</p><p>In the future we might write more about how we think people can get into this field and be able to do helpful research, but we haven't really done that yet, so in the meantime, I really hope that people reach out. Thank you so much and I'll take your&nbsp;questions.</p><h1>Q&amp;A</h1><p><strong>Question:</strong> Okay, so we've got a number of questions that have come in and I'm just gonna try to rifle through them and give you a chance to answer as many as we can. You emphasized the risk of viral pathogens. What about the, I think, more well known if not well understood problem of antibiotic resistance? Is that something that you're thinking about and how big of a concern is that for&nbsp;you?</p><p><strong>Claire Zabel:</strong> Yeah. I think that's a good question. The Open Philanthropy Project has <a href=\"https://www.openphilanthropy.org/research/cause-reports/antibiotic-resistance\">a report on antibiotic resistance</a> that I encourage you to read if you're curious about this topic. I think it's a really big concern for dealing with conventional bacterial pathogens. Our best guess is that it's not such a special concern for thinking about global catastrophic biological risks, first of all, because there's already immense selection pressure on bacteria to evolve some resistance to antibiotics, and while this mostly has really negative implications, it has one positive implication, which is that, if there's an easy way to do it, it's likely that it'll happen naturally first and not through a surprise attack by a deliberate bad&nbsp;actor.</p><p>Then another reason that we're worried about viruses to a greater extent than bacteria is because of their higher transmissibility and the greater difficulty we have disinfecting things from viral pathogens. So, I don't think that antibiotic resistance will be a big priority from the far-future biosecurity perspective. I think it's possible that we're completely wrong about this. I'm very open to that possibility, and what I'm saying is pretty low confidence right&nbsp;now.</p><p><strong>Question:</strong> Great. Next question. To what extent do small and large scale bio-risks look the same and to what extent do the counter measures for those small and large scale risks look the same, such that you can collaborate with people who have been more in the traditional focus area of the smaller scale&nbsp;risks?</p><p><strong>Claire Zabel:</strong> That's an interesting question. I think it's a complicated one and a simple answer won't answer it very well. When I think about the large scale risks, they look pretty different for the most part from conventional risks, mostly because they're highly engineered. They're optimized for destructiveness. They're not natural. They're not something we're very familiar with, so that makes them unlikely to be things that we have prepared responses to. They're likely to be singularly able to overwhelm healthcare systems, even in developed countries, which is not something that we have much experience&nbsp;with.</p><p>But the second part of the question about the degree to which efforts to address small scale risks help with big scale risks and vice versa, I think that that's somewhat of an open question for us and as we move towards prioritizing in the space, we'll have a better view. There's some actions that we can take. For example, advocacy to get the government to take biosecurity more seriously might help equally with both. On the other hand, I think developing specific counter measures, if we move forward with that, will be more likely to only help with large scale risks and be less useful with small scale risks, although there are counter examples that I'm thinking of right now, so that's definitely not an accurate blanket&nbsp;statement.</p><p><strong>Question:</strong> When you think about these sort of engineered attacks that could create the largest scale risk, it seems like one thing that has sort of been on the side of good, at least for now, is that it does take quite a bit of capital to spin up a lab and do this kind of bioengineering. But, as you mentioned, stuff is becoming cheaper. It's becoming more widely available. How do you see that curve evolving over time? Right now, how much capital do you think it takes to put a lab in place and start to do this kind of bad work if you wanted to and how does that look five, ten, twenty years&nbsp;out?</p><p><strong>Claire Zabel:</strong> I don't think I want to say how much it takes right now, or exactly what I think it will take in the future. I think the costs are falling pretty quickly. It depends on what ends up being necessary, so for example, the cost of DNA synthesis is falling really rapidly. It might be the case that that part is extremely cheap, but actually experimenting with a certain pathogen that you think might have destructive capability - for example, testing it on animals - might remain very expensive, and it doesn't seem like the costs of that part of a potential destructive attack are falling nearly as&nbsp;quickly.</p><p>Overall, I think costs will continue to fall but I would guess that the falling plateaus sometime in the next few&nbsp;decades.</p><p><strong>Question:</strong> Interesting. Does biological enhancement fall within your project at all? Have you spent time considering, for example, enhancing humans or working on gene editing on humans and how that might be either beneficial or potentially destabilizing in its own&nbsp;way?</p><p><strong>Claire Zabel:</strong> That's not something that we've really considered a part of our biosecurity&nbsp;program.</p><p><strong>Question:</strong> Fair enough. How interested is Open Philanthropy Project in funding junior researchers in biosecurity or biodefense? And relatedly, which would you say is more valuable right now? Are you looking more for people who have kind of a high level strategic capability or those who are more in the weeds, as it were, of wet synthetic&nbsp;biology?</p><p><strong>Claire Zabel:</strong> Yeah. I think that right now we'd be excited about EAs that are interested in either, potentially, depending on their goals in this field, the extent of the value alignment, and their dedication and particular talents. I think both are useful. I expect that the kind of specialization, for example, either in policy or in biomedical science will possibly be more helpful in the long term. I'm hoping that we'll gain a lot of ground on the strategic high level aspects of it in the next few years, but right now I think both are sorely&nbsp;needed.</p><p><strong>Question:</strong> Next question. For someone whose education and skills have been focused on machine learning, how readily can such a person contribute to the type of work that you're doing and what would that look like if they wanted to get&nbsp;involved?</p><p><strong>Claire Zabel:</strong> I don't know. I've never seen anyone try. I think that it would be possible because I think that there's a lot of possibility of someone who has no special background in this area, in general, becoming really productive and helpful within a relatively short time scale and I don't see machine learning background as putting anyone at a particular disadvantage. Probably it would put you at somewhat of an advantage, although I'm not sure how. I think that right now, the best way to go would probably be just to get a Masters or PhD in a related field and then try to move into one of the relevant organizations, or try to directly work at one of the relevant organizations like our biggest grantee in biosecurity, the Center for Health Security. And for that, I think that probably having a background in machine learning would be neither a strong drawback nor a huge&nbsp;benefit.</p><p><strong>Question:</strong> That's about all the time that we have for now,&nbsp;unfortunately.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "8ZwK6poQYgfdSvGu8", "title": "The case for reducing existential risk", "postedAt": "2017-10-01T08:44:59.879Z", "htmlBody": "<p>In 1939, Einstein wrote to Roosevelt:<sup class=\"footnote-ref\"><a href=\"#fn-6RnfgebXyizubrgaa-1\" id=\"fnref-6RnfgebXyizubrgaa-1\">[1]</a></sup></p>\n<blockquote>\n<p>It may be possible to set up a nuclear chain reaction in a large mass of uranium\u2026and it is conceivable \u2014 though much less certain \u2014 that extremely powerful bombs of a new type may thus be constructed.</p>\n</blockquote>\n<p>Just a few years later, these bombs were created. In little more than a decade, enough had been produced that, for the first time in history, a handful of decision-makers could destroy civilisation.</p>\n<p>Humanity had entered a new age, where we faced not only existential risks<sup class=\"footnote-ref\"><a href=\"#fn-6RnfgebXyizubrgaa-2\" id=\"fnref-6RnfgebXyizubrgaa-2\">[2]</a></sup> from our natural environment, but also those of our own creation.</p>\n<p>In this new age, what should be our biggest priority as a civilisation? Improving technology? Helping the poor? Changing the political system?</p>\n<p>Here\u2019s a suggestion that\u2019s not so often discussed: our first priority should be to <em>survive</em>.</p>\n<p>So long as civilisation continues to exist, we\u2019ll have the chance to solve all our other problems, and have a far better future. But if we go extinct, that\u2019s it.</p>\n<p>Why isn\u2019t this priority more discussed? Here\u2019s one reason: many people don\u2019t yet appreciate the change in situation, and so don\u2019t think our future is at risk.</p>\n<p>Social science researcher <a href=\"http://www.spencergreenberg.com/\">Spencer Greenberg</a> surveyed Americans on their estimate of the chances of human extinction within 50 years. The results found that many think the chances are extremely low, with over 30% guessing they\u2019re under one in ten million.<sup class=\"footnote-ref\"><a href=\"#fn-6RnfgebXyizubrgaa-3\" id=\"fnref-6RnfgebXyizubrgaa-3\">[3]</a></sup></p>\n<p>We used to think the risks were extremely low as well, but when we looked into it, we changed our minds. As we\u2019ll see, researchers who study these issues think the risks are over one thousand times higher, and are probably increasing.</p>\n<p>These concerns have started a new movement working to safeguard civilisation, which has been joined by Stephen Hawking, Max Tegmark, and new institutes founded by researchers at <a href=\"http://cser.org/about/who-we-are/\">Cambridge</a>, <a href=\"https://futureoflife.org/\">MIT</a>, <a href=\"https://www.fhi.ox.ac.uk/\">Oxford</a>, and elsewhere.</p>\n<p>In the rest of this article, we cover the greatest risks to civilisation, including some that might be bigger than nuclear war and climate change. We then make the case that reducing these risks could be the most important thing you do with your life, and explain exactly what you can do to help. If you would like to use your career to work on these issues, we can also give <a href=\"https://80000hours.org/speak-with-us/?int_campaign=article__extinction-risk--ab-52-1\">one-on-one support</a>.</p>\n<h3><strong><a href=\"https://80000hours.org/articles/existential-risks/#how-likely-are-you-to-be-killed-by-an-asteroid-an-overview-of-naturally-occurring-existential-risks\">Continue reading on 80,000 Hours' website</a></strong></h3>\n<p><em>This work is licensed under a <a href=\"https://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License.</a></em></p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-6RnfgebXyizubrgaa-1\" class=\"footnote-item\"><p><em>\"In the course of the last four months it has been made probable \u2014 through the work of Joliot in France as well as Fermi and Szil\u00e1rd in America \u2014 that it may become possible to set up a nuclear chain reaction in a large mass of uranium, by which vast amounts of power and large quantities of new radium-like elements would be generated. Now it appears almost certain that this could be achieved in the immediate future.</em>\n<em>\"This new phenomenon would also lead to the construction of bombs, and it is conceivable \u2014 though much less certain \u2014 that extremely powerful bombs of a new type may thus be constructed. A single bomb of this type, carried by boat and exploded in a port, might very well destroy the whole port together with some of the surrounding territory. However, such bombs might very well prove to be too heavy for transportation by air.\"</em></p>\n<p>Einstein\u2013Szil\u00e1rd letter, Wikipedia,\n<a href=\"https://web.archive.org/web/20171017093405/https://en.wikipedia.org/wiki/Einstein%E2%80%93Szil%C3%A1rd_letter\">Archived link</a>, retrieved 17 October 2017. <a href=\"#fnref-6RnfgebXyizubrgaa-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6RnfgebXyizubrgaa-2\" class=\"footnote-item\"><p>Nick Bostrom <a href=\"https://www.nickbostrom.com/existential/risks.html\">defines an existential risk</a> as event that \u201ccould cause human extinction or permanently and drastically curtail humanity\u2019s potential\u201d. An existential risk is distinct from a <a href=\"https://en.wikipedia.org/wiki/Global_catastrophic_risk\">global catastrophic risk (GCR)</a> in its scope \u2013 a GCR is catastrophic at a global scale, but retains the possibility for recovery. An existential threat <a href=\"https://www.theatlantic.com/ideas/archive/2019/06/2020-candidates-say-everything-existential-threat/591967/\">seems to be used</a> as a linguistic modifier of a threat to make it appear more dire. <a href=\"#fnref-6RnfgebXyizubrgaa-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-6RnfgebXyizubrgaa-3\" class=\"footnote-item\"><p>Greenberg surveyed users of Mechanical Turk, who tend to be 20-40 and more educated than average, so the survey doesn\u2019t represent the views of all Americans. See more detail in this video: <a href=\"https://youtu.be/tOSpj19eows?t=616\">Social Science as Lens on Effective Charity: results from four new studies \u2013 Spencer Greenberg</a>.</p>\n<p>The initial survey found a median estimate of the chance of extinction within 50 years of 1 in 10 million. Greenberg did three replication studies and these gave higher estimates of the chances. The highest found a median of 1 in 100 over 50 years. However, even in this case, 39% of respondents still guessed that the chances were under 1 in 10,000 (about the same as the chance of a 1km asteroid strike). In all cases, over 30% thought the chances were under 1 in 10 million. You can see a summary of all the surveys <a href=\"http://spencergreenberg.com/documents/Comparison%20of%20study%20results%20about%20probability%20of%20human%20extinction.xlsx\">here</a>.</p>\n<p>Note that when we asked people about the chances of extinction with no timeframe, the estimates were much higher. One survey gave a median of 75%. This makes sense \u2014 humanity will <em>eventually</em> go extinct. This helps to explain the discrepancy with some other surveys. For instance, \u201cClimate Change in the American Mind\u201d (May 2017, <a href=\"https://web.archive.org/web/20171031033116/http://climatecommunication.yale.edu/wp-content/uploads/2017/07/Climate-Change-American-Mind-May-2017.pdf\">archived link</a>), found that the median American thought the chance of extinction from climate change is around 1 in 3. This survey, however, didn\u2019t ask about a specific timeframe. When Greenberg tried to replicate the result with the same question, he found a similar figure. But when Greenberg asked about the chance of extinction from climate change in the next 50 years, the median dropped to only 1%. Many other studies also don\u2019t correctly sample low probability estimates \u2014 people won\u2019t typically answer 0.00001% unless presented with the option explicitly.</p>\n<p>However, as you can see, these types of surveys tend to give very unstable results. Answers seem to vary on exactly how the question is asked and on context. In part, this is because people are very bad at making estimates of tiny probabilities. This makes it hard to give a narrow estimate of what the population in general thinks, but none of what we\u2019ve discovered refutes the idea that a significant number of people (say over 25%) think the chances of extinction in the short term are very, very low, and probably lower than the risk of an asteroid strike alone. Moreover, the instability of the estimates doesn\u2019t seem like reason for confidence that humanity is rationally handling these risks. <a href=\"#fnref-6RnfgebXyizubrgaa-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "Benjamin_Todd"}}, {"_id": "rMrNoJeWQPETPKQ5E", "title": "How big a deal was the Industrial Revolution?", "postedAt": "2017-09-16T07:00:00.000Z", "htmlBody": "<p><strong>Explanatory note:</strong> This page grew out of one of my investigations for <a href=\"http://www.openphilanthropy.org/\">Open Phil</a>, but then I got fascinated and put a bunch of personal time into elaborating certain parts of it, and it evolved into something that I think is pretty cool, but which would take more work than it\u2019s worth to vet and edit it such that it would be appropriate for Open Phil\u2019s website, so we decided I should just post it here instead as a personal project. Hence, the below doesn\u2019t represent Open Phil\u2019s position on anything, and should be taken merely as my own personal guesses and opinions.</p>\n<hr>\n<p>(Probably best to start with my <a href=\"http://lukemuehlhauser.com/three-wild-speculations-from-amateur-quantitative-macrohistory/\">companion blog post</a>.)</p>\n<p>One way to look for opportunities to accomplish as much good as possible is to ask \u201cWhich developments might have an extremely large impact on human civilization,<sup class=\"footnote-ref\"><a href=\"#fn-Ghbx5MHSCs4AA6cn8-1\" id=\"fnref-Ghbx5MHSCs4AA6cn8-1\">[1]</a></sup> and is there any way we can (in expectation) nudge those developments in a positive direction?\u201d</p>\n<p>For example, in the context of philanthropy, the Rockefeller Foundation funded work on an improved agricultural approach that led to the <a href=\"https://en.wikipedia.org/wiki/Green_Revolution\">Green Revolution</a>, which some people have credited with kickstarting the development of the \u201c<a href=\"https://en.wikipedia.org/wiki/Four_Asian_Tigers\">Asian Tigers</a>,\u201d helping several countries transition from \u201cpoor\u201d to \u201cmiddle income,\u201d transforming India from being in the middle of a famine to being a wheat exporter, and saving over a billion people from starvation.<sup class=\"footnote-ref\"><a href=\"#fn-Ghbx5MHSCs4AA6cn8-2\" id=\"fnref-Ghbx5MHSCs4AA6cn8-2\">[2]</a></sup> Of course, the Rockefeller Foundation had no way of knowing their funding would have such incredible impact, but a rare win of that magnitude can make up for a large number of failed (and similarly uncertain) funding efforts. (See Holden Karnofsky\u2019s <a href=\"http://www.openphilanthropy.org/blog/hits-based-giving\">hits-based giving</a>.)</p>\n<p>However, some future developments might have even greater impact than the Green Revolution, and be more comparable in magnitude to the changes often attributed to the <a href=\"https://en.wikipedia.org/wiki/Industrial_Revolution\">industrial revolution</a>. Here, I refer to changes of this magnitude as \u201ctransformative,\u201d<sup class=\"footnote-ref\"><a href=\"#fn-Ghbx5MHSCs4AA6cn8-3\" id=\"fnref-Ghbx5MHSCs4AA6cn8-3\">[3]</a></sup> and I refer to developments which might precipitate such transformative changes as potential \u201ctransformative developments\u201d for human civilization.<sup class=\"footnote-ref\"><a href=\"#fn-Ghbx5MHSCs4AA6cn8-4\" id=\"fnref-Ghbx5MHSCs4AA6cn8-4\">[4]</a></sup></p>\n<p>In the future, I hope to spend more time identifying potentially transformative developments,<sup class=\"footnote-ref\"><a href=\"#fn-Ghbx5MHSCs4AA6cn8-5\" id=\"fnref-Ghbx5MHSCs4AA6cn8-5\">[5]</a></sup> especially those which might also be <a href=\"http://www.openphilanthropy.org/research/our-process#Exploring_potential_focus_areas\">tractable and neglected</a>. In this report, I hope to lay some groundwork by examining the magnitude of \u201ctransformative\u201d change. In particular, I ask:</p>\n<ul>\n<li>The industrial revolution is often considered the most transformative event in recorded history.<sup class=\"footnote-ref\"><a href=\"#fn-Ghbx5MHSCs4AA6cn8-6\" id=\"fnref-Ghbx5MHSCs4AA6cn8-6\">[6]</a></sup> How large, exactly, were the differences in human well-being before and after the industrial revolution?</li>\n<li>Have there been other transitions in recorded history of comparable magnitude, either positive or negative?</li>\n<li>How catastrophic would a development need to be to plausibly result in negative transformative change?</li>\n<li>What do these initial findings suggest about potential future transformative developments?</li>\n</ul>\n<p>My <a href=\"https://lukemuehlhauser.com/industrial-revolution/#Conclusions\">initial tentative conclusions</a> from this preliminary investigation can be summarized as follows:</p>\n<blockquote>\n<p>The gains in human well-being observed since the industrial revolution are vastly larger than pre-industrial fluctuations in human well-being. No other transitions in recorded history, either positive or negative, are remotely similar in magnitude. When thinking about which future developments might be most important, we should not forget that the size of their likely impact may differ by orders of magnitude. For example, a universal cure for cancer would bring a huge benefit to human well-being, but its expected impact seems likely to be vastly smaller than (for example) the likely impact of AI systems capable of automating most human labor, or the counterfactual benefit of preventing large-scale nuclear war.</p>\n</blockquote>\n<h3><strong><a href=\"https://lukemuehlhauser.com/industrial-revolution/#IndustrialRevolution\">Read the rest of the post</a></strong></h3>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-Ghbx5MHSCs4AA6cn8-1\" class=\"footnote-item\"><p>There are other ways to look for opportunities to accomplish as much good as possible, of course, and the Open Philanthropy Project pursues some of them; see the blog post on <a href=\"http://www.openphilanthropy.org/blog/worldview-diversification\">worldview diversification</a>. For more on the potential value of the sorts of \u201ctrajectory changes\u201d discussed in this report, see <a href=\"https://rucore.libraries.rutgers.edu/rutgers-lib/40469/\">Beckstead (2013)</a>, ch. 3.</p>\n<p>I would also like to examine which future developments might have the largest impact on the future well-being of <a href=\"http://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\">non-human moral patients</a>, but for simplicity, I discuss only human well-being on this page. <a href=\"#fnref-Ghbx5MHSCs4AA6cn8-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Ghbx5MHSCs4AA6cn8-2\" class=\"footnote-item\"><p>I paraphrase this summary from comments made by the Open Philanthropy Project\u2019s Managing Director Holden Karnofsky, who shared his impressions of the plausible impact of the Green Revolution at an Open Philanthropy Project research event held on June 6, 2017 (starting around 4:10 in the recording available <a href=\"http://www.givewell.org/research/research-discussions\">here</a>):</p>\n<p><em>\u2026the Rockefeller Foundation funded [an] improved agricultural approach that [some] people have credited with kickstarting the East Asian tigers development phenomenon, getting a lot of countries to go from poor to middle income, turning India from being in the middle of famine to being a wheat exporter, saving over a billion people from starvation, and resulting in a Nobel peace prize for Norman Borlaug\u2026 [This is] maybe one of the most significant humanitarian developments of the last century \u2014 or really, ever.</em></p>\n<p>The causal impacts of the Green Revolution in general, and the Rockefeller Foundation\u2019s funding in particular, are of course difficult to discern with any certainty, and I don\u2019t discuss the evidence for these specific claims here. Some sources that contributed to Karnofsky\u2019s impressions on this topic include those linked from <a href=\"http://blog.givewell.org/2009/03/16/can-the-green-revolution-be-repeated-in-africa/\">Can the Green Revolution be repeated in Africa?</a> <a href=\"#fnref-Ghbx5MHSCs4AA6cn8-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Ghbx5MHSCs4AA6cn8-3\" class=\"footnote-item\"><p>Holden Karnofsky introduced this use of the term \u201ctransformative\u201d when he <a href=\"http://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec1\">defined</a> \u201ctransformative artificial intelligence\u201d (transformative AI), roughly and conceptually, as \u201cAI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution.\u201d <a href=\"#fnref-Ghbx5MHSCs4AA6cn8-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Ghbx5MHSCs4AA6cn8-4\" class=\"footnote-item\"><p>This terminology isn\u2019t ideal, though \u2014 in part because most people use the term \u201ctransformative\u201d to refer to much smaller changes than the changes I label \u201ctransformative\u201d here. Perhaps someone else will suggest a better term. <a href=\"#fnref-Ghbx5MHSCs4AA6cn8-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Ghbx5MHSCs4AA6cn8-5\" class=\"footnote-item\"><p>Open Phil thinks <a href=\"http://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity\">transformative AI</a> is one such potentially transformative development. See <a href=\"http://www.openphilanthropy.org/blog/some-background-our-views-regarding-advanced-artificial-intelligence#Sec3\">here</a> for a summary of some earlier thinking about additional potentially transformative developments. <a href=\"#fnref-Ghbx5MHSCs4AA6cn8-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-Ghbx5MHSCs4AA6cn8-6\" class=\"footnote-item\"><p>See <a href=\"https://lukemuehlhauser.com/industrial-revolution/#historians\">this footnote</a>. <a href=\"#fnref-Ghbx5MHSCs4AA6cn8-6\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "lukeprog"}}, {"_id": "7wisrWdLNCboZdQWJ", "title": "Will MacAskill: Closing talk (2017)", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=OxEegN0HY8g&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=31\"><div><iframe src=\"https://www.youtube.com/embed/OxEegN0HY8g\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "g86DChwzCm8bZmKtK", "title": "Joseph Gordon-Levitt, Julia Galef, AJ Jacobs, and Will MacAskill: EA in media", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=MyweG9__CCQ&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=30&amp;t=16s\"><div><iframe src=\"https://www.youtube.com/embed/MyweG9__CCQ\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><h2>Transcript</h2><p><br>Nathan Young&nbsp; 0:04&nbsp;&nbsp;</p><p>I am honored to introduce our panelists. We've got a great panel for you here for EAs in media. First up is an actor. His acting career spans the last three decades and ranges from TV where he was on 3rd Rock from the Sun to art auctions, such as Mysterious Skin and Brick to multiplex movies such as Inception, 500 Days of Summer, and Snowden. He made his feature screenwriting and directorial debut with Don Jon, which was nominated for Best for Screenplay at the Independent Spirit Awards. He also founded and directs HitRecord, an online community of over half a million artists emphasizing collaboration over self-promotion. HitRecord has evolved into a community source production company, publishing books, putting out records producing videos for brands from LG to the ACLU, and winning an Emmy for its variety show HitRecord on TV, please welcome again Joseph Gordon-Levitt.</p><p><br>&nbsp;</p><p>Next up is Julia Galef. Julia is a writer and speaker focused on improving human judgment, especially about high-stakes questions. She has a background in statistics and social science, worked as a case writer for Harvard Business School, and then as a freelance journalist in New York. She has been the host of the Rationally Speaking Podcast since 2010, co-founded the Center for Applied Rationality in 2012, and is currently working for the Open Philanthropy Project on an investigation of expert disagreements. Please welcome Julia Galef.</p><p><br>&nbsp;</p><p>AJ Jacobs is next. He is an author, journalist, and human guinea pig. He is the author of four New York Times bestsellers including the Year of Living Biblically, The Know-It-All, and Drop Dead Healthy. He wrote a feature about Effective Altruism for Esquire magazine where he is a contributing editor. The article followed his attempt to figure out where to donate his writer's fee. He is a contributor to NPR's weekend edition. He has given three TED talks that have appeared on TED's main page and have more than 3 million total views. His book, The Year of Living Biblically is being turned into a TV show by CBS and will air in the fall. His new book, It's All Relative will be released in November, and it chronicles his attempt to unite the entire world in a single-family tree. Please welcome AJ Jacobs. And finally, to moderate this great panel, please welcome back Will MacAskill.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 2:45&nbsp;&nbsp;</p><p>Cool, okay, thank you all for coming to this panel. And a reminder that on the Bizzabo app, you can start submitting questions. You know, we'll have a conversation for 20 or 25 minutes, but then afterward, we were looking forward to seeing questions from all of you. So to begin with, let's just if each of you maybe, AJ, Julia, and Joe, talk a little bit about your background and your current relationship to effective altruism. Yeah, go for it.&nbsp;</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 3:22&nbsp;&nbsp;</p><p>Yes. Mine's working. Yeah, awesome. So my name is Joe. I just said an embarrassingly listened to my bio be read right in front of me that a poorly executed entrants. But I work in show business. And I first found out about EA because a friend invited my wife and me to the conference last year and was really just completely taken with it. You know, I hate to stereotype, Hollywood, but I think it's pretty true that it's emotionally driven and there's lots of good intentions and lots of people that are trying to do good with whatever leverage Hollywood gives you. But oftentimes those attempts to do good are very, very emotionally motivated by what makes the best story. And coming and learning about these concepts of not following what makes the best story but what can actually probably be shown to be doing the most good, I found incredibly compelling and noble and worth talking about some more. So happy to be here.&nbsp;</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 4:37&nbsp;&nbsp;</p><p>Okay, AJ, tell us a little bit about yourself.&nbsp;</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 4:38&nbsp;&nbsp;</p><p>&nbsp;I am a writer and a journalist. And as Nathan said a human guinea pig. So I like to take on a project and immerse myself in it for a year or two and then write about it. For instance, I wrote a book about living by all the rules of the Bible as literally as possible. So I had the 10 commandments. I followed those I had a beard down to here, I stone adulterers, but I use pebbles so that I didn't get arrested. And I tithe because they do say 10% in the Bible. I gave to orphans and widows as the Bible instructed, which I don't know if GiveWell approves of, but seemed okay. And I got involved because I wrote an article for Esquire last year, where I took my writer's fee, a couple of $1,000. And I wanted to figure out what I could do with that money to produce the most good in the world. And guess what I found is GiveWell. So I gave it to GiveWell, and to, to Will, and to Peter Singer. I sort of split it up a little. But I fell in love with the idea of effective altruism. And hopefully, there were a couple of converts from the article, but maybe not.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 6:04&nbsp;&nbsp;</p><p>Okay, thanks. Julia, can you also talk about a bit about your background relationship with EA?</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 6:11&nbsp;&nbsp;</p><p>Sure. Yeah. I originally thought I was going to go into academia, into social science research. And then I ended up deciding, you know, I quit grad school, I'd been an Economics program. And I told my parents, I think what I really want to do with my life is just spend as much time as I can, talking to really smart and thoughtful people about the most important issues. And my parents were like that, do you get paid for that? And the honest answer was, \"I have no idea, I'll find out.\" So I dropped out of grad school, I became a freelance journalist, I did a bunch of blogging, I started a podcast called Rationally Speaking. And I was sort of writing about and speaking about EA or EA-adjacent things kind of before EA was a thing like, I don't know, I wrote a post about trying to estimate the number of lives per calorie for different animal products and reached this sort of counterintuitive, or many people counterintuitive conclusion that eggs are actually worse in terms of costing animal life than beef is. So as I was posting stuff like that, for like Scientific Americans blog and talking about it on my podcast, and then about five years or so ago, I moved out here and co-founded a nonprofit called the Center for Applied Rationality, of which there are several representatives here leading sessions. So I encourage you to go to them. And then right now I'm doing a number of things I'm still doing the podcasts, I'm working on a book, won't be up for a while. And I'm also working with a foundation called the Open Philanthropy project, where my role is basically hosting conversations, bringing together people from tech and finance in the media, along with EA, and academia, and experts on really important questions like AI potential risks from AI, or criminal justice reform or land-use reform. And sort of getting the experts to better understand each other's models and getting the people outside of that field in tech, or the media or finance, to really understand the experts' models and sort of have heard the best arguments on both sides of these important topics. So I'm basically doing what I set out to do, which actually, to everyone's surprise, including my own.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 8:43&nbsp;&nbsp;</p><p>Thank you. And so yeah, a lot of people in this audience are really interested in promoting the ideas of effective altruism more widely. And I know that each of you in very different ways, have some interest in doing that whether you're doing it now or potentially in the future. What do you think of some of our real challenges with respect to trying to communicate the core ideas of EA more widely? Joe, do you want to start?&nbsp;</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 9:08&nbsp;&nbsp;</p><p>Sure. Well, I think this might be obvious, but the truth is that to me, it seems a lot of the principles of what EA is are very much at odds with how the media works. Because media and that's especially you know, television, but also newer media online, is not a platform that's conducive to reasoned thinking and really analyzing something and picking it apart and understanding it. It's much more about creating, just getting attention, getting eyeballs, and making things addictive. And so I don't have a solution to this, but it's maybe worth at least identifying the problem. It seems to me that what I really like about EA is completely across purposes with everything I know about how the media works. So, I don't know, I'd be interested to start brainstorming how do you then deal with that? You know, do you, I guess, come to conclusions through more reasoned thinking and then present those conclusions in the media unreasonably, more emotionally? I don't know. Is that sort of hypocritical? Is that going against the principles of EA? I do sort of think that if you really want to get any kind of message out in front of a wide audience, and not just preach to the choir, but like, get out there and into what you might call mainstream culture, you can't too reasonable.</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 11:00&nbsp;&nbsp;</p><p>Okay, yeah. Will, you and I actually talked about this, because I agree with Joe, that the media does not represent reality. As you said, most news is fake news. So we talked about this idea of the reality times, which would be a news site, which accurately reflected reality. So every day, the front page would be, instead of, you know, two people killed by a shark attack, the 2500 people died of malaria yesterday, or 48,000 died of heart disease. And it doesn't all have to be bad news. So like, when they had the London fire, with 70 people dying, you could print an article that 10s of 1000s of lives have been saved over the last decades because home fires are way down. And then you could put the shark attack on like the 50th page. And so I would love to actually start that. I don't know if I have much time to do it. But if anyone wants to do it, I'm happy to, pro bono, be the adviser.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 12:16&nbsp;&nbsp;</p><p>The idea as well is that the front page basically wouldn't change the day on day.&nbsp;</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 12:19&nbsp;&nbsp;</p><p>That's true, yes, exactly.&nbsp;</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 12:21&nbsp;&nbsp;</p><p>Because news is like yep, again, 5000 children died a minute. Same as yesterday.</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 12:28&nbsp;&nbsp;</p><p>Down in the corner on the bottom right corner, you could change every day, but yeah, the main one would be [the same].</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 12:33&nbsp;&nbsp;</p><p>Yeah, [inaduble] well, the other thing I love about this idea is representing it as, alien invaders have like tiny little helicopters flying into our homes and poisoning our children. What can we do about it? 1000s are dead. And then it's revealed as this malaria mosquitoes. Like, how you would cover this if it was an alien invasion?</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 13:00&nbsp;&nbsp;</p><p>Right. That's taking us the slow crisis. Yes, turning it and like get a reporter on the scene. Oh, my God.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 13:07&nbsp;&nbsp;</p><p>So their solution is okay, normal media is completely at odds with EA, maybe we just have to create our own. Julia, what are your thoughts on this?</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 13:17&nbsp;&nbsp;</p><p>I share Joe's pessimism. I was just thinking about this Saturday morning breakfast cereal comic about this problem in science journalism, where the first panel is a reporter talking to a scientist and the scientist is like, we had really successful results in our early cancer trials, we were able to, like reduce the spread of this cancer by this percent in this species. And then the next panel is the headline, Scientists Cure Cancer. And then the panel off the bat is the scientist coming back to the reporter and saying, like, kind of annoyed now like, \"No, no, we didn't cure cancer. We just, we'd like moved faster towards a future solution to cancer.\" And then the next panel is another headline thing, Scientists Invent Time Travel. Sorry, turn it louder? I don't know how to do that. So I'll just speak louder. Um, so yeah, it's a huge problem. And I think, a common failure mode for EAs when talking about these topics is that you know, EAs tend to be really, really like nerdy and intellectual. And we want to gravitate towards the most intellectually interesting things to talk about in interviews, or in the material we put out there. And the problem is that what's intellectually interesting to you is sort of weird or confusing or alienating to most people. And so you kind of have to be willing to talk about the stuff that to you is super boring and trivial, and just are</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 14:56&nbsp;&nbsp;</p><p>Or obvious.</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 14:57&nbsp;&nbsp;</p><p>Or obvious. Exactly. And just trust like, you know, maybe you can't even quite remember, but the first time you encountered the idea that whoa, a lot of ways that people try to do good actually don't work. And there are other things that are sort of demonstrably 100 times more effective. And wouldn't it be better to do the effective things than the ineffective things? Maybe that sounds obvious if it's the background of your life for years, but people hearing it for the first time, that's like, that's a really cool insight that they haven't thought about before. But to you, it's boring. So you have to kind of be willing to bore yourself, when talking to the media or putting stuff out there and just remind yourself as being intellectually interesting. Oh, and also being able to signal my capacity for nuance and sophistication, which I think a lot of us like to do in a conversation is that's not what this is for. This is for getting a message out there, which is hard.</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 15:49&nbsp;&nbsp;</p><p>Another thing I found, which what you're saying reminds me of, as far as boring yourself when talking to the media, I spend a fair amount of my life having to promote something that I've done and talking to media, and what you find is you you have to be repetitive. And it's really boring for yourself because you're just saying the same thing over and over and over again. And the natural instinct is, of course, to say new things to all the different people. But that's really not as effective. What's really the most effective thing is to find the two or three points that you want to make, and pretty much repeat those over and over and over and over and over again. And it's not as fun. But that's if you want to communicate something out to a lot of people. That's kind of how you have to do it. In my experience.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 16:43&nbsp;&nbsp;</p><p>Yeah. And so something that's related to this question of challenges and that being opposed is, you know, we talk about the media, but that's a variety of different things. So social media -- Twitter, and Facebook -- podcasts, articles, books, movies, documentaries. Which of these do you think are kind of more amenable? You know, you've got experience with all of them, between the three of you? Which do you think are the most amenable for EA? And what things should we maybe stay away from? You know, I made a joke in the Boston conference about imagine if Sir Francis Bacon had been trying to promote the scientific method on Twitter. And then he just gets all these messages calling him a cock in response. You know, probably wouldn't go down that well. I know that Julia's more for Twitter.</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 17:29&nbsp;&nbsp;</p><p>I'm very much for Twitter. One of the things I love about Twitter is there's a whole community of social scientists and statisticians and scientists who have conversations on Twitter, and they're actually debating with each other about the right threshold for the P-value and discussing the latest scientific results. Really cool. So yeah, I'm bullish on Twitter.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 17:51&nbsp;&nbsp;</p><p>Okay. And what do you think, AJ and Joe about other media as well?</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 17:55&nbsp;&nbsp;</p><p>Well, yeah, I mean, I think it's a fascinating topic, as far as you how useful is Twitter. Twitter being an example. And Twitter being a good one, because it's probably the most extreme example of having to be super short attention span. I use Twitter a lot. But I don't so much engage in conversations on Twitter. What I try to do is use Twitter to get people off of Twitter. And onto like, a community that you know, I started a sort of an artist community called HitRecord. And so Twitter's really effective at sort of attracting people who might be interested in what we're doing and bringing them over to us. So maybe that's a strategy to consider for EA because I could see how trying to discuss the stuff that would get discussed on this stage would be really difficult in 140 character tweets. But a quick sort of hook to get them to come off of Twitter and enter into whatever other online activity you want them to engage in. That's maybe another way to use Twitter.</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 19:04&nbsp;&nbsp;</p><p>Yeah. And, and my feeling is use everything possible. Just don't discriminate however you get your message out. But I think some forms of getting the message out are more effective. And, for instance, stories -- I actually think it's very unfortunate that humans love stories instead of statistics. It would be nice if we all found statistics as emotionally appealing, but we don't. So, you and I talked about this before, like really focusing in on one, like doing a story where you follow someone who got $1,000 from Give Directly and how it changed their life. And you can put that out in whatever form you want article video, and then say, this happens millions of times a day. So donate to this cause, as opposed to just saying, you know, there are a billion people who live in poverty, therefore you should donate just focusing in on the one person.</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 20:07&nbsp;&nbsp;</p><p>Stories are good. I think another thing you can do even on, you know, Twitter or social media where people are not going to read a long nuanced article is creating images that people will share and convey a message really well. So one person on Twitter who I think does this excellently and his message is actually pretty aligned with the EA is Max Roser, who does Our World in Data. And he shares these charts and graphs showing how dramatically the rate of people living in extreme poverty has declined over the years or showing how infant mortality has gone down are showing a map of the world and the incidence of malaria and you know, 1990 versus today, and they're just very compelling, and they tell a story in an instant. They don't go as viral as Trump memes, but among sort of thoughtful, intellectually curious people, I think they care a lot.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 21:07&nbsp;&nbsp;</p><p>And so one question from the audience, which I think is good, despite on this topic is about what about the idea of an EA comedy show, like a sketch show? Because I actually think there's so many good examples of EA thinking through comedy because it has that tension between like, there's this argument there, but it's also transgressing a taboo often. So one example of this is David Mitchell, a British comedian had a sketch called Not Enough Drownings, which was about how in this very large area of the UK, no one drowned. And he was saying, that's very bad allocation of resources if we're putting so many resources into, you know, getting one drowning down to zero, then, like, think of what that money could have done.</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 21:55&nbsp;&nbsp;</p><p>Like, if people never drowned in pools, then you're not swimming enough.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 22:00&nbsp;&nbsp;</p><p>Okay. Yeah, actually, kind of similar idea. I mean, do you think there's actually potential there?</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 22:07&nbsp;&nbsp;</p><p>It depends on who you want to watch it. And that's actually, I think, in all seriousness, an important thing to think about anytime you're talking about communicating on media, because on the one hand, preaching to the choir, it's thought of as not a good thing to do. But on the other hand, I mean, you were just talking Will, in your opening talk about, you know, the strength of a community. And a community's continual bond often comes from that kind of preaching to the choir of like, let's have you know, whatever it is a weekly comedy show, where we all get to laugh together about the things that we agree about, I don't think that's necessarily bad. I would guess you're not going to reach across the aisle, so to speak, and like, pull people who wouldn't otherwise agree with you to laugh at those kinds of things. But there could really be benefit to the people who are already on board laughing together.</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 23:12&nbsp;&nbsp;</p><p>And I think actually, it could be pretty effective. Like what we were just talking about, trying to cover a slow-motion disaster, like a fast crisis, where we talk about these killer missiles that are attacking 1000s of kids, if you do a video series on a bunch of different real crises, and do it in the style of, you know, CNN, I think that could be very funny. And, and that would get the message across not just for the people who already believe it, but be like, oh, yeah, that's true. Like, why are we focusing on a car crash when there are 1000s dying?</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 23:58&nbsp;&nbsp;</p><p>Maybe a good example of what you're talking about comes from Monty Python. There was this running thing they did and now for something completely different are the what's it called? Monty Python's Flying Circus. It was the \"Parrots News Network\", where they presented world news like, you know, an ocean liner crash, so there was a hurricane. And then they would always say no parrots were injured. I don't know how many people made this connection. But for me, the connection was like, you know, in the US when we cover news in the world, the main emphasis is always like we're Americans, like how did this impact us as Americans, but it like looks silly when it's a parrot but you know.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 24:42&nbsp;&nbsp;</p><p>Okay, so changing tack a little bit. So, Joe, you mentioned Well, one solution is maybe you get, do all the research, you come to certain conclusions, and then you just storify them. And I've heard at least from people I respect that -- but I don't know this is true --but at least they've set it to be true that Deep Impact and Armageddon had a really big impact on US policy with respect to near-earth asteroids, where they dramatically increase the amount of spending monitoring near-earth asteroids. So, you know, these two asteroid movies seem to have a great impact in terms of existential risk mitigation. And other people, [talking to Joe] you've told me a bit about Participant Media, which are, for example, the film Contagion where it was a socially motivated film to try and get people wanting a bit more about mass pandemics. Do you think we could do more of this? Do you think it's potentially impactful? How would we know? Could we do the same for AI?</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 25:41&nbsp;&nbsp;</p><p>I mean I'd be really curious to know how true that really is if those two Hollywood movies really had that influence and I would love to hear that they did because that would make me feel much more important for doing the work that I do. But, um, yeah, you mentioned Participant Media. So they're a company that their whole mandate is that they make movies that are supposed to have a positive social impact. And they've done movies that are very commercially successful, but they're also backed by someone who made this money. You know, Jeff Skoll, who made a lot of money because he was the first president of eBay. And he makes Participant Media happen. Now, some of their movies do well. But I don't know that in a purely market-driven production company you'd be able to do movies like that. But maybe, you know, combined with the sorts of philanthropic efforts that you're talking about, you could. I would also just tend to think that I think that stories can maybe have more impact when they're positive rather than negative like warnings. I find often like a warning in a movie or something might have the effect of trivializing the thing you're warning about just as much as really getting people concerned. Whereas showing just something positive, like for example,&nbsp; a simple, very middle of the road, mainstream sitcom like Will &amp; Grace -- it's the first sitcom to have a main character who was gay -- I think that probably did have an impact on marriage equality laws coming into effect some 10 years later. I don't know how you would measure that.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 27:48&nbsp;&nbsp;</p><p>So apparently, the film Babe resulted in a very significant drop in demand for bacon.</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 27:54&nbsp;&nbsp;</p><p>So that's interesting. Yeah, see, nicely. Pigs.</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 28:01&nbsp;&nbsp;</p><p>And Nemo, my kids told me, Nemo resulted in people buying a lot of clownfish. I don't know if that's good. But I think I love Joe's point about the positive, because I do find, you can definitely guilt people, that's pretty easy. If he could do a thought experiment that you're basically murdering people by having a frappuccino every morning instead of giving it to charity. And I do believe that's true.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 28:33&nbsp;&nbsp;</p><p>That's the EA message you want in the media</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 28:37&nbsp;&nbsp;</p><p>But it's a little off-putting. So I prefer your imagery in your book where you said, you know, everyone can be a hero. Everyone can be Oskar Schindler. Everyone can rush into a burning house, just by contributing to the right. So that I find more appealing, at least to me, I don't know what the evidence shows.&nbsp;</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 28:56&nbsp;&nbsp;</p><p>So I think yeah, the world of Hollywood is something that probably like I certainly know, basically nothing about. And I think that will be true for many in this audience. You know, we often think about what communities are potentially receptive to these kinds of ideas. Do you think that we should be trying to do outreach among the kind of people in Hollywood trying to get people you said there's a lot of good intentions? Should we try and, you know, steer them in this in a more EA direction if we can or not?</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 29:29&nbsp;&nbsp;</p><p>I would actually say yes. And I know I sort of started out by being sounding pessimistic. But I do think that and especially now, frankly, like, since the new president, there's probably more appetite in Hollywood for not just for activism, and but there is that but also for particularly, you know, more thoughtful, more rational thinking activism more rationally thoughtful activism. And yeah, I think that there probably would be receptivity there.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 30:11&nbsp;&nbsp;</p><p>Okay, great news. And so, yeah, what would you think then about, again, a couple of people have been asking in the audience, like, there are lots of different facets to kind of EA, what if we are going into kind of more mass media and trying to convey this message, what are the two or three things that we should be trying to distill EA down to? Because certainly, in my experience with dealing with the media, they just want to simplify everything differently, make it more controversial as well, what's the small number of messages that you think are kind of most core to the message?</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 30:51&nbsp;&nbsp;</p><p>Well, I mean, I was thinking on the way over about the different parts of EA, and I broke it down into three. And that was to give to the right cause, like AI or global poverty, to give to the right charity within that cause that's most effective. And also to give more money in general, just up the percentage you give. And I actually think to give more is the is really the easiest because the others you have to think about for a little, but like, the fact that everyone should give more money, that's pretty basic, pretty. So I don't know, those to me are the three keys to get across.</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 31:41&nbsp;&nbsp;</p><p>Here's something I've been thinking about recently in terms of EA messaging. So the original model, or like, a sort of default model of communication about EA is you should be giving more money to important EA causes. And then I think people know, advocates noticed that there was often a lot of pushback where people felt like, \"Oh, you're telling me I can't have my latte in the morning,\" or \"I'm a murderer.\" And they started modifying the message into this like two bucket model where like, look, in this bucket, you're spending money on yourself, on luxuries, you can have your latte, you can take vacations, but then like when you do give to charity, without the other bucket and EA, like our thesis is that there's like much better ways to get to charity than you know, a lot of people are currently doing. I think that there that the right model is actually a three-bucket model, which is maybe going against your simplify everything message. But so it seems to me now that what we want to say is like, okay, yeah, you have your first bucket, that's like the stuff you do for yourself, that's fine. Your second bucket is charitable giving or sort of giving, that you're doing an altruistic week for stuff that you just personally care about, like, you know, your local community center, you're like local arts organization, maybe the college you went to, if you care about that, or an animal shelter, and that's fine. That's you giving stuff that matters to you. And then the third bucket is helping the world. And that third bucket is what EA is really about. The third bucket is where EA can create a dramatic improvement in the effectiveness of people's giving both by helping them pick effective charities or nonprofits or charities but also think about which causes are gonna have the greatest impact which causes are important and neglected and tractable, etc. And I think my current intuition, I could be wrong about this is that separating those second and third buckets reduces that that like reflexive reaction that people have, where they have to defend why, you know, giving to their college is actually the most effective thing for the world. And they sort of tie themselves in knots because they feel like, you know, they're their own giving us the challenge and maybe will disagree with me. But my guess is that separating those buckets, makes the message like it reduces the barriers of the message.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 34:13&nbsp;&nbsp;</p><p>I mean, that's certainly how I convey things when I talk to families. Yeah, I actually have more buckets than you.</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 34:21&nbsp;&nbsp;</p><p>I'm guilty of having three.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 34:23&nbsp;&nbsp;</p><p>Well, let's not go into too much. Even within the second bucket, there are different reasons like one is reciprocity, like to your alma mater, another is like some, you know, part of your identity, maybe you give to an LBGT rights group because it's just kind of defining kind of who you are. The third is like, well, I have a personal connection. So even within that, like giving for things you like, there are various buckets, as well. And yeah, you can often you know, buy those things, they will be cheaply. So like 5% of your spending goes in that 95% on helping the world but that is hard once you've got like six buckets that's in between. Joe, did you have anything to say that?</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 35:06&nbsp;&nbsp;</p><p>Well, sure, I guess I would focus. Well, and maybe just for the sake of variety, focus less on giving money. Because I think generally, especially if we're talking about communicating via the media, and some stranger who doesn't know anything about EA, just hearing about it, if you start out by this is going to be about how you should give money to something. I think you're probably going to just turn a lot of people off right away. One of the things that I find the most appealing and I wonder how, if there's a way to like distill it down, is just thinking rationally. And I think there is, again, I was sort of touching on this a second ago, there is a real appetite nowadays, for that, in light of what's going on in Washington, in light of, I think a lot of people are sort of sick of the kind of meme culture and sort of the echo chamber effect. People talk about that a lot. And maybe if you could sort of just present the antidote to all that stuff is rational thinking. And that's sort of a soundbite version of it. But you know, rather than just adhering to, you know, one side or the other, how do you know what news is fake news, etc. There actually is a way to parse some of this stuff. And it started in the Scientific Revolution 500 years ago, and it's the pinnacle of humanity. And like, this is what EA is about. It's thinking, thinking rationally about, you know, what you're going to do with your life and your job, or what you're going to do with your money, et cetera, et cetera. But what we're really here to promote is, let's, let's think.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 36:54&nbsp;&nbsp;</p><p>Yeah, I really hope we are according to that.</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 36:57&nbsp;&nbsp;</p><p>My innermost thoughts come out of Joe Gordon-Levitt.</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 37:03&nbsp;&nbsp;</p><p>I just took that from one of your YouTube videos.</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 37:07&nbsp;&nbsp;</p><p>When I was writing my article on effective altruism, I tried and struggled with how to get across in like, an elevator pitch, what it is, and I don't know if I hit on the right metaphors, but one that resonated with some people was Moneyball for saving the world, like safer. And another one was like, if, if Mother Teresa and Spock had a baby, that would be effective altruism, although I know some people think --</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 37:42&nbsp;&nbsp;</p><p>Well, Julia is gonna object she has a whole talk on The Straw Vulcan.&nbsp;</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 37:46&nbsp;&nbsp;</p><p>And Mother Teresa. Yeah.</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 37:54&nbsp;&nbsp;</p><p>Moneyball is a really good example, though. I hadn't thought of that. But that's exactly what that movie is about. Yeah. I don't know if you guys have seen that movie. But it's really that's a really great movie for that.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 38:04&nbsp;&nbsp;</p><p>Okay, so I think the final question, then again, coming from the audience is just what, like, single or a couple of bits of the kind of most actionable advice would you have both for individuals in the room who are, you know, engaging in the media or trying to spread these messages, whether general EA or specific cause areas? And second for the sorts of organizations like the Center for Effective Altruism, and GiveWell, Open Philanthropy? If you give them one piece of advice, what would that be? And we can go, AJ, you can start.</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 38:33&nbsp;&nbsp;</p><p>I can start. Oh, I guess one thing I find effective, again, is not stressing the negative and the guilt but stressing the positive. And also the idea that everyone, you know, the media is very splintered, of course, but there are some things that go across the aisle, like self-help, for instance. Everyone wants to feel good. So stressing the idea that you can get that warm glow, there's all this science about how giving and helping the world makes you feel better. So maybe trying to focus on that, focus on how this can help you, the reader as opposed to just helping their world.</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 39:24&nbsp;&nbsp;</p><p>Oh, well, I talked earlier about this failure mode of wanting to jump right to the really intellectually-interesting-to-you topics. And that's a mistake. I personally have made many, many times. Another mistake that I've made many times that I suspect other EAs have or Will make is -- so I would write these blog posts or give these interviews. And I would think to myself, \"Oh, I don't want people to misunderstand me and think I'm saying X. So let me add a line in there.\" That's like, just to be clear, I'm not saying X. And then the comment section was full of people saying how can you say X. And, you know, the lesson is just kind of to Joe's point earlier, you have to say something again and again. And you have to say it loudly and in a way that's sort of salient to people that sticks. And you know, I think you also have to kind of, not just claim that you're not saying X, but sort of act and speak in a way that someone who didn't believe X would act and speak in. That's kind of an abstract thing.</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 40:38&nbsp;&nbsp;</p><p>\"I am not a crook\" is a classic example. Richard Nixon said I am not a crook, everyone thought he was a crook.&nbsp;</p><p><br>&nbsp;</p><p>AJ Jacobs&nbsp; 40:44&nbsp;&nbsp;</p><p>Or saying, I am the most humble man in the world.&nbsp;</p><p><br>&nbsp;</p><p>Julia Galef&nbsp; 40:49&nbsp;&nbsp;</p><p>I mean, I was thinking of things where it's reasonable. Okay, I'll just give you an example. If you're like criticizing a government program an intervention that was, you know, ineffective, for example. A lot of people are gonna read that and kind of assume like, Oh, they're a libertarian, they just like think the government should be small. And maybe that's not what you think, and not the impression you want to leave people with. But because people who tend to criticize government programs also statistically tend to dislike big government, then they're going to draw that assumption. So you kind of have to try to predict what assumptions or inferences people will make about you and your values based on what you literally said, and then sort of go farther than you think you need to go to erase that assumption. Because people, people reading my blog post, they're not reading it that closely, they may not even see your parse, that line that I put in there to try to protect myself. And they also have these priors that you really have to overcome, strong priors about what someone like you believe. And I think this is something that EA struggles with, that we kind of, in some ways, we kind of pattern match to like the current villain in the media, which is like neolibertarian tech bro, who wants to, like remake the world in his own image and isn't going to bother with, traditional institutions like democracy. And this is, like I see crazy, like, absurd critiques of people fitting that has been fit into or who themselves fit that stereotype. Like there was an article criticizing Mark Zuckerberg for spending money on cancer research. And the journalist was like, this is terrible. Because if you save old people, then the world will be overpopulated. And I was like, first of all, that's kind of a horrid thing to say, but also where were you all these years, but NIH and NSF were funding cancer research. It's only now that a tech bro is funding cancer research, that it's a bad thing. And I see the same thing with EA where, you know, because we're analytical, a lot of us are like software engineers or in finance, we tend to sort of have a similar like, with cleverness, we can do better than the existing wisdom, or the existing institutions. And that's shared by the, like tech pro archetype. Anyway, so there's some pattern matching happening there. And I think you have to kind of go out of your way to overcome that prior.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 43:14&nbsp;&nbsp;</p><p>Okay, Joe?</p><p><br>&nbsp;</p><p>Joseph Gordon-Levitt&nbsp; 43:15&nbsp;&nbsp;</p><p>Yeah, I guess, one piece of advice that just always think about if you're going to communicate via especially mass media of any kind is, know your audience know who you're speaking to. And especially, I think, in the case of EA, there are maybe two categories to think about. There are the folks who are already into thinking rationally and kind of into the kind of mindset that you talk about the scientific mindset that you mentioned in your opening talk. And if that's who you're talking to, great, then you can use, you know, that kind of rational argument. But if you're not talking to someone like that, if you're talking to a probably what would be a larger audience, I think you probably have to accept that that kind of rational argument isn't going to work. And you have to tell stories. You have to tell emotionally-based, you know, human interest stories, and those aren't necessarily bad. I happen to love those. I spend my life telling them but I think there are probably ways that you can tell stories that are pertinent to what EA is about. But you have to do it that way or else. No, no. Those folks won't listen.</p><p><br>&nbsp;</p><p>Will MacAskill&nbsp; 44:33&nbsp;&nbsp;</p><p>Oh, okay. Well, yeah, let us all thank our panelists here. This was a really fun and interesting talk.</p><p><br>&nbsp;</p><p>Thank you.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "WBEvtacARie6PuHr4", "title": "Tom Kalil: Policy entrepreneurship for EAs", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=wWt8B736pSw&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=29\"><div><iframe src=\"https://www.youtube.com/embed/wWt8B736pSw\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "ypoMcaBPTbMmy8jKD", "title": "Kerry Vaughan, Rachel Atcheson, & Matthew Johnson: Marketing EA", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=gEr1Umvlhzo&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=28\"><div><iframe src=\"https://www.youtube.com/embed/gEr1Umvlhzo\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 panel, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "QTXJzmjYQEJ2vXS4j", "title": "Kristian R\u00f6nn: Global challenges", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=St8BbOWk6SI&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=27\"><div><iframe src=\"https://www.youtube.com/embed/St8BbOWk6SI\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "5x8Qhy4bB8Arj5SBS", "title": "Ruair\u00ed Donnelly: Moral trade", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=RoomSp0THJs&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=26&amp;t=1s\"><div><iframe src=\"https://www.youtube.com/embed/RoomSp0THJs\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>A moral trade occurs when individuals with different values cooperate to produce an outcome that's better according to both their values than what they could have achieved&nbsp;individually.</i></p><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "prk2spv7DE7kRW4Xb", "title": "Claire Walsh: Embedding EA thinking in government decisions beyond the OECD", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=HbAd-hrgFWs&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=25\"><div><iframe src=\"https://www.youtube.com/embed/HbAd-hrgFWs\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "qZN4opfZs7iZfJkY6", "title": "Michael Page, Dario Amodei, Helen Toner, Tasha McCauley, Jan Leike, & Owen Cotton-Barratt: Musings on AI", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=JA4vW4oQavk&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=24\"><div><iframe src=\"https://www.youtube.com/embed/JA4vW4oQavk\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 panel, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "E9vnYhdheNDJSkLeX", "title": "William Marshall: Lunar colony ", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=Ei9qPiWJRqk&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=23\"><div><iframe src=\"https://www.youtube.com/embed/Ei9qPiWJRqk\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "98ZKhgiDmictAs5sS", "title": "Jose Valle, Georgia Ray, Julia Wise, Malo Bourgon, Jonathan Courtney, Sebastian Joy, & Kelsey Mulcahy: Lightning talks", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=dfGZVWTkUMY&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=22\"><div><iframe src=\"https://www.youtube.com/embed/dfGZVWTkUMY\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for these EA Global: San Francisco 2017 lightning talks, but we haven't created one yet. If you'd like to create a transcript for these talks, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "mvkKjYWWKZuK6ZBXM", "title": "Joan Gass, Bruce Friedrich, Svetha Janumpalli, Spencer Greenberg, & Eric Gastfriend: EAs in entrepreneurship", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=ZSy9hHv2YZ4&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=21\"><div><iframe src=\"https://www.youtube.com/embed/ZSy9hHv2YZ4\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 panel, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "8QN2om65eYHmAe2Lz", "title": "Spencer Greenberg: Social science as a lens on effective charity \u2014 results from four new studies", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=tOSpj19eows&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=20\"><div><iframe src=\"https://www.youtube.com/embed/tOSpj19eows\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we plan to post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "esimLMarooAyXh9jR", "title": "Jose Valle: Virtual reality for promoting empathy", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=DjJcWiwpZn4&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=19\"><div><iframe src=\"https://www.youtube.com/embed/DjJcWiwpZn4\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "mCSmr6LNNQD4AKj4z", "title": "Kerry Vaughan, Nick Beckstead, and Lewis Bollard: EA Funds", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=QudFciXt6L0&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=18\"><div><iframe src=\"https://www.youtube.com/embed/QudFciXt6L0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "xwDG64qAjGjQeXwfd", "title": "Nick Beckstead: EA community building", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=bDhl22Qv-Qg&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=17\"><div><iframe src=\"https://www.youtube.com/embed/bDhl22Qv-Qg\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this 2017 talk, the Open Philanthropy Project's </i><a href=\"http://www.nickbeckstead.com/\"><i>Nick Beckstead</i></a><i> discusses their grants supporting the growth of the effective altruism community. He also analyzes the nature of the effective altruism community, and discusses what sorts of work it needs more and less of. At the very end you will find an added section on what you can do to&nbsp;help.</i></p><p><i>The below transcript is lightly edited for&nbsp;readability.</i></p><h1>The Talk</h1><p>I'm Nick. I'm from the Open Philanthropy Project, where I\u2019m a Program Officer. Open Phil is a philanthropic foundation which uses effective altruist principles to guide our grant-making. I want to talk a little bit about my role as a Program Officer supporting the growth of the effective altruism community, discuss the goals that I have as the person who's guiding Open Phil\u2019s grant-making on that topic, discuss some of the grants that we have made on that topic, and some of the grants we might make in the future, and some of the suggestions that I have for the effective altruism community that have arisen from my experience with it in the process of thinking about that&nbsp;work.</p><p>I think of my main objective here as empowering the effective altruism community. That requires basically having good relationships with people in this community, understanding what is going well and badly in the community, what the community wants from Open Phil, what needs it has, and how Open Phil can help it to thrive. Then my goal is to recommend grants that are responsive to all of those, and all of the community's&nbsp;needs.</p><p>When I think about how to characterize the effective altruism community, you can do it by demonstration, like looking at these people who show up at these conferences and talk about these things, and have been doing so since maybe 2007 or 2009, depending on where you want to start counting it from. It\u2019s nice to think about what is special about this community and this set of people, why this is an interesting group to be thinking about&nbsp;supporting.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/15X2JokzHmo8gu0mA0CS0/e604d666a02d1b093ee24057a590ff68/Intellectual_flavors.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/15X2JokzHmo8gu0mA0CS0/e604d666a02d1b093ee24057a590ff68/Intellectual_flavors.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/15X2JokzHmo8gu0mA0CS0/e604d666a02d1b093ee24057a590ff68/Intellectual_flavors.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/15X2JokzHmo8gu0mA0CS0/e604d666a02d1b093ee24057a590ff68/Intellectual_flavors.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/15X2JokzHmo8gu0mA0CS0/e604d666a02d1b093ee24057a590ff68/Intellectual_flavors.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/15X2JokzHmo8gu0mA0CS0/e604d666a02d1b093ee24057a590ff68/Intellectual_flavors.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/15X2JokzHmo8gu0mA0CS0/e604d666a02d1b093ee24057a590ff68/Intellectual_flavors.png?w=1800&amp;q=70 1800w\"></figure><p>I guess the things I would say is it has a distinctive set of values and a distinctive set of norms around thinking. Some things that you could call \u2018intellectual flavors\u2019 that I've listed on this slide, which aren't deep commitments of the community or anything, but I feel like they tinge most of the discussion that happens here. You can also think of the community and characterize it, as is perhaps most commonly done, in terms of the sets of issues that it&nbsp;prioritizes.</p><p>When I speak of helping the effective altruism community, this is in addition to the list of grantees that I'll discuss later. This is what I mean. I think a lot of these things are pretty rare and pretty interesting and valuable. When I think about the grants that Open Phil are making in this area, I guess a question is, why make them? What would constitute success? I think the thing that's highest on my list right now is recruiting and identifying talent to work on the set of issues that's prioritized by the effective altruism community. I would go back to the standard list of EA causes: global development, animal welfare, transformative technologies, and global catastrophic risks, especially&nbsp;AI.</p><p>I think there are a number of other major benefits, but I think recruiting and identifying talent for those things is the way, if I imagine looking back on the grants that Open Phil has made in this area over the coming years, the way that I would most anticipate seeing a lot of success would be if we had brought people in to do valuable work on those areas. That could include strategic analysis to figure out which parts of these topics are most important to prioritize and in what ways, and also just doing valuable work on them, especially AI and global catastrophic risk&nbsp;missions.</p><p>I should say I've lumped some of the organizations that work on global catastrophic risks and risks from artificial intelligence into this portfolio because there's just a lot of overlap in the networks and trust relationships, and those are areas that I prioritized pretty highly in thinking about grants to make in this&nbsp;field.</p><p>I think there are some longer-term objectives that could be hoped for as well, such as shifting intellectual culture. You could imagine a world in which some of these core EA concepts become more popular and shape intellectual discourse in universities and permeate things, instead of just being something that all the people who show up at conferences like this are inclined to think&nbsp;about.</p><p>I also think a little bit about providing funding to impactful organizations. That's certainly been a major thing that the effective altruism community does, but it's not really a major focus of the grant-making that Open Philanthropy is doing in this area at this time. I'll say a little bit about that later in terms of the reasons for&nbsp;it.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/5UuYUiqfpCccUIG6cQIowM/3476d9f0fb46e7178641adeca5257d92/Grants_so_far.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/5UuYUiqfpCccUIG6cQIowM/3476d9f0fb46e7178641adeca5257d92/Grants_so_far.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/5UuYUiqfpCccUIG6cQIowM/3476d9f0fb46e7178641adeca5257d92/Grants_so_far.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/5UuYUiqfpCccUIG6cQIowM/3476d9f0fb46e7178641adeca5257d92/Grants_so_far.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/5UuYUiqfpCccUIG6cQIowM/3476d9f0fb46e7178641adeca5257d92/Grants_so_far.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/5UuYUiqfpCccUIG6cQIowM/3476d9f0fb46e7178641adeca5257d92/Grants_so_far.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/5UuYUiqfpCccUIG6cQIowM/3476d9f0fb46e7178641adeca5257d92/Grants_so_far.png?w=1800&amp;q=70 1800w\"></figure><p>These are the grants. You can think about a foundation at the end of the day in terms of the grants that it's making, and the grants that have been made so far through Open Phil to the effective altruism community are listed as follows. These were all made in the last year or so. So 80,000 hours in the Center for Effective Altruism, Center for Applied Rationality, SPARC, Founders Pledge, and ESPR (previously called EuroSPARC). We've written about all of these on our grants pages. I don't think I'll go into a lot of them much here because you'll be running into these organizations, but this is the core set of grants we've made over the last year in the EA&nbsp;community.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/6tR4jdLFuMEYguQKCwCoYk/576ec2d39005b2573f75be8ce5cacdbb/AI_grants_so_far.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/6tR4jdLFuMEYguQKCwCoYk/576ec2d39005b2573f75be8ce5cacdbb/AI_grants_so_far.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/6tR4jdLFuMEYguQKCwCoYk/576ec2d39005b2573f75be8ce5cacdbb/AI_grants_so_far.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/6tR4jdLFuMEYguQKCwCoYk/576ec2d39005b2573f75be8ce5cacdbb/AI_grants_so_far.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/6tR4jdLFuMEYguQKCwCoYk/576ec2d39005b2573f75be8ce5cacdbb/AI_grants_so_far.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/6tR4jdLFuMEYguQKCwCoYk/576ec2d39005b2573f75be8ce5cacdbb/AI_grants_so_far.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/6tR4jdLFuMEYguQKCwCoYk/576ec2d39005b2573f75be8ce5cacdbb/AI_grants_so_far.png?w=1800&amp;q=70 1800w\"></figure><p>Then there are some grants that I would say are more focused on potential risks from advanced AI and global catastrophic risks. The grantees here include the Future of Humanity Institute, Future of Life Institute, the Machine Intelligence Research Institute, and Robin Hanson at George Mason University. I won't get too much into the rationale for these things. We've written about all these grants on our&nbsp;website.</p><p>So why is additional funding a secondary priority for grants that Open Phil is making in this space? Well, I think the effective altruism community focused on this quite a lot in the past and has had a fair amount of success with it, and I think as a result lots more funding is available for the causes that the effective altruism community cares about the most. That's been a change in the situation, and I think it's caused me, as a Program Officer, to think about what this field needs, and it's caused a change in my view about what is most important for us to&nbsp;do.</p><p>I would love to see a world where, when people are talking about effective altruism, the focus is a little bit less on philanthropy as such, and is more broadly about what people can do with their lives in order to contribute to the world and most effectively make a difference. I see in many ways more potential for this community to make a big difference through actions in that kind of category. Anyway, that's a response to a change in the situation as I see&nbsp;it.</p><p>Then the other reason that I\u2019ve changed my mind on this is just a change in view about how the world works, particularly in terms of allocating diligent attention towards some vision, and allocating funding in order to solve problems in the world. I used to have a view that I would now describe more like na\u00efve microeconomics, where accomplishing good with money is like, say, buying a commodity in the world, where if you had a mission of having there be more cars of a certain type made, then really there's a very efficient way to translate money into cars. You have some verifiable specification of what this kind of car is and you have people who manufacture these cars, and if you just communicate to them exactly what you want, and you can really get quite a lot of&nbsp;cars.</p><p>I think that philanthropy, and especially in some of the more opaque and difficult to communicate areas that are important to the effective altruism community, is really not that similar to a commodity. I think that there are very large transaction costs, and it's very difficult to communicate exactly what the vision is for these different areas. As a result, I think it can be very difficult to scale by just bringing more money into the field and having more people than we already have working on earning to give type strategies. That's really an update to my view, and an input to a change in how important I think it is to bring in additional funding to the effective altruism&nbsp;community.</p><p>Partly in response to thinking about the grants that we've made over the last year or so, if I were going to summarize the theme of these grants I would say it's something like this: New funder enters the space. When the new funder enters the space, usually the easy thing to do is to look at who's doing work that resonates with their goals, who's doing the work that they are most excited about. We then work with those people to get them funding to do their shovel-ready projects, and expand them in a way that makes sense to the funder. It's more community-led in that way, and that's grant-making over the last year or&nbsp;so.</p><p>In the future I'm thinking about this constraint, as I see it in the effective altruist community, which is something like this: engaging people who have a deep understanding of the important visions that inspire this community, and getting them working on innovative projects carry those visions forward more. It's really about talent identification and recruitment to these problems, as I see it. I'm thinking about things for Open Phil to fund in this area, and here are a few priority&nbsp;areas.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.contentful.com/ohf186sfn6di/13liAbMD0KOSYii64usGSa/04f7382cbb481d4cb8bbd727062a7b17/Possible_areas_to_fund.png?w=100&amp;q=50\" srcset=\"https://images.contentful.com/ohf186sfn6di/13liAbMD0KOSYii64usGSa/04f7382cbb481d4cb8bbd727062a7b17/Possible_areas_to_fund.png?w=100&amp;q=50 100w, https://images.contentful.com/ohf186sfn6di/13liAbMD0KOSYii64usGSa/04f7382cbb481d4cb8bbd727062a7b17/Possible_areas_to_fund.png?w=480&amp;q=70 480w, https://images.contentful.com/ohf186sfn6di/13liAbMD0KOSYii64usGSa/04f7382cbb481d4cb8bbd727062a7b17/Possible_areas_to_fund.png?w=768&amp;q=70 768w, https://images.contentful.com/ohf186sfn6di/13liAbMD0KOSYii64usGSa/04f7382cbb481d4cb8bbd727062a7b17/Possible_areas_to_fund.png?w=992&amp;q=70 992w, https://images.contentful.com/ohf186sfn6di/13liAbMD0KOSYii64usGSa/04f7382cbb481d4cb8bbd727062a7b17/Possible_areas_to_fund.png?w=1200&amp;q=70 1200w, https://images.contentful.com/ohf186sfn6di/13liAbMD0KOSYii64usGSa/04f7382cbb481d4cb8bbd727062a7b17/Possible_areas_to_fund.png?w=1800&amp;q=70 1800w\"></figure><p>One of them is just fellowships providing education and training for community members who want to enter relevant fields. I think Will mentioned earlier in his talk the importance of increasing specialization as this community grows, and I think there are a number of fields where I would love to see a bit more of that. I'll talk about the details of that later. I'm thinking also a bit about recruitment in undergraduate communities and I'm also thinking a bit about research we could fund with an EA lens in academia. That's a little bit more of a nascent&nbsp;topic.</p><p>On that note, to summarize a bit in terms of suggestions for the EA community, I have said that I think we need less of an emphasis on earning to give in the community right now. I think people have been hearing that message for a while over the last couple of years, but I just thought I'd repeat that a little bit, and think about it and state my reasons for it. I would like to see more people in our community getting involved with areas where we don't have as much deep expertise and activity. High on my priority list there are areas in AI strategy. I think 80,000 hours have a very nice page explaining the need for that area and giving a bit of flavor for the questions that are involved there. Also expertise and machine learning for technical AI safety&nbsp;work.</p><p>I would really love to see a lot more people in the effective altruist community getting involved in biosecurity. I think that is a smaller potential global catastrophic risk than AI, but I think it's an important one, and I think it's an area where people from this community, a lot of them could plausibly make substantial contributions. It currently doesn't receive very much attention. There are also other roles in the US government, a variety of them that I would love to see EA's pursuing. I think Jason Matheny is a great example in his role as director of IARPA of what can be accomplished in that domain. I'd love to see more people in this community get expertise in biology and economics. That's part of the reason I mentioned funding, the possibility of Open Phil considering funding fellowships to get people in this community into roles like&nbsp;those.</p><p>More generally I think given the difficulty of communicating nuanced visions and deep context, I think what we want in terms of our outreach should be outreach plans that find the people who deeply understand the core ideas of effective altruism, especially people who want to provide full time attention and effort, who are needed to implement and refine some of these strategies that I think are the most important for the success of the community. Those are the main things that I wanted to say. I think I'll wrap up there and then we'll go for the chat.&nbsp;Thanks.</p><h1>Q&amp;A</h1><p><strong>Question:</strong> Thanks for the talk. I wanted to start by asking if you could expand a bit on what you're talking about with getting EA and academia a bit more synced up. How do you think EA should relate to&nbsp;academia?</p><p><strong>Nick Beckstead:</strong> Okay, well, that's a big question, but the thing I had in mind is something like this: I think there's something powerful that can happen when ideas are traced back to their foundations and deeply explained, and there are people in a field who are working within a paradigm. I think EA has a certain lens and set of questions that it asks when looking at the world. I can imagine a world in which people, perhaps most immediately in philosophy and economics departments, were thinking about questions that are of particular interest to the effective altruist community, and in which that scene is a more central part of their&nbsp;disciplines.</p><p>For example, there are certain views that many effective altruists have about the long-run future that are not standard orthodoxy and even clash with standard orthodoxy in economics, say about discount rates or how you're doing population ethics. You could imagine a world in which you had some economists who are thinking about some of these problems, these same questions, and using more similar assumptions to the effective altruist community. I could imagine good things happening from that&nbsp;shift.</p><p>I don\u2019t know. It's not a great answer exactly, but I think what I mean is somebody who is in one of these fields taking questions of substantial interest to the effective altruist community, working on them and tracing out their intellectual&nbsp;foundations.</p><p><strong>Question:</strong> I guess another model of how EA could relate to academia is setting up EA as an academic field in itself or something. It sounds like you might be more enthusiastic about interacting with existing&nbsp;fields.</p><p><strong>Nick Beckstead:</strong> I wouldn't say that necessarily. I think those are both very interesting kinds of opportunities. Open Phil supports the Future of Humanity Institute and in some ways they're technically part of the philosophy department in Oxford, but a lot of the papers that they're writing are not addressing central preoccupations of some other field. And yet I think there are important questions that someone should be thinking about rigorously. In some ways that's EA and academia, but it's not really trying to be prestigious according to the norms of some other discipline. I think both are interesting in their own&nbsp;way.</p><p><strong>Question:</strong> Thanks. One thing someone asks is what can we do to build a more effective community outside of the Bay Area? I'd be interested in how you might go about doing that, but also how important is online interaction versus meeting in person and creating hubs for the community in your&nbsp;view?</p><p><strong>Nick Beckstead:</strong> I'm not sure I have anything very systematic to say about that. I think online interaction is very important and in-person interaction is very important and they play different roles. How do you create effective hubs? I think that is a bit mysterious to me, so I won't attempt to answer&nbsp;that.</p><p><strong>Question:</strong> Going back to our previous discussion about interacting with philosophy and economics, someone asks what areas or questions of economics more specifically would you consider relevant for&nbsp;EA?</p><p><strong>Nick Beckstead:</strong> When I think about academic research, there are different types of value that you could hope to derive from it. One kind of value you could hope to derive from it would be: this is brand new information or knowledge that no one has and it's super important. There's another category which is more like this: if you think about EA type research, there's some knowledge that maybe a lot of the most engaged members of the EA community have strong opinions on, and I would be willing to bet they're right, but it hasn't really been traced back to its foundations and explained very well, or it hasn't been explained in the language of a certain discipline. Then there would be value if it were explained better, there would be more people who might think about the problem with that kind of framework and lens. That\u2019d be another kind of&nbsp;category.</p><p>The stuff I was saying about discount rates would be something I\u2019d put in the second category, where the EA community has worked out views on this that I think are right, but they're not the orthodoxy in economics, and there's a lot of nuance to it. I don't think all the nuance has been worked out exactly. That would be a simple example of&nbsp;something.</p><p>I think my mind often goes for the far future-y \u2018fate of the world\u2019 type questions because that's where I spent my time thinking as a researcher in academia more than anything else. I guess in that domain I see less in terms of category number one, where the economist could use the standard tools and frameworks of economics to obviously help a lot in a way that I can anticipate. I don't feel like I could tell you what the questions would be that would be really useful. You had a team of awesome economists working on those kinds of questions. But I can think of some in the second&nbsp;category.</p><p><strong>Question:</strong> I've heard some discussion of perhaps EAs going and looking more into how, say, the market for meat works, which might be a bit more in the first category where we can use standard tools of economic analysis to assess an&nbsp;issue.</p><p><strong>Nick Beckstead:</strong> Sure. Also it depends on how broadly you want to cast the lens of economics. There's a bunch of related questions that I would lump under the suggestive title of \u2018improving the reasonableness of institutions and broad functionalness of society\u2019 - a lot of Phil Tetlock\u2019s work is in that category. There could be field trials of ideas in that genre that I think would be valuable for a number of causes, including EAs who are interested in \u2018fate of the world\u2019 type&nbsp;stuff.</p><p><strong>Question:</strong> I have a slightly more critical question, which is how suspicious is it that the people working for EA orgs think that funding EA orgs is a top cause&nbsp;area?</p><p><strong>Nick Beckstead:</strong> I would say it's not incredibly suspicious. I think a lot of people who run organizations and ask for money say that about their organization or their field. Or maybe they wouldn't have an opinion about it, but they act as if it were true, which is similar. I think we should just assess it on its merits. But yeah, it would probably be a mistake to just believe that on a trust&nbsp;basis.</p><p><strong>Question:</strong> Maybe part of the problem is that a lot of people who are assessing it on its merits also work for EA orgs or are closely related to&nbsp;EAs.</p><p><strong>Nick Beckstead:</strong> Yeah. I don't know. I guess somebody who's not one of those people and hasn't dug in as far, maybe they should just see if it makes sense to them or if they don't have time for that, spot-check pieces of the argument that seem most likely to fail. Then if it seems fine, maybe you can believe it if you have limited time to spend on it. But yeah, I think it would be probably perverse to say \u201cWell, they said it, so therefore it\u2019s probably&nbsp;true.\u201d</p><p><strong>Question:</strong> You\u2019ve spent a lot of time thinking about this sort of area and how to build the EA community, and I expect you have reached some unusual or counterintuitive conclusions. I'm interested in where your views on the question of how to build an EA community differ most from the views that most people in the community&nbsp;have.</p><p><strong>Nick Beckstead:</strong> Depends on which segments of the community you're talking about, I guess. I feel like I'm not contrarian on my point that we should reallocate out of earning to give and into direct work and building deep expertise in the main EA cause areas. Although I still feel like if I hear a media article describing what effective altruism is, it\u2019s mostly as something that\u2019s about optimal charity donations, and I think that's not the meat of it for me. Where else am I very contrarian? I don't feel that contrarian actually, so I'm not&nbsp;sure.</p><p><strong>Question:</strong> Fair enough. Yeah, in the questions we've got a little bit of pushback against the idea that earning to give is not as valuable as direct work. So when a person asks, a lot of centrally EA orgs don't value earning to give donations, but it seems like a lot of valuable projects don't get funded sufficiently or the directors spend a lot of their time on fundraising. What best explains this&nbsp;discrepancy?</p><p><strong>Nick Beckstead:</strong> I guess you could distinguish between different claims that could be being made about earning to give. The most extreme claim would be like, \u201cMoney is useless. No one should earn to give.\u201d That's not the claim. I guess the claim is that the community is over-allocated in terms of labor and attention on earning to give relative to direct work: going and getting a PhD in a relevant field or starting your own&nbsp;venture.</p><p>So how can that still be true, even though not all the organizations are fully funded? I guess it depends partly on your opinions about what are the most pressing need in this community that are going strongly unfunded. I think if you look at the list of grantees that I presented, most of them are pursuing their most important projects and aren't spending a huge portion of their time on fundraising. Perhaps that question comes from somebody who has a difference of opinion from me about what the best work is to get funded. I guess the other way to be skeptical of me would be to say \u201cWell, of course, you're a funder. Your job is to recommend what should be funded. Maybe the gaps that remain are very invisible to you, but they're plain to somebody else who has different&nbsp;judgments.\u201d</p><p>I guess this is in some ways an interesting illustration of the points about earning to give. You can distinguish between the effective altruism community not having enough money and the effective altruism community not having enough diligent attention to make sure that money goes to all the best things. I think that we are somewhat bottlenecked on that, but that's different from just earning to give in a way. It\u2019s more like if someone's earning to give and they're spending a lot of diligent attention to try to think \u201cAlright, where exactly should this go? What is being missed by the other important players in this community?\u201d I'm not saying there's no low-hanging fruit left, and I think you can find it that&nbsp;way.</p><p>The thing I'm more skeptical of and would be very skeptical of would be if somebody said \u201cWell, I'm going to be earning to give and I'm going to just spend it all on the things that Nick Beckstead says are good. I'm going to donate it to the two EA funds that Nick runs, and that's the best idea.\u201d I would feel pretty strongly that if this person could go and provide diligent attention to something, it\u2019d be a lot less surprising to me if they did something great, if they were thinking very independently about how they were doing their&nbsp;funding.</p><p>I guess I'm saying the bottleneck for me is not how much funding I can recommend to be used well. It's much more the ability to spend time and really evaluate new things. Somebody has to do that. I think that's the thing that's bottlenecking us on the philanthropy side of this&nbsp;community.</p><p>Other caveats would be that you've got to think about what your comparative advantage is. If you feel like \u201cI don't know how I would do that at all, but I'm making great money in this job\u201d then great, maybe you should just keep earning to give and that's the best thing for you to do. I think that would be the strongest case somebody could make, if they were saying, \u201cI'm doing this earning to give thing. It's really going to be transformative.\u201d I guess that\u2019s my answer in rambling&nbsp;form.</p><p><strong>Question:</strong> It seems like you're enthusiastic about having a greater diversity of views about the funding decisions. Someone asks how can we avoid EA being an echo chamber, which is a slightly more general question for that&nbsp;point.</p><p><strong>Nick Beckstead:</strong> I am pretty enthusiastic about diversification of who's going to decide how funding is used. How do we avoid EA becoming an echo chamber? Well, if we had a community where we had more people getting deep expertise in other fields, I would expect that to be more of a limit on the echo chamber effect to some extent. There are things you can get away with saying and no one will critique them in the effective altruist community, they'll probably be different from the things you can get away with saying without anyone pushing back in other communities, and you would be introducing a bunch of different cognitive styles if we had that. People should get their information from a variety of sources and probably not tune in exclusively to an RSS feed or information diet that's all EA all the&nbsp;time.</p><p><strong>Question:</strong> Yeah, going to some of the seminars at the Academic Institute at Oxford on effective altruism, we get people in from philosophy and economics who are less familiar with effective altruism, and often their views are really interesting and they can sometimes rip to shreds some of the arguments that EAs present, which is really cool. But I also noticed, maybe particularly in the Bay Area, there\u2019s a bit more skepticism of academia as an outside view. I wonder if you had thoughts on&nbsp;that?</p><p><strong>Nick Beckstead:</strong> I don't share the skepticism of academia to that degree. I think academia has its own set of strengths and weaknesses, but at the end of the day I've been thinking a bit about the education and training aspect. You have this question: how could you have more people in this community with really valuable expertise in fields like biology and machine learning? Academia has this machine where you go in as a smart person with a basic knowledge of a subject, and you spend several years in there in a relatively low supervision setting, and then you come out as someone who at least has basic familiarity with most areas of the subject, can tell the difference between good and bad argumentation according to the norms of the discipline, and can incrementally advance on things in the field, and sometimes it produces a person who can make real advances on the field. That's a pretty remarkable thing to&nbsp;have.</p><p>When I look at the effective altruism community and a lot of the people who've done good things in it, certainly not all of them spend a bunch of time in academia, but people who have PhDs in some relevant academic field are not horribly underrepresented. I guess I would have to respond more specifically to a specific way of being skeptical of academia. But it seems to me that it provides substantial&nbsp;value.</p><p><strong>Question:</strong> And probably the final area because we\u2019re nearly out of time, but there are a few questions talking about diversity. Someone says that the EA community tends to recruit people with quite similar backgrounds. One of the questions talks about socio-economic, racial, cultural backgrounds. And another one talks a bit more about intellectual diversity, about rational, well-educated, privileged people. How worried should we be about diversity, and if we should be worried, what can we do to address&nbsp;that?</p><p><strong>Nick Beckstead:</strong> Yeah. I guess I would have different thoughts on each of them. I think it's certainly true that in EA as a field, we have a lot of people who are math, philosophy, computer science type people, and it's pretty disproportionate. I think those are good fields and it's done some interesting and good things for this community. I think there are fields where it would be great to expand that out and cover a number of other intellectual&nbsp;fields.</p><p>I think it's also very true that this is a community that skews on the privileged end on pretty much any access of privilegedness that you could name. I think that probably does bad things for us in terms of viewpoint diversity and who feels most welcome or who feels like they have good role models in this field and probably a number of other difficulties that I can\u2019t easily name. It would be great to improve that. I don\u2019t have a lot of great solutions for it though at this&nbsp;time.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "vq98sQBzjCZhzAq2u", "title": "Bruce Friedrich & Paul Shapiro: Fireside chat (2017)", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=B061oaF-j6g&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=16\"><div><iframe src=\"https://www.youtube.com/embed/B061oaF-j6g\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>Bruce Friedrich and Paul Shapiro talk clean meat, plant-based meat, and The Good Food Institute.</i></p><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "izD5LT6qvyfTqyCKv", "title": "Jan Leike, Helen Toner, Malo Bourgon, and Miles Brundage: Working in AI", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=gmL_7SayalM&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=15\"><div><iframe src=\"https://www.youtube.com/embed/gmL_7SayalM\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 panel, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "fqEcHtEvancXg4Jy4", "title": "Daniel Dewey: The Open Philanthropy Project's work on potential risks from advanced AI", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=Nfrh4K3d_Z0&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=14&amp;t=18s\"><div><iframe src=\"https://www.youtube.com/embed/Nfrh4K3d_Z0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this 2017 talk, </i><a href=\"https://www.openphilanthropy.org/about/team/daniel-dewey\"><i>Daniel Dewey</i></a><i> presents Open Philanthropy Project's work and thinking on advanced artifical intelligence. He also gives an overview over the field, distinguishing between strategic risks - related to how influential actors will react to the rise of advanced AI systems - and misalignment risks - related to whether AI systems will reliably do what we want them to&nbsp;do.</i></p><p><i>The transcript below is lightly edited for&nbsp;readability.</i></p><h1>The Talk</h1><p>I'm the program officer at the Open Philanthropy Project in charge of potential risks from advanced AI. This is an area we're spending a lot of our senior staff time on recently, so I wanted to give an update on the work that we're doing in this area, how we think about it, and what our plans are going&nbsp;forward.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=1800&amp;q=70 1800w\"></figure><p>So, there are four basic concepts that I want to really make sure to drive home during the course of this talk, and if you watch out for these, I think they'll help you understand how we're thinking about this&nbsp;area.</p><p>I think there are a lot of different ways to frame potential risks from advanced AI that can inform different kinds of approaches and interventions and activities. And it can be a bit hard to understand why we're doing the things we're doing without understanding the way we're thinking about them. Also, I should mention, I didn't really frame this talk up as the perfect introduction to this area if you're not already somewhat&nbsp;familiar.</p><p>These are the four basic&nbsp;concepts:</p><ol><li>Transformative AI, which is how we think broadly about the impacts that AI could have in the future that we care most about affecting our&nbsp;activities;</li><li>Strategic risks, having to do with how the most influential actors in the world will react to the prospect of transformative&nbsp;AI;</li><li>Misalignment risks, which have to do with being able to build AI systems that reliably do what their operators want them to&nbsp;do;</li><li>Our strategy in this area. The way we're currently planning on making a difference, which is field&nbsp;building.</li></ol><h2>Transformative AI</h2><p>So, to start off, there's this idea of transformative AI. Basically looking ahead at the kinds of impacts we expect AI to have. We think there are a lot things that could happen and there's a lot of uncertainty about precisely what is going to happen. But something that seems reasonable is to expect AI to have an impact that is comparable to or larger than that of the Industrial or Agricultural Revolutions. And that's intended to capture a lot of possible sorts of scenarios that could&nbsp;happen.</p><p>So, we might see AI progress lead to automated science and technology development, which could lead to a really rapid increase in technological progress. We might see artificial general intelligence (sometimes abbreviated AGI), meaning AI systems that can do anything that a human can do, roughly. And that would really change the dynamics of the economy and how the economy functions. We might see systems that can do anything that a human or a group of humans can do. So AI systems could operate organizations autonomously. Maybe companies, non-profits, parts of&nbsp;government.</p><p>And then sort of looming over all of this is the idea that we shouldn't really expect AI to stop at the point of human-level competence, but we should expect the development of super-intelligent AI systems. It's not clear exactly what the distribution of capabilities of these systems would be and there are a lot of different&nbsp;possibilities.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=1800&amp;q=70 1800w\"></figure><p>The reason I've chose this picture in the slides is because it shows the change in the way human influence was wielded on the world during the Industrial Revolution. You can see this traditional set of biofuel usage down at the bottom and then over the course of the Industrial Revolution, that became a very small percentage of the overall influence that humanity wielded. Most of what we were doing in the world came to depend on these new energy&nbsp;sources.</p><p>The idea of transformative impact comes from AI becoming a really large percentage of how humanity influences the world. That most of the influence we have could be via AI systems that are hopefully acting on our&nbsp;behalf.</p><p>Based on the conversations we've had with a lot of AI researchers, it's pretty reasonable to think that this could happen sometime in the next 20 years. I'm saying greater than 10% chance by 2036 because we said 20 years last year and so we don't want to always be saying 20 years later as years&nbsp;continue.</p><p>So there\u2019s this really big change in the world, there's a lot of variation in what could happen, and it's hard to predict exactly what is going to be most critical and what kinds of things we might want to make a difference&nbsp;on.</p><p>So here is our general strategy in this area. We can imagine two different worlds. One of them is a world where transformative AI comes somewhat by surprise, maybe it comes relatively early. And there aren't a lot of people who have been spending much of their career thinking full time about these problems, really caring about longterm outcomes for humanity. And then there's an alternate world where those professional people have existed for a while. They're working in fields with each other. They're critiquing each other's&nbsp;work.</p><p>And we think that the prospect of good outcomes is a lot more likely in cases where these fields have existed for a while, where they're really vibrant. They have some of the best people in policies, some of the best people in machine learning and AI research in them. And where those people have been thinking really specifically about how transformative AI could affect the long run trajectory of human&nbsp;civilization.</p><p>So, our basic plan is to affect field building. To try to move these fields ahead, in terms of quality and in terms of size. And a really useful thing about this is that if you wanna affect the longterm trajectory of civilization, you don't really get to run several experiments to see which interventions are going to work well. So it's really hard to get feedback on whether what you're doing is&nbsp;helping.</p><p>So, what we'd like to do is start really keeping track of how these fields grow over time so that we can tell which kinds of interventions are making a difference. And it's not a sure thing that field growth is the correct strategy to pursue but it at least gives us something to measure and track to see if what we're doing is making a&nbsp;difference.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=1800&amp;q=70 1800w\"></figure><h2>Strategic Risks</h2><p>I'm starting with strategic risks because I think they have historically been less emphasized in the EA community. By strategic risks, I mean risks that could be caused by the way major, influential actors in the world react to the prospect of artificial general intelligence, or super-intelligence, or other kinds of transformative AI. And the way that they choose to use these technologies to affect the world. So sort of the policies and strategies they&nbsp;adopt.</p><p>For example, if you expect this big curve of human influence in the world to be mostly about artificial intelligence in the future, then that's a big opportunity for different actors to have more influence in the future than they do today or an opportunity for that influence to be rebalanced. Maybe between different countries, between different industries. It feels like there's a strong chance that as influential actors start noticing that this might happen, that there could be preemptive conflict. There could be arms races or development races between governments or between&nbsp;companies.</p><p>If a government or company gains a really strong advantage in artificial intelligence, they might use it in a way that isn't in the best interest of the most people. So we could see a shift in the way resources and rights are distributed in the future. I classify that as a misuse of artificial intelligence. We want to make sure that transformative AI is used in a way that benefits the most people the&nbsp;most.</p><p>And then a final thing to think about is the possibility of accidental risks, risks of building AI systems that malfunction and do things that don't really benefit anyone, that weren't intentional. Then racing to develop artificial intelligence could be a big increase in that risk, because if you spend time and money and resources on making systems safer, you're spending less on&nbsp;racing.</p><p>What we'd like to do is build up a field of people who are trying to answer the key question of what should influential actors do in different scenarios depending on how AI development plays out. Its important to consider different scenarios because there\u2019s a lot of variation in how the future could&nbsp;go.</p><p>And there are a lot of existing relevant areas of expertise, knowledge and skill that seem like they're really relevant to this problem. So, geopolitics, global governance. It seems important for AI strategists to have pretty good working knowledge of AI and machine learning techniques and to be able to understand the forecasts that AI developers are making. And there's a lot of history in technology policy and the history of transformative technologies such that I hope that there are lessons that we could take from those. And of course, there's existing AI risk thought. So, Nick Bostrom's Superintelligence, things that have been done by other groups in the effective altruist&nbsp;community.</p><p>And so, our activities in this area of AI strategic risk right now, how are they going? I think that the frank summary is that we're not really sure how to build this field. Open Philanthropy Project isn't really sure. It's not really clear where we're going to find people who have the relevant skills. There's not, as far as we can tell, a natural academic field or home that already has the people who know all of these things and look at the world in this way. And so, our activities right now are pretty scattered and experimental. We're funding the Future of Humanity Institute and I think that makes sense to do, but we're also interacting a lot with government groups, think tanks, companies, people who work in technology policy, and making a few experimental grants to people in academia and elsewhere just to see who is going to be productive at doing this&nbsp;work.</p><p>I think it's really unclear and something I'd love to talk to people about more. Like how are we going to build this AI strategy field so that we can have professional AI strategists who can do the important work when it's most&nbsp;timely?</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=1800&amp;q=70 1800w\"></figure><h2>Misalignment Risks</h2><p>So, the other category of risk that I wanna talk about is misalignment risks. I've used a picture of a panda. This is an adversarial example. It's a crafted image that's designed to make an AI system make an incorrect decision. And it's been sort of a recent, really hot topic in machine learning because it shows the fragility of some kinds of machine learning models that are really popular right&nbsp;now.</p><p>This kind of fragility is not a full picture of the problems of AI misalignment. It\u2019s not a full picture of when AI systems don't reliably do the things that their operators want them to do, but I think it's a good simple, straightforward example. The intent of training a neural network on these images was to get the neural network to make the same classifications that humans would. And it turns out to not be very hard to come up with a situation where the neural network will just do something completely different from what any human would&nbsp;say.</p><p>So, broadly speaking, misalignment risks refer to situations where we can make really influential AI systems and most of our influence over the world is flowing through these AI systems, but we can't make these systems reliably pursue the objectives that their operators intend. So, if we see this, a similar shaped graph as ended the Industrial Revolution where almost everything that humans are doing in the world is going through AI systems, and most of the way the world goes in the future depends on those decisions sort of lining up well with what humans want, then it's a really bad situation if we're not really sure if AI systems are going to do the things we want them to do, if they misinterpret what we want them to do, if they're gonna act unreliably when they're in situations we haven't anticipated&nbsp;before.</p><p>So, we've been talking a lot to groups like the Machine Intelligence Research Institute, to the Future of Humanity Institute, and also to technical advisors of ours who are at industrial research labs like OpenAI and Deep Mind and then also to people in academia, machine learning&nbsp;researchers.</p><p>And there are a couple of priority areas of research that we think are really important if you want to advance the technical capability of building AI systems that reliably do the things that their operators want them to do: reward learning and&nbsp;reliability.</p><p>So reward learning is this idea that it would be quite bad if we could build AI systems that can pursue easily specifiable goals like things you can measure in the world that are like how much money is in this bank account or how rewards come in through this particular channel that's flowing back to the AI. Most of the things humans care about in the world aren't easily measured in that way. So, there's a question of whether we can get AI systems to learn a task by interacting with humans in a way that makes them sort of cooperatively refine their understanding of what our goals are and act conservatively in cases where they have a lot of uncertainty and where the impact on the world could be very great if they've made the wrong evaluation of what their operator's objectives&nbsp;are.</p><p>And then on the reliability side, there's this question of how we train AI systems in really limited subsets of the situations that they'll eventually be functioning in. So if we want AI systems to make important decisions in the world, especially if the world is changing rapidly and dramatically, we need to be really sure that AI systems are not going to function dramatically differently in those situations than they did in&nbsp;training.</p><p>At Open Philanthropy Project, we've encountered a bunch of different models and ideas about how hard AI alignment will be. There's some people we've talked to who think that AI alignment is like really, really closely related to all of the things that we'll need to do in order to make AI systems effective in the world in the first place. Those problems are just gonna be solved along the way. On this view, maybe it doesn't hurt to get started ahead of time, but it's not an urgent issue. And we've talked to other people who think that there are a ton of open, unsolved problems that we have no idea how to make traction on. And that we need to get started yesterday on solving these problems. And there are a lot of people in the middle. Probably the majority of people are somewhere in between, in terms of AI and machine learning&nbsp;researchers.</p><p>So, we're highly uncertain about how hard alignment will be and we think that it makes a lot of sense to get started on this academic field building in this area. If the worst case scenario is that we build this field and the problems turn out to be easier than we expected, that seems pretty&nbsp;good.</p><p>I think we're a lot clearer how misalignment field building will go than we are about how strategic risk field building will go. In reward learning and reliability, and then in AI alignment more broadly, I think that the academic field of AI and machine learning research contains the people who have the kinds of skills and capabilities that we need for AI alignment research already. And this is an area where philanthropic funding can just directly have an impact. There's a bit of a funding puzzle to do with having all these different chickens and eggs that you need in order to get a good research field up and running. And that includes having professors who can host students, having students who are interested in working on these problems and having workshops and venues that can coordinate the research community and kind of weave people together so that they can communicate about what questions are most&nbsp;important.</p><p>I think it's obvious that this kind of field building work could pay off in the longer term. If you imagine this AI alignment community building up over many decades, it's obvious. But actually, I think that even if we want to develop experts who will be ready to make essential contributions on short timelines, this is among the best ways to do that, because we're finding PhD students who have a lot of the necessary skills already and getting them to start thinking about and working on these problems as soon as we&nbsp;can.</p><p>So, this is a scenario where we've done a pretty significant amount of grant making so far and we have some more in the works. There have been a couple big grants to senior academics in artificial intelligence and machine learning. The biggest ones being to Stuart Russell and his co-investigators, several other professors, at the Center for Human Compatible AI, which is based in Berkeley and also has branches at couple of their universities. There's another big grant that went to Joshua Bengio and bunch of his co-investigators at The Montreal Institute for Learning Algorithms. And that's a fairly recent grant. There are more students coming into that institute in the fall who we're hoping to get involved with this&nbsp;research.</p><p>With other professors, we're making some planning grants so that we can spend time interacting with those professors and talking with them a lot about their research interests and how they intersect with our interests in this area. Overall, we're taking a really personal, hands-on approach with grants to academic researchers in this area because I think our interests and the research problems we think are most important are a little bit unusual and a little bit difficult to communicate&nbsp;about.</p><p>So, I think it's important for us to do these sort of relationship-based grants and to really spend the time talking to the students and professors in order to figure out what kinds of project would be most effective for them to&nbsp;do.</p><p>So far, the main support that we've lent to students is via their professors. So often academic grants will support a professor, part of a professor's time and much of several of their students' times. But this fall we're hoping to offer a fellowship for PhD students, which is a major way that machine learning PhD students are&nbsp;supported.</p><p>I'm quite bullish on this. I think that it's reasonable to expect a lot of the really good research and ideas to come from these PhD students who will have started thinking about these things earlier in their careers and had more opportunity to explore a really wide variety of different problems and approaches. But again, offering a PhD fellowship is not something we've done before so I think it's going to be sort of experimental and iterative to figure out how exactly it's going to&nbsp;work.</p><p>As far as workshops, we've held a workshop at Open Philanthropy Project for a bunch of grantees and potential grantees. Basically, as an experiment to see what happens when you bring together these academics and ask them to give talks about the AI alignment problem. We were quite happy with this. I think that people quickly jumped on board with these problems and are exploring a set of ideas that are closely related to the fields that they were working on before, but are approaching them from an angle that's closer to what we think might be required to handle AI&nbsp;alignment.</p><p>There are also workshops like Reliable Machine Learning in the Wild that have been in academic machine learning conferences, which are the major way that academics communicate with each other and publish results. Conferences dominate over journals in the field of machine learning. So we think supporting workshops at conferences is a good way to build up this&nbsp;community.</p><p>And it really depends on being able to communicate these problems to professors and students because they're the primary organizing force in these&nbsp;workshops.</p><p>There are other developments that I think you guys might be especially interested in. There's the Open Philanthropy Project partnership with OpenAI, which I think Holden talked about a little bit yesterday. We're quite excited about this. It's an unusual grant because it's not the case that we're just contributing money to a group and then letting them pursue the activities that they were going to pursue anyway. It's like a really active partnership between us and them to try to pool our talents and resources to pursue better outcomes from transformative&nbsp;AI.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=1800&amp;q=70 1800w\"></figure><p>So, I'm really excited about that. It's not clear exactly what kinds of results and updates and communications it makes sense to expect from that because it's still pretty early, but I have high hopes for it. We funded the Machine Intelligence Research Institute last year and we're still in a lot of conversations with them about their particular outlook on this problem and the work that they're&nbsp;doing.</p><p>There's a collaboration between OpenAI and Deep Mind. So this is something that the Open Philanthropy Project isn\u2019t funding or playing a role in directly, but I think it's an exciting development just for people who care about this area. So, OpenAI's a nonprofit and Deep Mind is part of Google, but in theory they could be viewed as competitors for producing artificial general intelligence. So I think it's really encouraging to see their safety teams working together and producing research on the alignment problem. I think that's a robustly positive thing to&nbsp;do.</p><p>I also happen to think that the research that they did jointly publish, which is about learning from human feedback - so, having an AI system demonstrate a series of behaviors and having a human rate those behaviors and using those ratings to guide the learning of the AI system - I think this is a really promising research direction. A lot of this research is related to Paul Christiano's concept of act-based agents, which personally I'm really optimistic about as a new direction in the AI alignment&nbsp;problem.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=1800&amp;q=70 1800w\"></figure><h2>Our Strategy in this&nbsp;Area</h2><p>So, overall, the takeaway here: last year we published a blog post on the philanthropic opportunity that we saw from transformative AI. And looking back on that a year later, I think that short timelines still look plausible. This greater than 10% chance over the next 20 years of developing transformative AI seems really real. And additionally, we increasingly think that Open Philanthropy Project can make the biggest difference in the world where timelines are short in that way. So, a major criterion that we apply to the work that we're doing is: would this be useful if AGI were developed within the next 20 years or&nbsp;so.</p><p>Neglectedness still looks really high. We haven't seen a lot of other funders jumping into this space over the next year and I think it was really possible given the increase in attention to artificial general intelligence, that this space would become much more crowded. I think Open Philanthropy Project and this community are still in a pretty unusual position to influence outcomes in this area just because it is so&nbsp;neglected.</p><p>And after having done some experiments in strategy and field building in technical AI alignment research, I think tractability looks higher than it did before. It's probably within the general range that we thought it was in, but maybe more concentrated in the high end. Just as we've gone on and talked to more and more AI researchers, it's been easier than expected to communicate the things that we're interested to find common ground between what they think they could do productive research on and what we think would make the biggest difference for the future trajectory of human&nbsp;civilization.</p><p>So those are the continued high-priorities for us. We're still spending a lot of senior staff time on it and I think it's a cause area that it makes sense to pay attention to if you're interested in the long-term trajectory of human&nbsp;civilization.</p><p>I'll take questions now, and thanks for your&nbsp;time.</p><h1>Q&amp;A</h1><p><strong>Question:</strong> Do you think that we should or if it is even possible to slow the advance of AI until some of these areas can mature that you're investing&nbsp;in?</p><p><strong>Daniel Dewey:</strong> I think that's a good question. My current guess is that we don't have very good levers for affecting the speed of AI development. I think there's so much money and so much pressure in the rest of society to develop artificial intelligence that it\u2019s not in a place where we have a particularly strong advantage. Slowing down technology is, I think, quite difficult to do and it would take a really concerted effort on the part of a much larger&nbsp;community.</p><p>But on top of that, I think it's a really open question how much it makes sense to think of this as like a race between two totally separate technologies, which are like capabilities and safety. My experience has been that you need a certain amount of capability in order to really do a lot of the research on AI&nbsp;safety.</p><p>So, yeah. It doesn't seem that tractable to me and even if it were more tractable, I think it's still sort of an open strategic&nbsp;question.</p><p><strong>Question:</strong> Okay. Great. Next&nbsp;question.</p><p>Given the massive advantage that someone or some group could gain from winning the AI race, let's say, it seems to this questioner that the strategic considerations are perhaps the biggest risk. So, how does the field building that you're engaged in help us avoid this sort of arms race scenario in&nbsp;AI?</p><p><strong>Daniel Dewey:</strong> I don't want to express too much confidence about this, but the way that I currently see the strategic field building work playing out is that we don't really want people making up their strategies on the fly, in a panic at the last minute. And if there are people who have done work ahead of time and gained expertise in the strategic considerations that are going on here, I think that we can have much better, more detailed, more well worked out plans for groups to coordinate with each other to achieve their shared&nbsp;interests.</p><p>And then also if there are some groups that we think will use AI more responsibly, or some governmental structures that we think would be more conducive to overall flourishing, I think that's not something you can work out at the last minute. So, I see developing a strategy for mitigating harms from misuse or from racing as something that we need these strategy experts to do. I don't think it's something that we can do in our spare time or something that people can do casually while they're working on something else. I think it's something that you really want people working on full&nbsp;time.</p><p>So I guess that's my perspective. Since we don't know what to do, that we should develop these&nbsp;experts.</p><p><strong>Question:</strong> Another question that touches on several of the themes that you just mentioned there. How do you expect that AI development will impact human employment and how do think that will then impact the way that governments choose to engage with this whole&nbsp;area?</p><p><strong>Daniel Dewey:</strong> Yeah. This a super good&nbsp;question.</p><p>I don't have a good answer to this question. I think that there are interesting lessons from self-driving cars where I think most people who have been keeping up with self-driving cars, with the raw technological progress, have been a little bit surprised by the slowness of this technology to roll out into the&nbsp;world.</p><p>So, I think one possibility that's worth considering is that it takes so long to bring a technology from a proof of concept in the lab to a broad scale in the world. That there could be this delay that causes a big jump in effective capabilities in the world where maybe we have, in the lab, the technology to replace a lot of human labor but it takes a long time to restructure the marketplace or to pass regulatory barriers or handle other mundane obstacles to applying a new&nbsp;technology.</p><p>But I think it's absolutely worth considering and it's an important strategic question if there going to be things like employment or things like autonomous weapons that will cause governments to react dramatically to AI in the really short term. In the US the big example is truck driving. Is autonomous truck driving going to cause some concerted reaction from the US government? I don't really know. I think this is a question we would like to fund to&nbsp;answer.</p><p><strong>Question:</strong> Obviously, there's a lot of debate between openness and more closed approaches in AI&nbsp;research.</p><p><strong>Daniel Dewey:</strong> Yeah.</p><p><strong>Question:</strong> The grant to OpenAI's a big bet, obviously, on the open side of that ledger. How are you thinking about open and closed or that continuum between those two extremes and how does your bet on OpenAI fit into&nbsp;that?</p><p><strong>Daniel Dewey:</strong> So, I don't actually think that the bet on OpenAI is a strong vote in favor of openness. I think that their philosophy, as I understand it in this area, is that openness is something that they think is a good heuristic. Like it's a good place to start from in some sense. That if one of the things you're worried about is uneven distribution of power, there's this powerful mechanism of distributing information and capabilities and technology more&nbsp;widely.</p><p>But if you go and look at what they've written about it, especially more recently, they've been pretty clear that they're going to be pragmatic and flexible and that if they're sitting around a table and they've developed something and their prediction is that releasing it openly would cause horrible consequence, they're not going to be like, \"Well, we committed to being open. I guess we have to release this even though we know it's going to be awful for the&nbsp;world.\"</p><p>My perspective on openness is that, I mean, this is a boring answer. I think it's one of these strategic questions that like you can do a shallow analysis and say like, if you're worried about the risk of a small group of people taking a disproportionate chunk of influence and that that would be really bad, then maybe you want to be more open. If you're mostly worried about offense beating defense and only one hostile actor could cause immense harm, then you're probably gonna be more excited about closedness then&nbsp;openness.</p><p>But I think we need to move past this shallow strategic analysis. Like, we need people working in a real way on the detailed, nitty-gritty aspects of how different scenarios would play out, because I don't think there's a simple conceptual answer to whether openness or closedness is the right&nbsp;call.</p><p><strong>Question:</strong> Well, we'll have it to leave it there for today. Round of applause for Daniel&nbsp;Dewey.</p><p><strong>Daniel Dewey:</strong> Cool. Thank&nbsp;you.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "Z5R5Snh7bfbnxoqAp", "title": "Kerry Vaughan, Michael Page, Alyssa Vance, & Anna Salamon: Celebrating failed projects", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=Y4YrmltF2I0&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=13\"><div><iframe src=\"https://www.youtube.com/embed/Y4YrmltF2I0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 panel, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "QH5XFgRamDKeAt9cQ", "title": "Duncan Sabien, Miranda Dixon-Luinenburg, Malo Bourgon, & Cathleen Kilgallen: Getting things done", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=trTslOidmq8&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=12\"><div><iframe src=\"https://www.youtube.com/embed/trTslOidmq8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 panel, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "HENDjaA37Euz9KqSp", "title": "Ruth Grace Wong, Hayley Cashdollar, Glenn Matlin, Theresa Condor, & Diane Gillespie: Logistics at scale", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=NjLl819ZVfs&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=11\"><div><iframe src=\"https://www.youtube.com/embed/NjLl819ZVfs\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p>I ran a <a href=\"https://sf.eaglobal.org/agenda/session/194815\"><u>panel</u></a> with experts from manufacturing, shipping / distribution, computation, and a social initiative at scale to expose the complexity, logistics, and engineering behind successful large projects. I\u2019m <strong>Ruth Grace Wong</strong>, Site Reliability Engineer at <a href=\"https://www.pinterest.com/\"><u>Pinterest</u></a>, and my panelists were:</p><p><strong>Hayley Cashdollar</strong>, Manager of Automation at <a href=\"https://www.proterra.com/\"><u>Proterra</u></a></p><p><strong>Glenn Matlin,</strong> Data Scientist at <a href=\"https://www.lendup.com/\"><u>LendUp</u></a></p><p><strong>Theresa Condor,</strong> Vice President of Corporate Development and Board Member at <a href=\"https://spire.com/\"><u>Spire Global</u></a></p><p><strong>Diane Gillespie,</strong> Emerita Professor at <a href=\"https://www.uwb.edu/\"><u>University of Washington Bothell</u></a></p><p>Here is the transcript of the <a href=\"https://www.youtube.com/watch?v=NjLl819ZVfs\"><u>video</u></a>, edited for readability.</p><p>&nbsp;</p><p><strong>Helen Toner:</strong> I\u2019m going to introduce the panel which is Logistics at Scale. Obviously if you want to do things in the world it helps to do those things with other people. If you want to do big things, there\u2019s going to be a lot of logistics issues that you will need to solve: how to get things done and how to make things run smoothly. I\u2019m sure once you\u2019ve worked in an organization that isn\u2019t taking care of its logistics you will never again underestimate how great it is to have people taking care of logistics. That\u2019s what this is going to be about. It\u2019s moderated by Ruth Grace Wong who is a Site Reliability Engineer at Pinterest and Ruth is going to take it from here and introduce the rest of the panelists.</p><p><strong>Ruth Grace Wong:</strong> Hi, my name is Ruth and we\u2019ve got some extremely talented individuals here. On the far end we\u2019ve got Hayley Cashdollar. She works for Proterra, designing and building a factory to make batteries for zero emission transit buses. Next to her we\u2019ve got Glenn Matlin who is a data scientist at LendUp which offers loans. Glenn works on machine learning driven underwriting and advanced product analytics so he\u2019s our software computation representative. Then we\u2019ve got Theresa Condor who\u2019s the VP of corporate development of Spire Global which monitors 90 percent of global trade with satellites. She\u2019s an industry veteran for distribution and supply chain. Finally we\u2019ve got Diane Gillespie who is a full-time volunteer for Tostan, an award-winning nonprofit that has brought non-formal human rights-based education to hundreds of communities in North and West Africa. We have six questions for the panel and then we will be taking audience questions. The first question is: Tell me briefly about how you got to the role that you\u2019re in now.</p><p><strong>Hayley Cashdollar:</strong> Thank you for hosting us. I am a mechanical engineer by training. I went to the University of Colorado for my bachelor\u2019s degree and from there I went straight to Tesla Motors where I worked on powertrain manufacturing engineering. I designed and built equipment that made the rotor and the cooling systems for the Model S. I then found that I really liked the mechanical design on that equipment but didn\u2019t know anything about the software, and so I went back to grad school to get another degree in mechanical engineering with with an emphasis on controls engineering. In my current role I explore both the software and the hardware side of manufacturing problems.</p><p><strong>Glenn Matlin:</strong> My background was really more about in econometrics and I came from Florida where we saw firsthand examples of bad financial lending in 2008. The subprime lending market cratered all of Florida in terms of employment and the economy. Having studied that, I worked my way out towards San Francisco because I was very interested in trying to use my skills to build better financial services. Most of my work was in analytics. A lot of the self-trained self-taught got me out here so I\u2019m much less of an academic on my team than others. I tend to come from a do it yourself and work your way up kind of perspective on things.</p><p><strong>Theresa Condor:</strong> Hi everyone. I\u2019ve been with Spire Global basically since we started the company in 2012. I\u2019m not technically a co-founder but have been doing everything from the beginning with the co-founding team. I\u2019m not technically in the space sector. Our company Spire builds satellites: we put them up into orbit, listen to the earth and collect various types of data and then sell the data as a service. I\u2019m not traditionally someone coming from the space sector. I\u2019m not even an engineer. What interests me about what we do at Spire is how you can use the data to have an impact on big problems to help businesses be more efficient. I think for the purposes of this particular conference the things that I\u2019m interested in are our ability, from our data, to identify illegal fishing that takes place on a global basis, to monitor global trade patterns, to collect a unique type of weather information from our satellites in places like Africa and over the oceans where you have no other type of infrastructure that can do it.</p><p><strong>Diane Gillespie:</strong> Thanks again for hosting. I\u2019m in awe of all of you. I\u2019m Diane Gillespie and I\u2019m from the University of Washington, the Bothell campus. It\u2019s a very interdisciplinary, exciting place to be. I retired to help Tostan which is a non-profit program operating now in six countries in West and North Africa. It is a three year non formal human rights-based education program that works in resource-poor communities. It\u2019s probably best known for the one of the outcomes of the program: that communities decide to abandon female genital cutting. The founder of this organization, <a href=\"https://en.wikipedia.org/wiki/Molly_Melching\"><u>Molly Melching</u></a>, was recognized by the <a href=\"http://skoll.org/\"><u>Skoll foundation</u></a>, and they did their first book called <a href=\"https://www.tostan.org/about-us/however-long-night/\"><u>However Long The Night</u></a> on how she came to create an organization that has had this kind of impact. My role as a volunteer is to work in monitoring and evaluation. I also help with the training center.</p><p><strong>Ruth Grace Wong:</strong> Amazing, thanks everyone. So when you\u2019re doing something at scale there\u2019s a huge opportunity for success but there\u2019s also a huge opportunity for failure. Do you have any principles that you follow in scaling up, and also what\u2019s the biggest mistake you\u2019ve ever made?</p><p><strong>Hayley Cashdollar:</strong> Thanks for the question. There are so many mistakes that it is hard to choose from. I have found it very helpful to lean on and completely trust the people around me because if you are scaling a project you cannot do it by yourself. That comes from hiring the right people to help you but then putting your faith 100% in those people and letting them do the job that you hired them to do. One of the mistakes that we made as we rapidly scaled our manufacturing facility was overlooking ergonomics on one of our systems and it\u2019s been a real pain to go back and try to figure out how we can help avoid injuring our workers on one particular piece of equipment. If you are scaling really quickly it is really easy to overlook that kind of problem and I wish that we had taken a little bit more time to evaluate that particular part of that project. I think that comes with a lot of hardware scaling anytime you\u2019re manufacturing.</p><p><strong>Glenn Matlin:</strong> When you\u2019re operating at scale I have found that really small decisions turn out to have a gigantic impact down the line. They\u2019re the kind of decisions you never really end up thinking are important and they\u2019re the kind of decisions you make just because they seem so mundane. I found that the principle of really thinking slowly about what you\u2019re doing but being very fast and deliberate about it has been very useful. Taking the time to really consider all those small decisions and how they might impact you down the line is important. It lets you design and deal with edge cases. It lets you deal with situations where you get unexpected behavior from people using your service or your software. There are so many small decisions that end up becoming really impactful later on. One of our co-founders, Jake, is our current CTO. When he started, the company just decided to put floating-point storage for the value of our loans. I imagine this is like a really small thing to people but it turns out that when you\u2019re doing compliance for regulatory bodies you need to calculate every single cent perfectly and you can\u2019t really do that with imprecise floating calculations. That small decision five years later is a huge problem we had to go back and fix. To this day I think Jake would be like no, that was a mistake and it was headache but we now really think really tight about every little decision we make in our architecture because it pays off down the line.</p><p><strong>Theresa Condor:</strong> I think fundamentally a principle that\u2019s important to us in looking at scale is to make sure that you have a problem and a solution that you should scale. This means that it\u2019s it\u2019s big enough and it\u2019s going to impact enough people but at the same time you can you can do it without having to move in lockstep with the same number of people that are trying to implement it. What I mean is that if you\u2019re you\u2019re having an impact and you have to keep hiring the exact same number of people, that\u2019s not something that is going to scale, or you have to question whether you have the right solution. So you really want to have as wide a gap as possible in terms of the number of people that you impact and the number of people that it actually takes to to affect that. I think that\u2019s fundamentally the the big problems that we want to tackle with our satellite constellation, and we\u2019re talking about the the 1 billion people that rely almost exclusively on fish for their protein source and the other 4 billion people that do eat fish in some way for their nutrition, 7 billion people on the planet that are all impacted by weather to some extent and that continuing on. So big problems are the kind that really should scale and if you need to hire too many people in order to do that, it just won\u2019t work. People, for us, is really the biggest thing that we worry about and probably the biggest decisions that we\u2019ve made that have not gone well is hiring the wrong people or good people at the wrong time.</p><p><strong>Diane Gillespie:</strong> So the principle I think for Tostan has been fidelity to the curriculum it\u2019s created into its interactive pedagogy. There\u2019s a lot of pressure to cut back especially the work in social norm change. People want to do it quicker and faster. The integrity of the curriculum and the interactive way in which the curriculum is taught is really critical when you start to scale up. When you do that there\u2019s lots of pressures to fudge it. The biggest mistake Tostan made \u2014 I\u2019m not sure Molly Melching would say this \u2014 is that it went to Somalia to implement its program. Its national headquarters are in Dakar, Senegal and it looked like a perfect setting because it\u2019s in Somalia the worst type of female genital cutting is practiced and people beg Tostan to come. But it was a challenge in many ways because of the distance and the training but mostly there was no government and so the people who were working to train the Somalians in the program were actually arrested. It looked like a perfect setting but the kind of unforeseen details that arose in the actual implementation made it just too difficult to continue the implementation.</p><p><strong>Ruth Grace Wong:</strong> Wow, that sounds like a very difficult situation. The next question is: I work by day as a site reliability engineer and I find that when we\u2019re at scale I have to start writing systems to automate things and then you start having to write systems to manage the systems. What\u2019s something that you find that you have to do in a completely different way at scale compared to not at scale?</p><p><strong>Glenn Matlin:</strong> One of the big things that we have to really plan for is those edge cases that I was talking about, mainly because of the fact that we have to report and be held accountable by numerous government regulatory bodies at the federal and state level. Oftentimes when you\u2019re in an environment where you\u2019re filled with people, around people who are like really energetic and they want to move fast and they want to do big things and they think they\u2019re really going to help people, we also have to make sure that we really think about every little decision we make, mainly because those edge cases can cause problems if we\u2019re not thinking carefully about what we do. Financial reporting is an incredibly sensitive topic for a lot of people, and the financial services access that these people have is really their livelihood. So when we do our work we really think about them and we really focus on what we\u2019re doing for them. If we make a mistake with that edge case somebody is not going to get a loan, they are not going to make rent or they might lose access to their home or vehicle. So our work is very important because we really think about the person at the end of it and making those mistakes is very costly for them.</p><p><strong>Hayley Cashdollar:</strong> As a part of building a factory my entire existence is because we want to scale. My first year at Proterra I worked closely with a team of engineers where we built all of our prototype packs exclusively by hand and exclusively by engineers and that is not something that can scale. I\u2019d say those are probably the most expensive battery packs ever made. A team of about 15 people, all engineers, building by hand for many, many hours. As we look to scale any sort of automation project, one of the things that we have to do is consider that you have to approach the problem in a completely different way. So you can\u2019t use all the lessons learned on a manual hardware build and apply those to an automated hardware build because machines work differently than people do. Sometimes that makes it a lot easier and sometimes that makes it a lot harder. In order to bridge that gap and tackle those problems quickly I\u2019ve found that it is important to do as much process prototyping as possible to prove that in transition from a manual to automated process, you aren\u2019t overlooking something big that\u2019s going to end up delaying a project. That\u2019s been really key for Proterra as we\u2019ve scaled our factory.</p><p><strong>Theresa Condor: </strong>So for us this is very particular to our industry but it\u2019s building satellites in a different way. Traditionally you\u2019re building one satellite you\u2019re spending many years on it and it\u2019s kind of a bespoke design. For us we\u2019re doing many satellites. We have 40 that are in orbit right now and we\u2019re getting up to a steady state of 100, so that means we\u2019re building on average one satellite every single week. So you definitely have to do things differently. A lot of that is is is related to doing it almost in an assembly-line way. The very first satellite that we built was here in San Francisco and we did it in what was basically a conference room with three people. It took many hours and we were frantic to get it out to the to the airport within half an hour so that it could make it to the rocket on time. Now it\u2019s a completely different story where we have a whole manufacturing team. The MRP system that we use, the documentation of what is done, so we\u2019re not relying on the tribal knowledge of those three people that built the first couple of satellites, so that it can actually be done day in day out no matter who is building it, that everything is put together the right way. The team that we ended up hiring to do the manufacturing is again, it\u2019s not necessarily the engineers who designed it which is what it was initially. We\u2019ve got someone who came in from Blackberry doing high volume manufacturing. The team is not necessarily engineers who actually are putting all the parts together. It\u2019s a very different way of doing satellites that allows us to get to such a large number of them and then of course it\u2019s really all about the data and what you do with the data.</p><p><strong>Diane Gillespie:</strong> Tostan works in 22 different languages so you can imagine the scale up from Wolof which is the language they taught in Senegal. So when they expanded to Guinea Bissau you have Portuguese, well you can \u2014 I won\u2019t name them all. So all of the material, because they work in resource-poor communities that are essentially illiterate, you can\u2019t hand people textbooks. The program eventually over the three years does some work in literacy and numeracy but you have to create new classroom materials in the language because Tostan teaches in national languages. Funders don\u2019t appreciate that that expense has to be built into the process. I would also say that we had to totally change our M&amp;E system because, as you say, the sort of indigenous knowledge of the people who first worked in the program was not easily translatable to the new countries that Tostan went into, and we also need a system that can standardize across countries that have some cultural variation. So we have a grant from the Gates Foundation that\u2019s very interesting in terms of helping us do a monitoring and evaluation system that really gives programmed feedback country-to-country. Allows us to ask really important questions about why, Mali, for example is not \u2014 the attitudes there aren\u2019t changing as fast as in another country</p><p><strong>Ruth Grace Wong:</strong> Wow very interesting. For large projects you need a lot of people, so what necessary partnerships have you had to make in your work? Examples include partnering with government or outsourcing part of your project to another company or even relying on local people in the field. So how do you decide when to do something in-house versus relying on a partnership?</p><p><strong>Glenn Matlin:</strong> Yeah when we\u2019re thinking about if we\u2019re going to build it or buy it, we have to really decide long-term: does it make sense for us to really invest all this time into programming it, having people on staff to maintain it long term, and then dealing with at the updates that we\u2019re going to need to do inevitably? When we make that build or buy decision there are a few things we think about, the first of which is: is it our core competency? LendUp and the data science team at LendUp is really focused on one thing: making machine learning models to do better underwriting, get people more approvals with lower loss rates and pass that savings on to them. So we focus on that. As a result, a lot of times other data science teams at big companies might build their own BI solutions or visualizations or their own internal infrastructure. You have to decide; we can\u2019t necessarily do that. We can pull something off the shelf because there are tons of BI solutions or we can use Airflow or something else from another company that\u2019s open source. Oftentimes we try to just figure out, is this something we are really good at, and if it is we\u2019re going to keep doing it. Other times though especially because of the whole regulatory \u2014 I have to always talk about compliance \u2014 we really try to partner and work with the people that are monitoring us. We invite them in. We are very different than the Wells Fargos and Citi Banks of the world in that we really want them to come in, investigate us, look at us, help us understand that how we are doing things is how they expect us to stay compliant. We want to stay in that safe harbor area and really respect the rules because it\u2019s very important for us. In that vein we really try to partner with the state and federal agencies that do monitor us and help us understand what they\u2019re looking for when we do business.</p><p><strong>Hayley Cashdollar:</strong> I love your answer because I cannot echo enough that you have to think about what your core competency is and think about what your business does. For example, at Proterra we make electric buses and that necessitates making batteries and to make batteries you have to make a factory and that may be full of robots. We do not make robots. If we were to hire a bunch of people and invest in a team, which we could do \u2014 we could make a team of people that\u2019s incredible at designing pieces of equipment. But then what would they do after they\u2019re done? So it\u2019s better for us on projects like that that are outside of what our business model is set up to achieve to hire outside help for those. A lot of my job is actually co-designing with outside integrators and working with them to help us accomplish our automation needs on the manufacturing floor. We have a small team working on automation. Instead of the about 60 engineers that it took for these projects, only two of them work at Proterra. And I think that thinking about your business and the direction that you\u2019re going is really important when you\u2019re making those decisions.</p><p><strong>Theresa Condor:</strong> So I guess I\u2019ll say something that maybe is slightly controversial and I would say that partnerships are tricky. I think it\u2019s something where both sides need to have skin in the game and often that means monetary skin in the game in some way. Otherwise I think there\u2019s a long term chance that the partnership will not be successful and potentially even cause more damage. We build our satellites. We initially started thinking that we could use a lot of commercial off-the-shelf components, tap into the supply chain. What we want to focus on is the data and what you do with the data not per se the satellite constellation. What we ended up finding out is that we had more to lose about the parts when the parts are not delivered on time, not delivered to spec, do not work once there in space, than any of our suppliers did. So we ended up bringing pretty much everything in-house. We do the whole design of the spacecraft. We do all of the sub-system components ourselves. We integrate everything ourselves. We test everything ourselves. We do the whole chain minus the rocket launch. So in that sense we\u2019re not building rockets, but we\u2019re doing everything else. And that was a lesson for us in where do partnerships work and where do they not and how do you have to structure it. I would say the same thing can also be the case on the distribution side if you\u2019re working with resellers or you\u2019re partnering with international organizations or NGOs or any other type of organization. They can be powerful for sure they can also be a time sink if there\u2019s not a very clear understanding of alignment but also skin in the game, I would say.</p><p><strong>Diane Gillespie:</strong> Tostan partners with NGOs a lot but I think its\u2019 most valued partnership is with the social mobilizers who actually grow out of the educational experience they have and become, what we call, the agents of organized diffusion. They spread what they\u2019ve learned and they\u2019re very \u2014 when you do development from the inside out \u2014 they know how to work with their family members, they know how to work with their extended family members. So you have them leading their own development. That partnership has led to a couple of interesting developments. Tostan leaves in place what they call a community management committee, and the really dynamic ones have begun federating, and they are now independent NGOs that get their own money. The other thing that\u2019s happening is that, especially women are running for political office on human rights platforms. They\u2019re able now to affect policy at the local region. So I would say that the most valuable partnership Tostan really has is with the people who are emerging out of the classes that are taught.</p><p><strong>Ruth Grace Wong:</strong> That\u2019s incredible. I really like how I can ask the same question and then get different responses based on the context. The second last question is: when doing projects at a large scale important aspects of the work can be lost, for example product quality in manufacturing, or accommodating all your users in software, or even retaining the personal relationships that drive change in a social initiative. What aspects of your work are easier at scale and what aspects are the most challenging to do well at scale?</p><p><strong>Glenn Matlin:</strong> I think I\u2019ve kind of harped on the hard parts. It\u2019s edge cases it\u2019s dealing with the fact that when you\u2019ve got a million people, a billion dollars churning through your platform, something\u2019s going to go wrong; it\u2019s inevitable. Instead, I just want to harp on the parts that get easier, because I think that\u2019s really the benefit of the field. When you do things that scale in my field, you get a lot more data. And a lot more information is powerful because you need that information to make good inferences and good predictions. From the machine learning side, more data just means you have better predictions. We can get better loss rates, better approval rates. Again, we pass the savings on to the customer. It\u2019s fantastic to just have a huge amount of data churning through your platform. As another benefit from the data science side, when you\u2019ve got a lot of people and a lot of data suddenly you get to try some something new, try experimentation. It\u2019s hard when you\u2019ve got a small amount of customers to try anything new. You\u2019ve got to retain them and you\u2019ve got to make sure you\u2019re getting it right just for them, but when you\u2019ve got room to play with, suddenly you\u2019re finding yourself able to test new things. Just one small example from my work is that we\u2019re finding that just adjusting the default amount of a loan by $25 can drastically change repayment rates. It turns out that for people that we are lending to, that $25, that default that they normally run with because they\u2019re not tending to engage with the full site, makes a huge difference in repayment rates. It\u2019s small things like that that can really go a long way.</p><p><strong>Hayley Cashdollar:</strong> For us one of the challenges of scaling has been maintaining our quality, despite scaling. I have found that you have to communicate what you need from a quality standpoint in the right way. It\u2019s not just the binary, this is good, this is bad, it\u2019s why. And communicating that why can be very challenging in a technical environment when you\u2019re speaking to somebody with a completely different background than you. That\u2019s been a challenge that I\u2019ve been working on with my team a lot. The great thing about scaling is that things go so fast. Since April my company has made 30 times the number of batteries that we made in the prior year and it is so great to have that information available and get more buses out on the road</p><p><strong>Theresa Condor:</strong> I would echo that the great thing about scaling is that things go so fast, and you\u2019re just constantly surprised where you can be. We started the beginning of the summer at 20 satellites and now we\u2019re mid-August and we have 40 satellites. We just did three launches in the past two months and that\u2019s just neat and exciting. The difficult side is to go back to a little bit what I was talking about in one of the earlier questions, which is people. That is the thing that we worry about the most, and what we find the most challenging. When you\u2019re still a smaller company and you\u2019re the ten people, you can manage the culture. You have everyone coming in and they care about it, and they feel special because they\u2019re part of building something early on. That\u2019s so difficult to do as you grow. I mean, it\u2019s something everybody talks about in Silicon Valley of how do you do that as you add more people in it. I would say it really is a challenge to make sure that everybody is staying excited and driving towards the bigger picture when you don\u2019t necessarily know everyone and you\u2019re not touching and talking with them directly. Sometimes it feels like we spend more time almost dealing with internal things around culture and engagement and keeping everyone moving in the same direction than we spend with external related things. Sometimes that\u2019s frustrating but that\u2019s humans and humans are messy and at the end of the day that\u2019s just as important as the external facing side.</p><p><strong>Diane Gillespie:</strong> So I would put it in terms of training. That\u2019s the challenge when you\u2019re scaling up, especially if you\u2019re going to have fidelity to both the philosophy and the product and your terms, but in terms of the educational program. Training is a great challenge because you have to train facilitators in all of these different countries, but once the program starts the energy level that\u2019s produced is just, as they\u2019re saying, really phenomenal. Now with Tostan\u2019s programs, 7,000 communities have come together in public settings to declare their intention to abandon female genital cutting and child marriage, and they are celebrations. These are events that are planned by the people who come, the politicians come, everyone is involved in this. It\u2019s really a new social norm that is beginning. So once it is at scale it\u2019s just amazing</p><p><strong>Ruth Grace Wong</strong>: That\u2019s really cool. Okay so the last question is the one that are most curious about. Clearly you all do a lot of work. Many of you are doing startups or otherwise doing work with a high burnout rate. What personal sacrifices have you made for your work and do you have any advice for the audience about burnout and prioritizing what\u2019s important to you?</p><p><strong>Glenn Matlin:</strong> Yeah, I am looking forward to this question as well. This month I am taking my company\u2019s credit card to scale from thousands to hundreds of thousands. I am preparing my first international travel conference and workshops. I\u2019ve never travelled before, will certainly have to navigate that. I\u2019m moving apartments at the same time all this month. If you live in San Francisco you understand what a total nightmare that is. I\u2019m also trying to figure out how to be 30 at the same time. So this month has been very very busy and I have no time. So the truth is I\u2019m being held together mostly with coffee and glue. At any given moment I might fall apart, but the truth is the motivation of the enormity of what we\u2019re about to do with the credit card product really keeps me going. How to deal with it. I\u2019m a very happy guy. I\u2019m very optimistic. I am also very stoic about these kinds of things. My suggestions are prepare to fail. You\u2019re going to fail a lot. Everyone\u2019s going to tell you\u2019re wrong. You\u2019re going to have to get ready for things to go badly. You\u2019re going to have to prepare for that and learn how to pick yourself up a lot. Ultimately it\u2019s not about succeeding on every go. It really doesn\u2019t work that way. My mentor has pretty much said repeatedly: we gain success through repeated brutal and awful failure. That is how machine learning works best, and it turns out to do it well so it\u2019s a good philosophy to adapt when you\u2019re doing something new and difficult.</p><p><strong>Hayley Cashdollar:</strong> As far as personal burnout goes, I met you (Ruth) in the midst of one of the most challenging times of my career which was earlier this year where we had less than a month until our factory launched. It was a time where every single project I ever worked on was coming together and it was very challenging and time-consuming. One of the tools that I found was leaning on the people around me and helping get the people that I was working with as committed to those projects as I was, because then when I was low there\u2019s somebody to lean on and bring me back up. That is good for the times where I was working very hard and long hours, but I also intend to lead a cyclical life and I will have four months where I work non-stop, and I will plan that I have time to not work as hard, like right now where I actually will leave work on time for dinner sometimes. I think planning in advance that I will have a cyclical life and know that I have will have no social obligations from January to April but then months with travel and time with friends after that it makes those really hard pushes easy to get through because you know what\u2019s next.</p><p><strong>Theresa Condor:</strong> I think the reality is that burnout is a very real thing. I know there have been some pretty publicized debates recently in Silicon Valley about whether you can have it all and be successful and have balance, or whether you really do need to almost work yourself to death in order to accomplish something big. At least in our mind and our experience with our company is that sometimes if you want to do something that is very challenging and that you think will have a big impact and hasn\u2019t been done before the reality is you just have to run and work harder and faster than anyone else and I think that\u2019s a very real thing for us. We have gone all-in on this. My husband actually also works with our company. He\u2019s the CEO and a co-founder, so we\u2019ve been building this organization together. I was pregnant with our daughter when we were fundraising together for series A and when she was a month old he came home and said, well I think we have to move to Scotland where we\u2019re opening the manufacturing plant for the satellites. So we moved to Scotland as soon as she got her vaccinations and then from there it\u2019s just been, everywhere I go she travels with me, and I have babysitters and we\u2019re registered in daycares around the world, and so it\u2019s just we\u2019re all in. It\u2019s a constant whirlwind of managing everything. I think for us it is work that we\u2019re doing together as a family, and I think it would be very very difficult if you have someone with that level of focus and the spouse or partner is not involved in it and really has a hard time understanding why things need to be a certain way and so it\u2019s made it all in. I mean, we took that risk we have no side job or stable income outside of the company that we\u2019ve created and we\u2019re bouncing around between continents and our whole life is in this thing, but at the same time I think that\u2019s what\u2019s made it possible as well.</p><p><strong>Diane Gillespie:</strong> It\u2019s interesting that you\u2019ve asked this question. I\u2019m not the entrepreneur, actually Molly Melching is my sister so I\u2019ve seen firsthand burnout. However she\u2019s not burned out when she\u2019s close to the people, to the source. She gets burned out by bureaucracy, management kinds of issues. But <a href=\"https://www.ashoka.org/en\"><u>Ashoka</u></a> and the <a href=\"http://skoll.org/\"><u>Skoll foundation</u></a> have recognized that social entrepreneurs have a high burnout rate, and they have designed retreats \u2014 she\u2019s going through one of them right now \u2014 where people have a chance to sit back and do reflection and they can\u2019t talk about work. This has been transformative. I really recommend that people think about scheduling a time for reflection with others who are doing similar kind of work. I mean certainly you can reflect alone, a lot of people do meditation, but it\u2019s actually being with like-minded people who are experiencing similar kinds of pressures and frustrations to develop yourself outside of the boundaries of your work life.</p><p><strong>Ruth Grace Wong:</strong> Amazing. So that was the last question I have. We will be here for office hours until about five, five ten if you want to talk to any of the panelists and do we have time for audience questions?</p><p><strong>Helen Toner:</strong> I think rather than taking audience questions for everyone we should just thank our panel now and then you can come up and ask questions. The next talks will be in here at 5:15 which will be a round of lightning talks, so let\u2019s thank our panel.</p><p>[Applause]</p><p><i>Thanks to Alan Kalbfleish at </i><a href=\"https://www.pascalpress.com/\"><i><u>Pascal Press</u></i></a><i> for helping me edit this transcript, and Elizabeth Van Nostrand of </i><a href=\"http://acesounderglass.com/\"><i><u>AcesoUnderGlass.com</u></i></a><i> for facilitating bringing Diane to the panel</i></p>", "user": {"username": "ruthgrace"}}, {"_id": "5K5MpM6WnwwggMYbx", "title": "Lewis Bollard: Global farm animal welfare opportunities", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=nsXdIKyFof0&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=10\"><div><iframe src=\"https://www.youtube.com/embed/nsXdIKyFof0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "eiqYwbd5kWuXfFvkW", "title": "Catherine Hollander & Josh Rosenberg: GiveWell top charities and incubation grants", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=gQ2VkExFVhA&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=9\"><div><iframe src=\"https://www.youtube.com/embed/gQ2VkExFVhA\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "ALFp39F7qzTNZhHPn", "title": "Ofir Reich: Center for Effective Global Action", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=xMjijJk1WhY&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=8\"><div><iframe src=\"https://www.youtube.com/embed/xMjijJk1WhY\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "KC5PhJANXhiwbhMq5", "title": "Katja Grace: AI safety", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=r91Co5CeOCY&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=7\"><div><iframe src=\"https://www.youtube.com/embed/r91Co5CeOCY\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "PZkhfNMQ9MxoN7rp7", "title": "Duncan Sabien: Convinced, not convincing (San Francisco)", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<p><i>Duncan also </i><a href=\"https://forum.effectivealtruism.org/posts/5ugAidYCHvERXqiKo/duncan-sabien-convinced-not-convincing-boston\"><i>gave a version of this talk</i></a><i> at EA Global: Boston 2017.</i></p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=2QQEHUI1BUQ&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=6&amp;t=3s\"><div><iframe src=\"https://www.youtube.com/embed/2QQEHUI1BUQ\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this EA Global: San Francisco 2017 talk, Duncan Sabien presents an introduction to the Centre for Applied Rationality's tools for increasing motivation, avoiding mistakes, and collaborating effectively.</i></p><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "qCSotFYziHobA6JER", "title": "Holden Karnofsky: Fireside chat (2017)", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=uJB9qgZNk6U&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=5\"><div><iframe src=\"https://www.youtube.com/embed/uJB9qgZNk6U\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In the future, we may post a transcript for this EA Global: San Francisco 2017 talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "DGQHZZNMdjDghgu2S", "title": "Owen Cotton-Barratt: What does (and doesn't) AI mean for effective altruism?", "postedAt": "2017-08-11T08:19:00.000Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=gATWIWiIy_8&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=4&amp;t=7s\"><div><iframe src=\"https://www.youtube.com/embed/gATWIWiIy_8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this 2017 talk, The Future of Humanity Institute's </i><a href=\"https://www.fhi.ox.ac.uk/team/owen-cotton-barratt/\"><i>Owen Cotton-Barratt</i></a><i> discusses what strategy effective altruists ought to adopt with regards to the development of advanced artificial intelligence. He argues that we ought to adopt a portfolio approach \u2014 i.e., that we ought to invest resources in strategies relevant to several different AI scenarios. At the very end you will find an added section on what you can do to&nbsp;help.</i></p><p><i>The below transcript is lightly edited for&nbsp;readability.</i></p><h1>The Talk</h1><p>Some of you may have noticed that a bunch of people in this community seem to think that AI is a big deal. I was going to talk about that a little bit. I think that there are a few different ideas which feed into what we should be paying a lot of attention to. One is that from a moral perspective, the biggest impacts of our actions - and perhaps even overwhelmingly so - are the effects of our actions today on what happens in the long term future. Then there's some pretty empirical ideas. One is that artificial intelligence might be the most radically transformative technology that has ever been developed. Then actually artificial intelligence is something that we may be able to influence the development of. Influencing that could be a major lever over the future. If we think that our actions over the long term future are important, this could be one of the important mechanisms. Then as well, that artificial intelligence and the type of radically transformative artificial intelligence could plausibly be developed in the next few&nbsp;decades.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=1800&amp;q=70 1800w\"></figure><p>I don't know what you think of all of these claims. I tend to think that they're actually pretty plausible. For the rest of this talk, I'm going to be treating these as assumptions, and I want to explore the question: if we take these seriously, where does that get us? If you already roughly agree with these, then you can just have a like sit back and see how much you agree with the analysis, and maybe that's relevant for you. If you don't agree with one of those claims, then you can treat this as an exercise in understanding how other bits of the community might think. Maybe some of the ideas will actually be usefully transferrable. Either way, if there are some of these that you haven't thought much about before, I encourage you to go and think about them - take some time afterwards. Because it seems to me at least that these are, each of these ideas is something which potentially has large implications for how we should be engaging in the world in this project of trying to help it. It seems like it's therefore the kind of thing which is worth having an opinion&nbsp;on.</p><p>Okay, so I'm going to be exploring where this gets us. I think a cartoon view people sometimes hold is if you believe in these ideas, then you think everybody should quit what they're working on, and drop everything, and go and work on the problem of AI safety. I think this is wrong. I think there are some related ideas in that vicinity where there's some truth. But it's a much more nuanced picture. I think for most people, it is not correct to just quit what they're doing, to work on something safety related instead. But I think it's worth understanding in what kind of circumstances it might be correct to do that, and also how the different pieces of the AI safety puzzle fit&nbsp;together.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=1800&amp;q=70 1800w\"></figure><p>I think that thinking about timelines is important for AI. It is very hard to have any high level of confidence in when AI might have different capabilities. Predicting technology is hard, so it's appropriate to have uncertainty. In fact, here's a&nbsp;graph.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=1800&amp;q=70 1800w\"></figure><p>You can see the bunch of faint lines showing individual estimates of people working in machine learning research of when they expect high level AI to be developed. Then this bold red thing is the median of those. That's quite a lot of uncertainty. If you take almost any individual's view, and certainly this aggregate view, that represents very significant uncertainty over when transformative AI might occur. So we should be thinking about&nbsp;that.</p><p>Really our uncertainty should follow some kind of smooth distribution. For this talk, I'm gonna talk about four different scenarios. I think that the advantage of anchoring the possibilities as particular scenarios and treating them as discrete rather than continuous is that it becomes easier to communicate about, and it becomes easier to visualize, and think, \"Okay, well what would you actually do if the timeline has this type of&nbsp;length?\"</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=1800&amp;q=70 1800w\"></figure><p>The first scenario represents imminent AI, maybe something on the scale of 0 to 10 years away. In this case, it's more likely that we actually know or can make educated guesses already about who the important actors will be around the development of&nbsp;AI.</p><p>I want to explore a little bit about what strategies we might pursue based on each of these different timelines. If you assume this first one, then there's no time for long processes. If your idea was, \"Well, I'll do a degree in CS, and then I'll go and get a PhD in machine learning, and then I'll go into research,\" you're too late. On the other hand, if you are already in a position where you might be able to do something in the short term, then it could be worth paying attention to. But I feel for a lot of people, even if you think there is some small chance of this first scenario happening (which in general you want to pay attention to) it may be that there isn't a meaningful way to&nbsp;engage.</p><p>The next possible scenario would be maybe between 10 and 25 years out. This is a timescale in which people can naturally build careers. They can go and they can learn things. They can develop networks. They can build institutions. They can also build academic fields. You can ask questions, get people motivated, and get them interested in the framing of the question that you think is important. You can also have time for some synthesis and development of relevant ideas. I think that building networks where we persuade other people who maybe aren't yet in a direct position of influence, but might be later, can be a good&nbsp;idea.</p><p>If we look a bit further to another possible scenario, maybe between 25 to 60 years out, that's a timescale at which people who are in the important fields today may be retiring. Paradigms in academic fields might have shifted multiple times. It becomes hard to take a zoomed in view of what it is that we need, but this means that it's more important and build things right rather than quickly. We want to build solid foundations for whatever the important fields are. When I say the important fields here, I'm thinking significantly about technical fields of how we build systems which do what we actually want them to do. I'm also thinking about the kind of governance, policy, and processes in our society around AI. Who should develop AI? How should that be structured? Who is going to end up with control over the things which are&nbsp;produced?</p><p>These scenrios are all cartoons. I'm presenting a couple of stylized facts about each kind of timeline. There will be a bit of overlap of these strategies, but just to give an idea of how actually the ideal strategy changes. Okay. The very distant maybe more than 60 years out, anything, maybe it's even hundreds of years, at this level predictability gets extremely low. If it takes us this long to develop radically transformative AI, it is quite likely that something else radically transformative will have happened to our society in the meanwhile. We're less likely to predict what the relevant problems will be. Instead, it makes sense to think of a strategy of building broad institutions, which are going to equip the people of that time to better deal with the challenges that they're facing&nbsp;then.</p><p>I think actually it's plausible that the effective altruism community, and the set of ideas around that community, might be one broad, useful institution for people of the far future. If we can empower people with tools to work out what is actually correct, and the motivation and support to act on their results, then I'd be like, \"Yep, I think we can trust those future people to do&nbsp;that.\"</p><p>The very long term is the timescale at which other very transformative things occurring in our society are more likely to happen. This can happen on the shorter timescales as well. But if you think on a very long timescale, there is much more reason to put more resources toward other big potential transitions, rather than just AI. I think that AI could be a big deal, but it's definitely not the only thing that could be a big&nbsp;deal.</p><p>Okay. I've just like talked us through different timelines. I think that most reasonable people I know put at least some nontrivial probability on each of these possible scenarios. I've also just outlined how we probably want to do different things for the different scenarios. Given all of this, what should we actually be doing? One approach is to say, \"Well, let's not take these on the timelines. Let's just do things that we think are kind of generically good for all of the different timelines.\" I think that that's a bad strategy because I think it may miss the best opportunities. There may be some things which you only notice are good if you're thinking of something more concrete rather than just an abstract, \"Oh, there's gonna be AI at some point in the future.\" Perhaps for the shorter timelines, that might involve going and talking to people who might be in a position to have any effect in the short term, and working out, \"Can I help you with&nbsp;anything?\"</p><p>Okay. The next kind of obvious thing to consider is, well, let's work out which of these scenarios is the most likely. But if you do this, I think you're missing something very important, which is that we might have different degrees of leverage over these different scenarios. The community might have different leverage available for each scenario. It can also vary by individual. For the short timelines, probably leverage is much more heterogeneous between different people. Some people might be in a position to have influence, in that case it might be that they have the highest leverage there. By leverage, I mean roughly, conditional on that scenario actually pertaining, how much does you putting in a year of work, trying your best, have an effect on the outcome? Something like&nbsp;that.</p><p>Okay. Maybe we should just be going for the highest likelihood multiplied by leverage. This of course is like the place where we have the most expected impact. I think there's something to that. I think that if everybody properly does that analysis for themselves and updates as people go and take more actions in the world, then in theory that should get you to the right things. But the leverage of different opportunities varies both as people take more opportunities and also even just for an individual. I've known people who think that they've had different opportunities they can take to help short timelines and then a bunch of other opportunities to help with long timelines. This is a reason not to naively go for highest likelihood multiplied by&nbsp;leverage.</p><p>Okay. What else? Well, can we think about what portfolio of things we could do? I was really happy about the theme of this event because thinking about the portfolio and acting under uncertainty is something I've been researching for the past two or three years. On this approach, I think we want to collectively discuss the probabilities of different scenarios, the amount of leverage we might have for each, and the diminishing returns that we have on work aimed at each. Then also we should discuss about what that ideal portfolio should look like. I say collectively because this is all information where when we work things out for ourselves, we can help inform others about it as well, and we can probably do better using collective epistemology than we can&nbsp;individually.</p><p>Then we can individually consider, \"Okay, how do I think in fact the community is deviating from the ideal portfolio? What can I do to correct that?\" Also, \"What is my comparative advantage here?\" Okay. I want to say a couple of words about comparative advantage. I think you know the basic idea. Here's the cartoon I think of it in terms&nbsp;of:</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=1800&amp;q=70 1800w\"></figure><p>You've got Harry, Hermione, and Ron, they have three tasks to do, and they've gotta do one task each. Hermione is best at everything, but you can't just get Hermione to do all the things. You have to allocate them one to one. So it's a question of how do you line the people up to the things so that you have everyone doing something that they're pretty good at it, and overall you get all of the important things done? I think that this is something that we can think of at the level of individuals choosing, \"What am I going to work on? Well, I've got this kind of skillset.\" It's something that we can think of at the level of groups as well. We can ask, \"What is my little local community going to work on?\" or \"What is this organization going to do, and how do we split up responsibility between different&nbsp;organizations?\"</p><p>Comparative advantage is also a concept you can think of applied over time. This is a little bit different because people's actions in the past are fixed, so we can't affect those. But you can think there's things that might want to be done and we can do some of these. People in the past did some of them. People in the future might do some of them and there's a coordination question of what we have a comparative advantage at relative to people in the future. This is why when I was looking at longer scenarios, the next generation in the distant cases, I was often thinking it was better to let people in the future solve the concrete problems. They're gonna be able to see more clearly what is actually to be solved. Meanwhile, we have a comparative advantage at building the processes, the communities, the institutions which compound over time, and where getting in early is really&nbsp;helpful.</p><p>If you're taking something like this portfolio approach, I think that most projects should normally have at least a main scenario in mind. This forces you to be a little bit more concrete and to check that the things you're thinking of doing actually line up well with the things which are needed in some possible world. I also think you want to be a bit careful about checking that you're not doing anything which would be bad for other scenarios. There's always an opportunity cost. If you're doing something where you're thinking, \"I want to help with this short timeline scenario,\" then you're not doing something else you could've done to help with the next generation in a longer timeline&nbsp;scenario.</p><p>You could also have situations where maybe I would think that if AI is imminent, the right thing to do is to run around and say, \"Everybody panic. AI is coming in five years. It's definitely coming in five years.\" If it definitely were coming in five years, maybe that would be the right thing to do. I actually don't think it is. Even if it were, I think that would be a terrible idea because if you did that, then people, if it <i>didn't</i> occur in five years and we were actually in a world where radically transformative AI was coming in 25 years, then in 15 years, a lot of people are gonna go, \"We've heard that before,\" and not want to pay attention to it. This is a reason to have an idea of paying some attention to the whole idea of the portfolio that as a community we want to be paying attention to even if individually, most projects should have a main scenario in mind. Maybe as an individual, your whole body of work has a main scenario in mind. It's still worth having an awareness of where other people are coming from, and what they're working on, and what we're doing collectively&nbsp;then.</p><p>I've mostly talked about timelines here. I think that there are some other significant uncertainties about AI. For instance, how much is it that we should be focusing on trying to reduce the chances of catastrophic accidents from powerful AI? Or how much of the risk is coming from people abusing powerful technologies? We hypothesized it was gonna be a radically transformative technology with influence over the future. How much of that influence actually comes through things which are fairly tightly linked to the AI development process? Or how much influence appears after AI is developed? If most of the influence comes from what people want in the world after an AI is developed, it might makes sense to try to affect people's wants at that&nbsp;point.</p><p>In both of these cases, I think we might do something similar to portfolio thinking. We might say, \"Well, we've put some weight on each of these possibilities,\" and then we think about our leverage again. Maybe for some of them, we shouldn't be split. Some of them we might do. We can't do this with all of the uncertainties. There are a lot of uncertainties about&nbsp;AI.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=1800&amp;q=70 1800w\"></figure><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=1800&amp;q=70 1800w\"></figure><p>Here's a slide from another talk. It just lists a lot of questions. A lot of them about how AI might develop. We can all have nuanced views about each of these questions. That's fine. We need to do some picking and choosing here. But I do think that we should strive for nuance. I think the reason is that there's a lot of uncertainty, and we could potentially have extremely nuanced views about a lot of different things. The world is complicated, and we have a moderately limited understanding of it. One of the things which may make us better equipped for the future is trying to reduce our limits on our&nbsp;understanding.</p><p>What can individuals do? I think consider personal comparative advantage. You can ask yourself, \"Could I seriously be a professional researcher in this?\" Check with others as well. I think people vary in their levels of self-confidence, so I actually think that others' opinions often can be more grounding than our own opinion for this. It's a pretty specialized skillset that I think is useful for doing technical safety research. Most people in the community are not gonna end up with that skillset and that's fine. They should not be quitting their jobs, and going to try and work on safety research. They could be saying, \"Well, I want to give money to support this,\" or they could be aiming at other parts of this portfolio. They could say, \"Well, I want to help develop our institutions to build something where we're gonna be better placed to deal with some of the longer timeline&nbsp;scenarios.\"</p><p>You could also diversify around those original assumptions that I made. I think that each of them is pretty likely to be true. But I don't think we should assume that they are all definitely true. We can check whether in fact there are worlds where they're not true that we want to be putting some significant weight onto. I think also just helping promote good community epistemics is something that we can all play a part in. By this I mean pay attention to why we believe things and communicate our real reasons to people. Sometimes you believe a thing because of a reason like: \"Well, I read this in a blog post by Carl Shulman, and he's really smart.\" He might provide some reasons in that blog post, and I might be able to pallet the reasons a little bit. But if the reason I really believe it is I read that, that's useful to communicate to other people because then they know where the truth is grounded in the statements I'm making, and it may help them to be able to better see things for themselves, and work things out. I also think we do want to often pay attention to trying to see the underlying truth for ourself. Good community epistemics is one of these institutions which I think are helpful for the longer timelines, but I think they're also helpful for our community over shorter periods. If we want to have a portfolio, we are going to have to coordinate and exchange views on what the important truths&nbsp;are.</p><figure class=\"image image_resized\" style=\"width:750px\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=1800&amp;q=70 1800w\"></figure><p>What does AI mean for effective altruism? My view is that it isn't the one thing that everyone has to pay attention to, but it is very plausibly a big part of this uncertain world stretching out in front of us. I think that we collectively should be paying attention to that and working out what we can do, so we can help increase the likelihood of good outcomes for the long-term&nbsp;future.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "F8xoTeDwuzovcgmgE", "title": "Will MacAskill: Doing good together", "postedAt": "2017-08-11T08:19:53.812Z", "htmlBody": "<figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=DQbrtJYWPlU&amp;list=PLwp9xeoX5p8NnWYsybl_ZRMtaK7uBr4sN&amp;index=3\"><div><iframe src=\"https://www.youtube.com/embed/DQbrtJYWPlU\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><hr><p><i>In this keynote from EA Global: San Francisco 2017, Will MacAskill highlights and encourages the transition from effective altruists doing good as individuals, to effective altruism doing good as a coordinated community.</i></p><p><i>In the future, we may post a transcript for this talk, but we haven't created one yet. If you'd like to create a transcript for this talk, contact </i><a href=\"mailto:aaron.gertler@centreforeffectivealtruism.org\"><i>Aaron Gertler</i></a><i> \u2014 he can help you get started.</i></p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "kMkoseGXdMGXhipui", "title": "Suffering in Animals vs. Humans", "postedAt": "2017-08-15T00:03:26.096Z", "htmlBody": "<h2>Summary</h2><p>There's a wide consensus that at least higher animals can consciously suffer. Even if we had doubts about this fact, it wouldn't much affect our expected-value calculations, because animals are so much more numerous than humans. It's sometimes claimed that humans suffer more intensely than animals because of deeper emotional experiences, but I think that raw pain itself represents a nontrivial fraction of the total badness of suffering, and even if we did count animals less, it again wouldn't much affect calculations (given their numbers).</p><h2>Animal consciousness</h2><p>Nearly all modern-day scientists agree that at least mammals and birds are almost certainly conscious of their emotions. The \"<a href=\"https://en.wikipedia.org/wiki/Animal_consciousness#Cambridge_Declaration_on_Consciousness\">Cambridge Declaration on Consciousness</a>\" was one clear expression of this consensus.</p><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"https://reducing-suffering.org/wp-content/uploads/2014/09/Bobcat_having_caught_a_rabbit.jpg\"></figure><p>That animals can consciously suffer almost needs no discussion, but agnostics remain, some of them noble in spirit.</p><p>One agnostic animal advocate is Marian Stamp Dawkins, whose \"<a href=\"http://www.huffingtonpost.com/marian-stamp-dawkins/animal-welfare_b_1581615.html\">Convincing the Unconvinced That Animal Welfare Matters</a>\" encouraged animal advocates not to claim that science knows that animals are conscious and instead to advance other reasons for caring about animals, including human-centered ones. In reply, Marc Bekoff wrote \"<a href=\"http://www.huffingtonpost.com/marc-bekoff/animal-consciousness_b_1519000.html\">Dawkins' Dangerous Idea: We Really Don't Know If Animals Are Conscious</a>,\" arguing that Dawkins was ignoring too much of the overwhelming case for animal consciousness that already exists. There are many more debates of this type, but I'll focus on the Dawkins-Bekoff exchange here.</p><h3>Scientific certainty vs. practical action</h3><p>I think the most important distinction that needs to be made is between \"certainty\" in science and \"certainty\" in ethics. Dawkins is right that science should remain skeptical of animal consciousness and that we should seek out proof independent of existing assumptions.</p><p>But while Dawkins is correct that we don't know \"for certain\" whether animals are conscious, this statement is misleading to many laypeople who assume that she must mean the odds are around 50%. I don't know what she thinks the odds actually are, but I would give above an 80% chance of chicken consciousness and above, say, 85% for pig consciousness (compared against maybe <a href=\"https://en.wikipedia.org/wiki/Solipsism\">95%</a> that you're conscious). With odds like that, it's best to say that the case is proved or else the public will misunderstand. Many people are not motivated by less than absolute certainty, and I think Bekoff is right that emphasizing scientific doubt is going to hurt animals on average. (Just look at what talking about uncertainty does for the global-warming debate.)</p><h3>Evidence for animal consciousness</h3><p>Now, Dawkins is totally correct that we don't understand exactly why animals are conscious. Indeed, we don't even know why people are conscious. What exactly does being conscious allow you to do that you can't do if you're not conscious? As <a href=\"http://en.wikipedia.org/wiki/Blindsight\">blindsight</a> demonstrates, you can walk and avoid objects without being conscious of them. If we ourselves didn't experience our consciousness through our own minds, then we would definitely have scientific doubts about whether people are conscious, too.</p><p>There are manifold articles demonstrating sophisticated, self-reflective behavior in animals that Bekoff and others take to imply consciousness, and indeed these are excellent pieces of evidence. However, they are not conclusive proof of consciousness because we can't even prove that humans are conscious using such tests at the present time. (In the future, once we really understand how consciousness works in the brain, we should be able to assess consciousness just by looking at the brain and what algorithms it's running.)</p><p>I think arguably the strongest reason we should believe animals are conscious is that they're close to us on the evolutionary tree, and their brain structures are remarkably similar. In \"<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/14658059\">New evidence of animal consciousness</a>\" (2004), Donald R. Griffin and Gayle B. Speck note that \"the search for neural correlates of consciousness has not found any consciousness-producing structure or process that is limited to human brains\" (p. 1). And in \"<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3274778/\">Building a neuroscience of pleasure and well-being</a>\" (2012), Kent C. Berridge and Morten L. Kringelbach comment:</p><blockquote><p>Progress has been facilitated by the recognition that hedonic brain mechanisms are largely shared between humans and other mammals, allowing application of conclusions from animal studies to a better understanding of human pleasures. [...]</p><p>Some might be surprised by high similarity across species, or by substantial subcortical contributions, at least if one thinks of pleasure as uniquely human and as emerging only at the top of the brain. The neural similarity indicates an early phylogenetic appearance of neural circuits for pleasure and a conservation of those circuits, including deep brain circuits, in the elaboration of later species, including humans.</p></blockquote><p>Probably dozens of other papers could be quoted in a similar fashion. Based on this, 50% is too low as a probability for mammal and bird consciousness.</p><h3>Should we use other arguments to help animals?</h3><p>What about the effort that Dawkins proposes: Making people care about animals for human-welfare reasons? If we could press a button to do this, I'd be in favor of it. But when we're parceling out our scarce resources for helping animals, I think this undertaking should go pretty low on the priority list. It's great if we can help animals in the short term in this way, but if we're going to prevent future humans from <a href=\"https://reducing-suffering.org/will-space-colonization-multiply-wild-animal-suffering/\">multiplying wild-animal suffering into the galaxy</a>, we had better make sure our descendants actually care about animals. Even if helping animals sometimes coincides with human interests today, it's not guaranteed that will remain true in the future.</p><p>Finally, I did like this statement from Dawkins, as quoted in <a href=\"http://www.huffingtonpost.com/marc-bekoff/animal-consciousness_b_1519000.html\">Bekoff's article</a>: \" ... it is much, much better for animals if we remain skeptical and agnostic [about consciousness] ... Militantly agnostic if necessary, because this keeps alive the possibility that a large number of species have some sort of conscious experiences ... For all we know, many animals, not just the clever ones and not just the overtly emotional ones, also have conscious experiences.\" (p. 177) It's totally appropriate to talk about probabilities and expected values in the right context, but my complaint to Dawkins is that among the general public, the language of uncertainty makes people confused and less motivated.</p><h2>Is animal suffering less bad than human suffering?</h2><p>Even if people agree that animals can suffer, they may suggest that animals suffer less intensely because they don't have the same high-level mental suffering that humans do.</p><figure class=\"image image_resized\" style=\"width:100%\"><img src=\"https://reducing-suffering.org/wp-content/uploads/2014/10/Ochotona_princeps_rockies.jpg\"></figure><p>In response, I would first point out that it's unclear whether the claim is true that animals have substantially less sophisticated mentation, at least \"higher\" animals like mammals. Animals <a href=\"http://en.wikipedia.org/wiki/Animal_psychopathology\">show many of the psychopathologies</a> that humans do and <a href=\"http://en.wikipedia.org/wiki/Animal_model_of_depression\">are used as models for depression</a> when testing drugs. Elephants <a href=\"http://en.wikipedia.org/wiki/Elephant_cognition#Death_ritual\">have death rituals</a>. Crows <a href=\"http://www.theatlantic.com/technology/archive/2012/01/science-can-neither-explain-nor-deny-the-awesomeness-of-this-sledding-crow/251395/\">appear to go sledding for fun</a>. Marc Bekoff, Jonathan Balcombe, and other ethologists have written numerous books documenting the complex emotional lives of mammals, birds, fish, and <a href=\"http://www.thecephalopodpage.org/behavior.php\">even octopuses</a>.</p><p>Secondly, what if we thought animals did suffer less? Well, I guess I would ask, How much less do they suffer? I don't think it's orders of magnitude less, and if not, then the basic calculations showing that, at the margin, animal welfare takes priority over human welfare would remain. Suppose you were a <a href=\"https://www.onegreenplanet.org/animalsandnature/the-u-s-chicken-industry-must-stop-boiling-birds-alive/\">chicken being scalded and drowned alive in a boiling defeathering tank</a>. How much less bad would this experience be if you didn't have broader thoughts about the end of your life, the injustice of your situation, how much you'll miss your friends, etc.? I suspect that the raw physical pain would overwhelm these subsidiary thoughts during the moment, and even if not, I don't think the higher-level thoughts would be vastly stronger than the raw pain.</p><p>Finally, there are many times when humans may in fact suffer less because of their understanding of the situation. Humans enduring a bout of food poisoning can know that the agony will end after a day or two and that their friends and family will help them in the mean time. Animals going through the same experience may have no idea what's happening to them, whether it will end, or what will become of their lives.</p><p>While it may be an illusion, I subjectively feel as though my daily experiences are less emotional, and hence less morally important from a hedonistic perspective, now that I'm an adult compared with when I was a child, because as a child I had less ability to regulate my emotional state.</p><p><a href=\"https://pdfs.semanticscholar.org/a0e7/7be0ec25556402e9378c6c33961b83f16393.pdf\">Rolls (2008)</a>, p. 152:</p><blockquote><p>In humans, grief may be particularly potent because it becomes represented in a system which can plan ahead, and understand the enduring implications of the loss. (Thinking about or verbally discussing emotional states may also in these circumstances help, because this can lead towards the identification of new or alternative reinforcers, and of the realization that, for example, negative consequences may not be as bad as feared.)</p></blockquote><p>The points discussed above are fascinating to ponder, and it's valuable to hear from other people which of their own experiences they've found most unpleasant. That said, we modern humans live extremely comfortable lives compared with factory-farmed or wild animals, so it isn't surprising that most of our worst memories may be of purely emotional injury. In any event, regardless of where we settle on the question of the relative magnitudes of animal and human pain, physical and psychological pain, I don't think it's likely to tip the balance of our calculations about where our dollars and hours will <a href=\"https://reducing-suffering.org/donation-recommendations/\">do the most good</a>.</p><h2>Importance of self-awareness?</h2><p><a href=\"http://doi.org/10.1073/pnas.1619316114\">LeDoux and Brown (2017)</a> consider a representation of <i>self</i> as crucial for emotion:</p><blockquote><p>Without the self there is no fear or love or joy. If some event is not affecting you, then it is not producing an emotion. When your friend or child suffers you feel it because they are part of you. When the suffering of people you don\u2019t know affects you emotionally, it is because you empathize with them (put yourself in their place, feel their pain): no you, no emotion. The self is, as noted above, the glue that ties such multidimensional integrated representations together (156).</p></blockquote><p>This idea that selfhood is important for emotion strikes me as odd. My imagination of a strong emotion like pain is that it's mostly a message of \"bad thing happening now!\", and situating that pain within the context of me as someone experiencing it is merely icing on the cake. What matters most about pain is the strong motivation it produces, and that motivation doesn't seem to depend significantly on a concept of myself as the experiencer.</p><p>LeDoux and Brown add:</p><blockquote><p>Tulving argued that autonoetic consciousness is an exclusive feature of the human brain (135). Other animals could, in principle, experience noetic states about being in danger. However, because such states lack the involvement of the self, as a result of the absence of autonoetic awareness, the states would not, in our view, be emotions.</p></blockquote><p>Even if this hypothesis is true (for some sufficiently specific notion of \"autonoetic consciousness\"), it seems overly parochial to me to only count those kinds of minds as having emotions.</p>", "user": {"username": "Brian_Tomasik"}}, {"_id": "ZFWnamW5R5dysmHK6", "title": "Three wild speculations from amateur quantitative macrohistory", "postedAt": "2017-09-12T09:47:51.112Z", "htmlBody": "<p><em>Note: As usual, these are my personal guesses and opinions, not those of my employer.</em></p>\n<p>In <a href=\"http://lukemuehlhauser.com/industrial-revolution/\">How big a deal was the Industrial Revolution?</a>, I looked for measures (or proxy measures) of human well-being / empowerment for which we have \u201cdecent\u201d scholarly estimates of the global average going back thousands of years. For reasons elaborated at some length in the full report, I ended up going with:</p>\n<ol>\n<li>Physical health, as measured by <em>life expectancy at birth.</em></li>\n<li>Economic well-being, as measured by <em>GDP per capita (PPP)</em> and <em>percent of people living in extreme poverty</em>.</li>\n<li><em>Energy capture</em>, in kilocalories per person per day.</li>\n<li>Technological empowerment, as measured by <em>war-making capacity</em>.</li>\n<li>Political freedom to live the kind of life one wants to live, as measured by <em>percent of people living in a democracy</em>.</li>\n</ol>\n<p>(I also especially wanted measures of subjective well-being and social well-being, and also of political freedom as measured by global rates of slavery, but these data aren\u2019t available; see the report.)</p>\n<p>Anyway, the punchline of the report is that when you chart these six measures over the past few millennia (<a href=\"https://docs.google.com/spreadsheets/d/1JBN9bd7aSI4_IxXzmrmXhy4em8z88xL0Z3BbT_N9c1Q/edit#gid=0\">data</a>; <a href=\"https://jsfiddle.net/lukeprog/3k77r1j7/11/\">zoomable</a>), you get a chart like this (axes removed for space reasons):</p>\n<p><img src=\"https://lukemuehlhauser.com/wp-content/uploads/all-curves-with-events.png\" alt=\"all curves with events\"></p>\n<p>(And yes, there\u2019s still a sharp jump around 1800-1870 if you <a href=\"https://jsfiddle.net/zcghLjfy/2/\">chart this on a log scale</a>.<sup class=\"footnote-ref\"><a href=\"#fn-699H2nonLkv4tZzJe-1\" id=\"fnref-699H2nonLkv4tZzJe-1\">[1]</a></sup>)</p>\n<p>Basically, if I help myself to the common (but certainly debatable) assumption that \u201cthe industrial revolution\u201d is the primary cause of the dramatic trajectory change in human welfare around 1800-1870,<sup class=\"footnote-ref\"><a href=\"#fn-699H2nonLkv4tZzJe-2\" id=\"fnref-699H2nonLkv4tZzJe-2\">[2]</a></sup> then my one-sentence summary of recorded human history is this:</p>\n<blockquote>\n<p>Everything was awful for a very long time, and then the industrial revolution happened.</p>\n</blockquote>\n<p>Interestingly, this is <em>not</em> the impression of history I got from the world history books I read in school. Those books tended to go on at length about the transformative impact of the wheel or writing or money or cavalry, or the conquering of this society by that other society, or the rise of this or that religion, or the disintegration of the Western Roman Empire, or the Black Death, or the Protestant Reformation, or the Scientific Revolution.</p>\n<p>But they could have ended each of those chapters by saying \u201cDespite these developments, global human well-being remained roughly the same as it had been for millennia, by every measure we have access to.\u201d<sup class=\"footnote-ref\"><a href=\"#fn-699H2nonLkv4tZzJe-3\" id=\"fnref-699H2nonLkv4tZzJe-3\">[3]</a></sup> And then when you got to the chapter on the industrial revolution, these books could\u2019ve said: \u201cFinally, for the first time in recorded history, the trajectory of human well-being changed completely, and this change dwarfed the magnitude of all previous fluctuations in human well-being.\u201d</p>\n<p>This is especially clear if we look at at GDP per capita, for which we have especially detailed (but still quite uncertain!) estimates. Here\u2019s GDP per capita (PPP) from 1-1800 CE:</p>\n<p><img src=\"https://lukemuehlhauser.com/wp-content/uploads/GDP-per-capita-1-1800.png\" alt=\"GDP per capita 1800\"></p>\n<p>If this is roughly accurate, then the drop in GDP per capita from 1000 to 1300 probably felt pretty awful to those living at the time, and the subsequent recovery probably felt pretty great. But these sorts of fluctuations are so small compared to what happened <em>after</em> the industrial revolution that they show up as a flat line when we include the post-industrial era:</p>\n<p><img src=\"http://lukemuehlhauser.com/wp-content/uploads/GDP-per-capita.png\" alt=\"GDP per capita\"></p>\n<p>That said, the long-run estimates I rely on are pretty uncertain, and my particular choice of measures to capture \u201cwell-being\u201d is obviously questionable (and to some, no doubt <em>objectionable</em>), so I will still refer to the basic picture presented above as \u201cspeculative.\u201d<sup class=\"footnote-ref\"><a href=\"#fn-699H2nonLkv4tZzJe-4\" id=\"fnref-699H2nonLkv4tZzJe-4\">[4]</a></sup></p>\n<p>Thus I\u2019ll say my <strong>first speculation</strong> from my brief expedition in \u201cquantiative macrohistory\u201d is this:</p>\n<blockquote>\n<p>Human well-being was pretty awful by modern standards until the industrial revolution, after which most things we care about got <em>vastly</em> better in the span of a century or two.</p>\n</blockquote>\n<p>But that is probably not a surprising claim to most readers of this blog, especially those who have studied economic history.</p>\n<p>Fortunately, my <strong>second speculation</strong> is probably even more speculative and controversial:</p>\n<blockquote>\n<p>If we had all the data, and we did a factor analysis of \u201cwhat mattered for human well-being in recorded history,\u201d I suspect most of the variance in human well-being would be explained by a primary factor for <em>productivity</em>, and a secondary factor for <em>political freedom</em>.</p>\n</blockquote>\n<p>The first factor probably isn\u2019t surprising. Technological progress, energy capture, other aspects of productivity, and wealth all feed on each other and are in some cases hard to even distinguish, and they all generally improve physical health and many other things we care about.</p>\n<p>But why do I suggest <em>political freedom</em> as a plausible second factor, given that \u201cpercent of people living in a democracy\u201d tracks so closely with all the other measures of well-being covered above? Largely, it\u2019s because I think \u201cpercent of people enslaved\u201d is a more important measure of political freedom than democracy is, and while I <a href=\"http://lukemuehlhauser.com/industrial-revolution/#SlaveryMethodology\">wasn\u2019t able</a> to collect/guess enough data points to chart global slavery over time, I read enough of the history of slavery to get the impression that <em>if</em> I could chart it, it might not track that well with the other measures discussed above \u2014 at least, not until about 1900.</p>\n<p>Instead of being well-represented by a simple hockey-stick, my rough guess is that \u201cpercent of people enslaved\u201d\u2026</p>\n<ul>\n<li>\u2026was quite low near the dawn of recorded history, and gradually rose until about 500 CE,</li>\n<li>held steady or diminished somewhat from 500-1500, and then</li>\n<li>exploded upward during the international slave trade, and finally</li>\n<li>dropped precipitously as the anti-slavery movement made its way around the world.</li>\n</ul>\n<p>Anyway, if we run with this wild speculation, then we might conclude that to improve human well-being going forward, there are basically two great battles to be fought: one for greater productivity, and the other for greater political freedom. Except, that hasn\u2019t <em>actually</em> been the case since the invention of nuclear weapons, due to <a href=\"http://www.existential-risk.org/concept.pdf\">existential risk</a>.</p>\n<p>Perhaps the biggest reason to be skeptical of this 2nd speculation is that it\u2019s based on measures for which I was able to find long-run data, and that means it can\u2019t make use of data on various aspects of subjective well-being (e.g. moment-to-moment happiness, or sense of meaning) or aspects of social well-being (e.g. sense of community). Perhaps those would be clear major factors if we had long-run data for them. But perhaps not!<sup class=\"footnote-ref\"><a href=\"#fn-699H2nonLkv4tZzJe-5\" id=\"fnref-699H2nonLkv4tZzJe-5\">[5]</a></sup></p>\n<p>My <strong>third speculation</strong> is a happier thought:</p>\n<blockquote>\n<p>Fortunately, it seems it would take a <em>lot</em> of deaths \u2014 maybe 15% of world population or even more \u2014 to knock civilization off its current positive trajectory (via deaths, anyway).</p>\n</blockquote>\n<p>This speculation results from <a href=\"http://lukemuehlhauser.com/industrial-revolution/#Negative\">cataloguing</a> the deadliest events in history, and finding that the worst of them (the Black Death and Genghis Khan) each killed about 10% of world population, and the deadliest event <em>since</em> our current positive trajectory began (with the industrial revolution) was the World War I + Spanish flu double whammy, which killed 4.1% of world population. At a glance, none of these events seem to have \u201ccome close\u201d to prompting a downward spiral akin to a \u201cnegative industrial revolution\u201d or (in the case of WWI+flu) knocking us off our current positive trajectory. Perhaps this is <em>slightly</em> reassuring about whether (say) a biosecurity disaster that killed 100 million people (&lt;1% of world population) could be an existential threat to humanity (as opposed to being merely <em>completely horrifying, worse than the Holocaust</em>).</p>\n<p>(Please remember that there are numerous caveats, clarifications, etc. in the <a href=\"http://lukemuehlhauser.com/industrial-revolution/\">full report</a>; please consider reading them before lambasting my wild speculations or their underlying assumptions. Also, I think these speculations, especially the latter two, are pretty unstable: I don\u2019t think it would take that much study and debate to make me heavily revise them, or abandon them altogether.)</p>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-699H2nonLkv4tZzJe-1\" class=\"footnote-item\"><p>This sentence added Sep. 16, 2017. Log scale chart courtesy of reader Johannes Dahlstr\u00f6m. <a href=\"#fnref-699H2nonLkv4tZzJe-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-699H2nonLkv4tZzJe-2\" class=\"footnote-item\"><p>Changed from \u201c1820-1870\u201d to \u201c1800-1870\u201d on Sep. 16. <a href=\"#fnref-699H2nonLkv4tZzJe-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-699H2nonLkv4tZzJe-3\" class=\"footnote-item\"><p>This is not to say there wasn\u2019t important progress before the industrial revolution, of course. (This footnote added Sep. 16, 2017.) <a href=\"#fnref-699H2nonLkv4tZzJe-3\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-699H2nonLkv4tZzJe-4\" class=\"footnote-item\"><p>In particular, I\u2019m not sure that subjective well-being improved as dramatically as other aspects of well-being seem to have improved following the industrial revolution, and subjective well-being is arguably the most important aspect of human well-being. See the report for more on this. <a href=\"#fnref-699H2nonLkv4tZzJe-4\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"fn-699H2nonLkv4tZzJe-5\" class=\"footnote-item\"><p>This paragraph added Sep. 16, 2017. <a href=\"#fnref-699H2nonLkv4tZzJe-5\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section>\n", "user": {"username": "lukeprog"}}, {"_id": "eXgCREwL5PQWHEtvm", "title": "Biosecurity as an EA Cause Area", "postedAt": "2017-08-13T07:00:00.000Z", "htmlBody": "<p><i>In this 2017 talk, the Open Philanthropy Project's </i><a href=\"https://www.openphilanthropy.org/about/team/claire-zabel\"><i>Claire Zabel</i></a><i> talks about their work to mitigate Global Catastrophic Biological Risks. She also discusses what effective altruists can do to help, as well as differences between biological risks and risks from advanced AI. At the very end you will find an added section on what you can do to&nbsp;help.</i></p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=7OzaTw7rGXE&amp;feature=emb_logo\"><div><iframe src=\"https://www.youtube.com/embed/7OzaTw7rGXE\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p><i>The below transcript is lightly edited for&nbsp;readability.</i></p><p>Today I'm going to talk to you about biosecurity as a cause area and how the Open Philanthropy Project is thinking about it. I think that this is a cause area that EAs have known about for a while, but haven't dug into as deeply as some of the other things that have been talked about at this event - like AI and farm animal welfare and global poverty - but I think it's important. I think it's an area where EAs have a chance to make a huge difference, especially EAs with a slightly different set of skills and interests than those required by some of the other cause areas. I would love to see more EAs engaging with&nbsp;biosecurity.</p><p>When I say biosecurity, I want to make sure we're clear on the problem that I'm talking about. I'm focusing on what we're calling Global Catastrophic Biological Risks at the Open Philanthropy Project. I'm going to talk to you about how we see that risk and where we see that risk - where we think it might be coming from. I'm going to talk to you about how I think EAs can make a difference in it. Then I want to note that I'm not really focusing too much on the specific work that we've done and that others have done. I thought it would be more interesting for people to get a sense of what this area is like and the strategic landscape as we see it before getting into the details of specific organizations and people, so hopefully that's helpful for&nbsp;everyone.</p><p>I also want to note quickly that I think this is an area where a lot less thinking has been done for a much shorter period of time, so to a greater extent everything should be viewed as somewhat preliminary and uncertain. We might be changing our minds in the near&nbsp;future.</p><p>The cause for concern when we think about global catastrophic biological risks is something that could threaten the long term flourishing of human civilization, that could impair our ability to have a really long, really big future full of joy and flourishing for many different sentient beings. That's kind of different from what you might think about biological risks that most people talk about, which are often things like Ebola or Zika. Ebola or Zika are unbelievably tragic for the people afflicted by them, but it doesn't seem like the evidence suggests that they have a realistic chance of causing international civilizational collapse and threatening our long-term&nbsp;future.</p><p>To take this further, we predict that we would need a really extremely big biological catastrophe to threaten the long-term future. We're really thinking about something that kills or severely impairs a greater proportion of the entire human civilization than what happened in either of the world wars or in the 1918 flu pandemic. That kind of implies that we're thinking about fatalities that could range into the hundreds of millions or even the billions. There's a lot of really amazing work that could go into preventing smaller risks, but that's not really what we've been focusing on so far. It's not what I anticipate us focusing on in the future. Overall, we're currently ranking the prevention of global catastrophic biological risks as a high priority, although I think it's somewhat uncertain. I think it's high priority to figure out more and then we might readjust our beliefs about how much we should prioritize&nbsp;it.</p><p>So what are these risks even like? We think the biggest risks are from biological agents that can be easily transmitted that can be released in one area and spread, as opposed to something like Anthrax, which is very terrible in the space that it's released, but it's hard to imagine it really coming to afflict a large proportion human&nbsp;civilization.</p><p>Then within the space of infectious diseases, we're thinking about whether the most risky type of attack would be something that happened naturally that just came out of an animal reservoir, or something that was deliberately done by people with the intention of causing this kind of destruction. Or it might be the middle ground of something that might have been accidentally released from a laboratory where people were doing&nbsp;research.</p><p>Our best guess right now is that deliberate biological attacks are the biggest risk. Accidental risk somewhere in the middle, and natural risk is low. I want to explain why that is because I think a lot of people would disagree with that. Some of the reasons I'm skeptical of natural risks are that first of all, they've never really happened before. Humans have obviously never been caused to go extinct by a natural risk, otherwise we would not be here talking. It doesn't seem like human civilization has come close to the brink of collapse because of a natural risk, especially in the recent&nbsp;past.</p><p>You can argue about some things like the Black Death, which certainly caused very severe effects on civilization in certain areas in the past. But this implies a fairly low base rate. We should think in any given decade, there's a relatively low chance of some disease just emerging that could have such a devastating impact. Similarly, it seems like it rarely happens with nonhuman animals that a pathogen emerges that causes them to go extinct. I know there's one confirmed case in mammals. I don't know of any others. This scarcity of cases also implies that this isn't something that happens very frequently, so in any given decade, we should probably start with a prior that there's a low probability of a catastrophically bad natural pathogen&nbsp;occurring.</p><p>Also, we're in a much better situation than we were in the past and than animals are in some ways, because we have advanced biomedical capabilities. We can use these to create vaccines and therapeutics and address a lot of risks from different pathogens that we could&nbsp;face.</p><p>Then finally, on kind of a different vein, people have argued that there's some selection pressure against a naturally emerging highly virulent pathogen because when pathogens are highly virulent, often their hosts die quickly and they try to rest before they die and they're not out in society spreading it the way you might spread the cold, if you go to work when you have the&nbsp;cold.</p><p>Now, before you become totally convinced about that, I think that there's some good countervailing considerations to consider about humanity, that make it more likely that a natural risk could occur now than in the past. For example, humanity is much more globalized, so it might be the case that in the past there were things that were potentially deadly for human civilization, but humans were so isolated it didn't really spread and it wasn't a huge deal. Now everything could spread pretty much around the&nbsp;globe.</p><p>Also, civilization might be more fragile than it used to be. It's hard to know, but it might be the case that we're very interdependent. We really depend on different parts of the world to produce different goods and perhaps a local collapse could have implications for the rest of the globe that we don't yet&nbsp;understand.</p><p>Then there's another argument one can usually bring up, which is if you're so worried about accidental or engineered deliberate attacks, there's also not very much evidence of those being a big deal. I would agree with this argument. There haven't been very many deliberate biological weapon attacks in recent times. There's not a strong precedent. Nonetheless, our best guess right now is that natural risks are pretty unlikely to derail human&nbsp;civilization.</p><p>When we think in more detail about where catastrophic biological attack risks come from, we can consider the different potential actors. I don't think that we've really come to a really strong view on this. I do want to explain the different potential sources. Some possible sources could be different states. For example, in bio-weapons programs, states could develop pathogens as weapons that have the potential to be destructive. Small groups, such as terrorists or other extremists might be interested in developing these sorts of capabilities. Individuals who have an interest, people working in various sorts of labs: in academia, in the government and on their own. There are DIY biohacker communities that do different sorts of biological experimentation. Those are the different groups that might contribute to catastrophic biological&nbsp;risk.</p><p>There are different kinds of pathogens and I think here - our thinking is even more preliminary - we're especially worried about viral pathogens, because there's proven potential for high transmissibilities and lethality among viruses. They can move really fast. They can spread really fast. We have fewer effective countermeasures against them. We don't have very good broad spectrum antivirals that are efficacious and that means that if we had a novel viral pathogen, it's not the case that we have a huge portfolio of tools that we can expect to be really helpful against&nbsp;it.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/6h57qday8E2yyucoWQIggg/483449fe12b576ae4f967c2f3d4ae644/biosecurity_1.png?w=1800&amp;q=70 1800w\"></figure><p>I've created this small chart that I think can help illustrate how we divide up these risks. On the top there's a dichotomy of whether the pathogen is more natural or more engineered and then on the vertical axis a dichotomy of whether it emerged naturally (or accidentally) or was a deliberate release. The reason I'm flagging these quadrants is because I think there are two different ways to increase the destructiveness of an attack. One is to engineer the pathogen really highly, and the other is to optimize the actual attack type. For example, if you released a pathogen at a major airport, you would expect it to spread more quickly than if you released it in a rural village. Those are two different ways in which you can become more destructive, if you're interested in doing that. Hopefully you're not. My current guess is that there's a lot more optimization space in engineering the actual pathogen than in the release type. There seems to be a bigger range, but we're not super confident about&nbsp;that.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/YMRRNGwj4W4wGMICM2ui0/5895d873b21ae6d017c3989dbf8db838/biosecurity_2.png?w=1800&amp;q=70 1800w\"></figure><p>Here's where the risk we see is coming from. There are advances in gene editing technology, which is a really major source of risk. I think that they've created a lot more room to tinker with biology in general to both lower resources and lower levels of knowledge required and to a greater overall degree, create novel pathogens that are different from what exists in nature, that you can understand how they work. This has amazing potential to do a lot of good but it also has potential to be misused. It's becoming a lot cheaper to synthesize DNA and RNA, to get genetic material for different pathogens. This means that these capabilities are becoming more widely available, just because they're cheaper. Regulating them and verifying buyers is becoming a bigger proportion of the costs, which means companies are more and more incentivized to stop regulating sales and verifying&nbsp;buyers.</p><p>Biotech capabilities are becoming more available around the world. They're spreading to different areas, to new labs. Again, this is mostly a sign of progress. People are having access to technology, places in Asia and around the world are having large groups of very talented scientists and that's really great for the most part, but it means there are more potential sources of risk than there were in the&nbsp;past.</p><p>Then finally, all of those things are happening much faster than governments can possibly hope to keep up and than norms can evolve, so that leads you to the situation where the technology has outpaced our society and our means of dealing with risk, and that increases the level of&nbsp;danger.</p><p>Now I'll contrast and compare biosecurity with AI alignment, because I think AI alignment is something people are much more familiar with. It might be helpful to draw attention to the differences, for getting people up to speed. I think that overall, there's a smaller risk of a far future negative trajectory change from biosecurity. Overall it seems like a smaller risk to&nbsp;me.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/cN1IHChmikssSAcQMQE8s/2b8854804356ded1cbd66d662f725b36/biosecurity_3.png?w=1800&amp;q=70 1800w\"></figure><p>With addressing biosecurity risk, there are fewer potential upsides. With an AI, you can imagine that if it develops really well, it has this amazing potential to increase human capabilities and cause human flourishing. With biosecurity, we're basically hoping that just nothing happens. The best outcome is just nothing. No attacks occur. No one dies. Society progresses. In the case of AI alignment, maybe somebody develops an aligned AI, which would be great. But for biosecurity, we're really about preventing downside risks. More of the risk here comes from people with actively bad intentions as opposed to people with good intentions or people who are just interested in the research, especially if you believe and you agree with me that deliberate attacks are the most likely source of&nbsp;concern.</p><p>In biosecurity more than AI, I think there are many more relevant actors on both sides, as opposed to there being a few labs with a lot of capabilities in AI. It could be the case that we end up with a situation in biosecurity where there are millions of people that are capable of doing something that would be pretty destructive. And also, we can unilaterally develop counter measures against their attacks. There's less connection between the sources of the risk and the sources of the risk reduction. They're more divorced from one another. There's more possible actors on the sides of attack and&nbsp;defense.</p><p>I think that the way that The Open Philanthropy Project is seeing this field right now is somewhat different from how most people are seeing it. Most of the discussion in the field of biosecurity is focused on much smaller risks than the ones that we're worried about. I think discussion of things with greater than one million fatalities was kind of taboo up until very recently. It's been difficult for us to find people that are interested in working on that kind of thing. I think that part of the reason for that, is that it's been really hard to get funding in the space, so people want to make sure their work seems really relevant. And since small attacks and small outbreaks are more common, a good way to make your work more relevant is to focus on&nbsp;those.</p><p>There's ongoing debate in the field about whether natural, deliberate or accidental releases are the biggest risks. I don't think people are synced up on what the answer to that question is. I don't think everyone agrees with us that deliberate is mostly the thing to worry about. Then people are really trying to walk this tightrope of regulating risky research while not regulating productive research, maintaining national competitiveness, and encouraging productive biotech&nbsp;R&amp;D.</p><p>Given all of that, we have some goals in this space. They're kind of early goals. They won't be sufficient on their own. They're mostly examples, but I think they could get us pretty far. The first thing is we just really need to understand the relevant risks in particular. I'm keeping it very high level for now, because there's not a lot of time, and partly because I think that talking about some of these risks publicly is not a productive thing to do, and also because we're pretty uncertain about them. I think it would be really helpful to have some people dig into the individual risks. Think about what one would need to do in order to pull off a really catastrophic bio attack. How far out is that being a possibility? What sorts of technological advancements would need to occur? What sorts of resources would one need to be able to access in order to do that? If we can answer these questions, we can have a sense of how big catastrophic biosecurity risks are and how many actors we need to be worried&nbsp;about.</p><p>Understanding particular risks will help us prioritize things we can do to develop counter measures. We want to support people in organizations that increase the field's ability to respond to global catastrophic biological risks. The reason for that is that right now the field of biosecurity has lacked funding for a long time. A lot of people have left the field. Young people are having a very difficult time going into the field. Hopefully that's changing, but it's still a pretty dire situation, in my view. We want to make sure that the field ends up high quality with lots of researchers that care about the same risks we care about, so people that show signs of maybe moving in that direction, we're very enthusiastic about supporting, in&nbsp;general.</p><p>Then finally, we want to develop medical counter measures for the things that we're worried about. We've started having our science advisors look into this. We have some ideas about what the worst risks are and if we can develop counter measures in advance and stockpile those, I think we would be much better prepared to address risks when they come&nbsp;up.</p><p>Finally, I want to talk to you a little bit about what I think EAs can do to help. I see a lot of potential value in bringing parts of the EA perspective to the field. Right now there aren't a lot of EAs in biosecurity and I think that the EA perspective is kind of special and has something special to offer people. I think some of the really great things about it are, first of all, the familiarity with the idea of astronomical waste and the value of the far future. That seems like it's somewhat hard to understand. It's a bit weird and counterintuitive and philosophical, but a lot of EAs find it compelling. A lot of other people find it wacky or haven't really heard about it. I think having more concern about that pool of value and those people in the future who can't really speak for themselves could do the field of biosecurity a lot of&nbsp;good.</p><p>Another thing that I think is amazing about the EA perspective, is comfort with explicit prioritization, the ability to say, \"We really need to do X, Y, and Z. A, B, and C are lower priority. They'll help us less. They're less tractable. They're more crowded. We should start with these other things.\" I think right now, the field doesn't have a clear view about that. There's not a very well thought out and developed road map to addressing these concerns. I think EAs would be good at helping with&nbsp;that.</p><p>Finally, I think a lot of EAs have a skepticism with established methods and expertise. That's great because I think that's necessary actually in almost every field. Especially in fields that involve a complicated interplay of natural science and social science. I think that there's a lot of room for things to be skewed in certain directions. I haven't seen too much harmful skew, but guarding against it would be really&nbsp;helpful.</p><p>There's some work going on at the Future of Humanity Institute that we're very excited about. It seems like there's a lot of low hanging fruit right now. There are a lot of projects that I think an EA could take on and they'd be pretty likely to make progress. I think biosecurity progress is more of a matter of pulling information together and analyzing it, and less based only in pure&nbsp;insight.</p><p>I think that you should consider going into biosecurity if you are an EA concerned with the far future, who wants to make sure that we all get to enjoy our amazing cosmic endowment, and if you think that you might be a good fit for work in policy or in the biomedical&nbsp;sciences.</p><p>This is an area where I think that a lot of safety might come from people not overhyping certain sorts of possibilities as they emerge, at least until we develop counter measures. It's important to have people that feel comfortable and are okay with the idea of doing a lot of work and then not sharing it very widely and actually not making it totally open, because that could actually be counterproductive and increase risk. That's what I hope that people will be willing to do. I hope that we find some EAs who want to move into this field. If you feel like you're interested in moving into this field, I would encourage you to reach out to me or grab me sometime at this conference and talk about both what you'd like to do and what might be stopping you from doing&nbsp;it.</p><p>In the future we might write more about how we think people can get into this field and be able to do helpful research, but we haven't really done that yet, so in the meantime, I really hope that people reach out. Thank you so much and I'll take your&nbsp;questions.</p><p><strong>Question:</strong> Okay, so we've got a number of questions that have come in and I'm just gonna try to rifle through them and give you a chance to answer as many as we can. You emphasized the risk of viral pathogens. What about the, I think, more well known if not well understood problem of antibiotic resistance? Is that something that you're thinking about and how big of a concern is that for&nbsp;you?</p><p><strong>Claire Zabel:</strong> Yeah. I think that's a good question. The Open Philanthropy Project has <a href=\"https://www.openphilanthropy.org/research/cause-reports/antibiotic-resistance\">a report on antibiotic resistance</a> that I encourage you to read if you're curious about this topic. I think it's a really big concern for dealing with conventional bacterial pathogens. Our best guess is that it's not such a special concern for thinking about global catastrophic biological risks, first of all, because there's already immense selection pressure on bacteria to evolve some resistance to antibiotics, and while this mostly has really negative implications, it has one positive implication, which is that, if there's an easy way to do it, it's likely that it'll happen naturally first and not through a surprise attack by a deliberate bad&nbsp;actor.</p><p>Then another reason that we're worried about viruses to a greater extent than bacteria is because of their higher transmissibility and the greater difficulty we have disinfecting things from viral pathogens. So, I don't think that antibiotic resistance will be a big priority from the far-future biosecurity perspective. I think it's possible that we're completely wrong about this. I'm very open to that possibility, and what I'm saying is pretty low confidence right&nbsp;now.</p><p><strong>Question:</strong> Great. Next question. To what extent do small and large scale bio-risks look the same and to what extent do the counter measures for those small and large scale risks look the same, such that you can collaborate with people who have been more in the traditional focus area of the smaller scale&nbsp;risks?</p><p><strong>Claire Zabel:</strong> That's an interesting question. I think it's a complicated one and a simple answer won't answer it very well. When I think about the large scale risks, they look pretty different for the most part from conventional risks, mostly because they're highly engineered. They're optimized for destructiveness. They're not natural. They're not something we're very familiar with, so that makes them unlikely to be things that we have prepared responses to. They're likely to be singularly able to overwhelm healthcare systems, even in developed countries, which is not something that we have much experience&nbsp;with.</p><p>But the second part of the question about the degree to which efforts to address small scale risks help with big scale risks and vice versa, I think that that's somewhat of an open question for us and as we move towards prioritizing in the space, we'll have a better view. There's some actions that we can take. For example, advocacy to get the government to take biosecurity more seriously might help equally with both. On the other hand, I think developing specific counter measures, if we move forward with that, will be more likely to only help with large scale risks and be less useful with small scale risks, although there are counter examples that I'm thinking of right now, so that's definitely not an accurate blanket&nbsp;statement.</p><p><strong>Question:</strong> When you think about these sort of engineered attacks that could create the largest scale risk, it seems like one thing that has sort of been on the side of good, at least for now, is that it does take quite a bit of capital to spin up a lab and do this kind of bioengineering. But, as you mentioned, stuff is becoming cheaper. It's becoming more widely available. How do you see that curve evolving over time? Right now, how much capital do you think it takes to put a lab in place and start to do this kind of bad work if you wanted to and how does that look five, ten, twenty years&nbsp;out?</p><p><strong>Claire Zabel:</strong> I don't think I want to say how much it takes right now, or exactly what I think it will take in the future. I think the costs are falling pretty quickly. It depends on what ends up being necessary, so for example, the cost of DNA synthesis is falling really rapidly. It might be the case that that part is extremely cheap, but actually experimenting with a certain pathogen that you think might have destructive capability - for example, testing it on animals - might remain very expensive, and it doesn't seem like the costs of that part of a potential destructive attack are falling nearly as&nbsp;quickly.</p><p>Overall, I think costs will continue to fall but I would guess that the falling plateaus sometime in the next few&nbsp;decades.</p><p><strong>Question:</strong> Interesting. Does biological enhancement fall within your project at all? Have you spent time considering, for example, enhancing humans or working on gene editing on humans and how that might be either beneficial or potentially destabilizing in its own&nbsp;way?</p><p><strong>Claire Zabel:</strong> That's not something that we've really considered a part of our biosecurity&nbsp;program.</p><p><strong>Question:</strong> Fair enough. How interested is Open Philanthropy Project in funding junior researchers in biosecurity or biodefense? And relatedly, which would you say is more valuable right now? Are you looking more for people who have kind of a high level strategic capability or those who are more in the weeds, as it were, of wet synthetic&nbsp;biology?</p><p><strong>Claire Zabel:</strong> Yeah. I think that right now we'd be excited about EAs that are interested in either, potentially, depending on their goals in this field, the extent of the value alignment, and their dedication and particular talents. I think both are useful. I expect that the kind of specialization, for example, either in policy or in biomedical science will possibly be more helpful in the long term. I'm hoping that we'll gain a lot of ground on the strategic high level aspects of it in the next few years, but right now I think both are sorely&nbsp;needed.</p><p><strong>Question:</strong> Next question. For someone whose education and skills have been focused on machine learning, how readily can such a person contribute to the type of work that you're doing and what would that look like if they wanted to get&nbsp;involved?</p><p><strong>Claire Zabel:</strong> I don't know. I've never seen anyone try. I think that it would be possible because I think that there's a lot of possibility of someone who has no special background in this area, in general, becoming really productive and helpful within a relatively short time scale and I don't see machine learning background as putting anyone at a particular disadvantage. Probably it would put you at somewhat of an advantage, although I'm not sure how. I think that right now, the best way to go would probably be just to get a Masters or PhD in a related field and then try to move into one of the relevant organizations, or try to directly work at one of the relevant organizations like our biggest grantee in biosecurity, the Center for Health Security. And for that, I think that probably having a background in machine learning would be neither a strong drawback nor a huge&nbsp;benefit.</p><p><strong>Question:</strong> That's about all the time that we have for now,&nbsp;unfortunately.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "Mw9ZxmZqiaXM2rb49", "title": "What does (and doesn't) AI mean for effective altruism?", "postedAt": "2017-08-12T07:00:00.000Z", "htmlBody": "<p><i>In this 2017 talk, The Future of Humanity Institute's </i><a href=\"https://www.fhi.ox.ac.uk/team/owen-cotton-barratt/\"><i>Owen Cotton-Barratt</i></a><i> discusses what strategy effective altruists ought to adopt with regards to the development of advanced artificial intelligence. He argues that we ought to adopt a portfolio approach - i.e., that we ought to invest resources in strategies relevant to several different AI scenarios. At the very end you will find an added section on what you can do to&nbsp;help.</i></p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=gATWIWiIy_8&amp;feature=emb_logo\"><div><iframe src=\"https://www.youtube.com/embed/gATWIWiIy_8\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p><i>The below transcript is lightly edited for&nbsp;readability.</i></p><p>Some of you may have noticed that a bunch of people in this community seem to think that AI is a big deal. I was going to talk about that a little bit. I think that there are a few different ideas which feed into what we should be paying a lot of attention to. One is that from a moral perspective, the biggest impacts of our actions - and perhaps even overwhelmingly so - are the effects of our actions today on what happens in the long term future. Then there's some pretty empirical ideas. One is that artificial intelligence might be the most radically transformative technology that has ever been developed. Then actually artificial intelligence is something that we may be able to influence the development of. Influencing that could be a major lever over the future. If we think that our actions over the long term future are important, this could be one of the important mechanisms. Then as well, that artificial intelligence and the type of radically transformative artificial intelligence could plausibly be developed in the next few&nbsp;decades.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/5U1fK3Sp7aMYMmOCk8Iu60/7a92aa2159d335ef7107779049e8b43e/AI_in_EA_1.png?w=1800&amp;q=70 1800w\"></figure><p>I don't know what you think of all of these claims. I tend to think that they're actually pretty plausible. For the rest of this talk, I'm going to be treating these as assumptions, and I want to explore the question: if we take these seriously, where does that get us? If you already roughly agree with these, then you can just have a like sit back and see how much you agree with the analysis, and maybe that's relevant for you. If you don't agree with one of those claims, then you can treat this as an exercise in understanding how other bits of the community might think. Maybe some of the ideas will actually be usefully transferrable. Either way, if there are some of these that you haven't thought much about before, I encourage you to go and think about them - take some time afterwards. Because it seems to me at least that these are, each of these ideas is something which potentially has large implications for how we should be engaging in the world in this project of trying to help it. It seems like it's therefore the kind of thing which is worth having an opinion&nbsp;on.</p><p>Okay, so I'm going to be exploring where this gets us. I think a cartoon view people sometimes hold is if you believe in these ideas, then you think everybody should quit what they're working on, and drop everything, and go and work on the problem of AI safety. I think this is wrong. I think there are some related ideas in that vicinity where there's some truth. But it's a much more nuanced picture. I think for most people, it is not correct to just quit what they're doing, to work on something safety related instead. But I think it's worth understanding in what kind of circumstances it might be correct to do that, and also how the different pieces of the AI safety puzzle fit&nbsp;together.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/2LDwAU31e8Igowm0ySkOEM/98131637c32d5a3ef98838581e1a6252/AI_in_EA_2.png?w=1800&amp;q=70 1800w\"></figure><p>I think that thinking about timelines is important for AI. It is very hard to have any high level of confidence in when AI might have different capabilities. Predicting technology is hard, so it's appropriate to have uncertainty. In fact, here's a&nbsp;graph.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/3ixtKv61UQ8yC282iqcaQQ/93109d1262270ee3a92b009a834d4354/AI_in_EA_3.png?w=1800&amp;q=70 1800w\"></figure><p>You can see the bunch of faint lines showing individual estimates of people working in machine learning research of when they expect high level AI to be developed. Then this bold red thing is the median of those. That's quite a lot of uncertainty. If you take almost any individual's view, and certainly this aggregate view, that represents very significant uncertainty over when transformative AI might occur. So we should be thinking about&nbsp;that.</p><p>Really our uncertainty should follow some kind of smooth distribution. For this talk, I'm gonna talk about four different scenarios. I think that the advantage of anchoring the possibilities as particular scenarios and treating them as discrete rather than continuous is that it becomes easier to communicate about, and it becomes easier to visualize, and think, \"Okay, well what would you actually do if the timeline has this type of&nbsp;length?\"</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/339pjtrbc4C4MksWaCmQoC/eb3faada59c67b8b9c1a8014a23307a7/AI_in_EA_4.png?w=1800&amp;q=70 1800w\"></figure><p>The first scenario represents imminent AI, maybe something on the scale of 0 to 10 years away. In this case, it's more likely that we actually know or can make educated guesses already about who the important actors will be around the development of&nbsp;AI.</p><p>I want to explore a little bit about what strategies we might pursue based on each of these different timelines. If you assume this first one, then there's no time for long processes. If your idea was, \"Well, I'll do a degree in CS, and then I'll go and get a PhD in machine learning, and then I'll go into research,\" you're too late. On the other hand, if you are already in a position where you might be able to do something in the short term, then it could be worth paying attention to. But I feel for a lot of people, even if you think there is some small chance of this first scenario happening (which in general you want to pay attention to) it may be that there isn't a meaningful way to&nbsp;engage.</p><p>The next possible scenario would be maybe between 10 and 25 years out. This is a timescale in which people can naturally build careers. They can go and they can learn things. They can develop networks. They can build institutions. They can also build academic fields. You can ask questions, get people motivated, and get them interested in the framing of the question that you think is important. You can also have time for some synthesis and development of relevant ideas. I think that building networks where we persuade other people who maybe aren't yet in a direct position of influence, but might be later, can be a good&nbsp;idea.</p><p>If we look a bit further to another possible scenario, maybe between 25 to 60 years out, that's a timescale at which people who are in the important fields today may be retiring. Paradigms in academic fields might have shifted multiple times. It becomes hard to take a zoomed in view of what it is that we need, but this means that it's more important and build things right rather than quickly. We want to build solid foundations for whatever the important fields are. When I say the important fields here, I'm thinking significantly about technical fields of how we build systems which do what we actually want them to do. I'm also thinking about the kind of governance, policy, and processes in our society around AI. Who should develop AI? How should that be structured? Who is going to end up with control over the things which are&nbsp;produced?</p><p>These scenrios are all cartoons. I'm presenting a couple of stylized facts about each kind of timeline. There will be a bit of overlap of these strategies, but just to give an idea of how actually the ideal strategy changes. Okay. The very distant maybe more than 60 years out, anything, maybe it's even hundreds of years, at this level predictability gets extremely low. If it takes us this long to develop radically transformative AI, it is quite likely that something else radically transformative will have happened to our society in the meanwhile. We're less likely to predict what the relevant problems will be. Instead, it makes sense to think of a strategy of building broad institutions, which are going to equip the people of that time to better deal with the challenges that they're facing&nbsp;then.</p><p>I think actually it's plausible that the effective altruism community, and the set of ideas around that community, might be one broad, useful institution for people of the far future. If we can empower people with tools to work out what is actually correct, and the motivation and support to act on their results, then I'd be like, \"Yep, I think we can trust those future people to do&nbsp;that.\"</p><p>The very long term is the timescale at which other very transformative things occurring in our society are more likely to happen. This can happen on the shorter timescales as well. But if you think on a very long timescale, there is much more reason to put more resources toward other big potential transitions, rather than just AI. I think that AI could be a big deal, but it's definitely not the only thing that could be a big&nbsp;deal.</p><p>Okay. I've just like talked us through different timelines. I think that most reasonable people I know put at least some nontrivial probability on each of these possible scenarios. I've also just outlined how we probably want to do different things for the different scenarios. Given all of this, what should we actually be doing? One approach is to say, \"Well, let's not take these on the timelines. Let's just do things that we think are kind of generically good for all of the different timelines.\" I think that that's a bad strategy because I think it may miss the best opportunities. There may be some things which you only notice are good if you're thinking of something more concrete rather than just an abstract, \"Oh, there's gonna be AI at some point in the future.\" Perhaps for the shorter timelines, that might involve going and talking to people who might be in a position to have any effect in the short term, and working out, \"Can I help you with&nbsp;anything?\"</p><p>Okay. The next kind of obvious thing to consider is, well, let's work out which of these scenarios is the most likely. But if you do this, I think you're missing something very important, which is that we might have different degrees of leverage over these different scenarios. The community might have different leverage available for each scenario. It can also vary by individual. For the short timelines, probably leverage is much more heterogeneous between different people. Some people might be in a position to have influence, in that case it might be that they have the highest leverage there. By leverage, I mean roughly, conditional on that scenario actually pertaining, how much does you putting in a year of work, trying your best, have an effect on the outcome? Something like&nbsp;that.</p><p>Okay. Maybe we should just be going for the highest likelihood multiplied by leverage. This of course is like the place where we have the most expected impact. I think there's something to that. I think that if everybody properly does that analysis for themselves and updates as people go and take more actions in the world, then in theory that should get you to the right things. But the leverage of different opportunities varies both as people take more opportunities and also even just for an individual. I've known people who think that they've had different opportunities they can take to help short timelines and then a bunch of other opportunities to help with long timelines. This is a reason not to naively go for highest likelihood multiplied by&nbsp;leverage.</p><p>Okay. What else? Well, can we think about what portfolio of things we could do? I was really happy about the theme of this event because thinking about the portfolio and acting under uncertainty is something I've been researching for the past two or three years. On this approach, I think we want to collectively discuss the probabilities of different scenarios, the amount of leverage we might have for each, and the diminishing returns that we have on work aimed at each. Then also we should discuss about what that ideal portfolio should look like. I say collectively because this is all information where when we work things out for ourselves, we can help inform others about it as well, and we can probably do better using collective epistemology than we can&nbsp;individually.</p><p>Then we can individually consider, \"Okay, how do I think in fact the community is deviating from the ideal portfolio? What can I do to correct that?\" Also, \"What is my comparative advantage here?\" Okay. I want to say a couple of words about comparative advantage. I think you know the basic idea. Here's the cartoon I think of it in terms&nbsp;of:</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/3DpO2dWeDu8mwmACaYyyoc/21dd21f38df4c4e41eb444fb4b557d6a/AI_in_EA_5.png?w=1800&amp;q=70 1800w\"></figure><p>You've got Harry, Hermione, and Ron, they have three tasks to do, and they've gotta do one task each. Hermione is best at everything, but you can't just get Hermione to do all the things. You have to allocate them one to one. So it's a question of how do you line the people up to the things so that you have everyone doing something that they're pretty good at it, and overall you get all of the important things done? I think that this is something that we can think of at the level of individuals choosing, \"What am I going to work on? Well, I've got this kind of skillset.\" It's something that we can think of at the level of groups as well. We can ask, \"What is my little local community going to work on?\" or \"What is this organization going to do, and how do we split up responsibility between different&nbsp;organizations?\"</p><p>Comparative advantage is also a concept you can think of applied over time. This is a little bit different because people's actions in the past are fixed, so we can't affect those. But you can think there's things that might want to be done and we can do some of these. People in the past did some of them. People in the future might do some of them and there's a coordination question of what we have a comparative advantage at relative to people in the future. This is why when I was looking at longer scenarios, the next generation in the distant cases, I was often thinking it was better to let people in the future solve the concrete problems. They're gonna be able to see more clearly what is actually to be solved. Meanwhile, we have a comparative advantage at building the processes, the communities, the institutions which compound over time, and where getting in early is really&nbsp;helpful.</p><p>If you're taking something like this portfolio approach, I think that most projects should normally have at least a main scenario in mind. This forces you to be a little bit more concrete and to check that the things you're thinking of doing actually line up well with the things which are needed in some possible world. I also think you want to be a bit careful about checking that you're not doing anything which would be bad for other scenarios. There's always an opportunity cost. If you're doing something where you're thinking, \"I want to help with this short timeline scenario,\" then you're not doing something else you could've done to help with the next generation in a longer timeline&nbsp;scenario.</p><p>You could also have situations where maybe I would think that if AI is imminent, the right thing to do is to run around and say, \"Everybody panic. AI is coming in five years. It's definitely coming in five years.\" If it definitely were coming in five years, maybe that would be the right thing to do. I actually don't think it is. Even if it were, I think that would be a terrible idea because if you did that, then people, if it <i>didn't</i> occur in five years and we were actually in a world where radically transformative AI was coming in 25 years, then in 15 years, a lot of people are gonna go, \"We've heard that before,\" and not want to pay attention to it. This is a reason to have an idea of paying some attention to the whole idea of the portfolio that as a community we want to be paying attention to even if individually, most projects should have a main scenario in mind. Maybe as an individual, your whole body of work has a main scenario in mind. It's still worth having an awareness of where other people are coming from, and what they're working on, and what we're doing collectively&nbsp;then.</p><p>I've mostly talked about timelines here. I think that there are some other significant uncertainties about AI. For instance, how much is it that we should be focusing on trying to reduce the chances of catastrophic accidents from powerful AI? Or how much of the risk is coming from people abusing powerful technologies? We hypothesized it was gonna be a radically transformative technology with influence over the future. How much of that influence actually comes through things which are fairly tightly linked to the AI development process? Or how much influence appears after AI is developed? If most of the influence comes from what people want in the world after an AI is developed, it might makes sense to try to affect people's wants at that&nbsp;point.</p><p>In both of these cases, I think we might do something similar to portfolio thinking. We might say, \"Well, we've put some weight on each of these possibilities,\" and then we think about our leverage again. Maybe for some of them, we shouldn't be split. Some of them we might do. We can't do this with all of the uncertainties. There are a lot of uncertainties about&nbsp;AI.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/2Uw77StjJSWAwMKWKg6Ask/34b5fe2b4b0e0b49c870aa2e928c50d1/AI_in_EA_6.png?w=1800&amp;q=70 1800w\"></figure><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/VjdTkgIh2ucYEGiUUQGiI/3389013baad5fdadd86a9c84b9da4c46/AI_in_EA_7.png?w=1800&amp;q=70 1800w\"></figure><p>Here's a slide from another talk. It just lists a lot of questions. A lot of them about how AI might develop. We can all have nuanced views about each of these questions. That's fine. We need to do some picking and choosing here. But I do think that we should strive for nuance. I think the reason is that there's a lot of uncertainty, and we could potentially have extremely nuanced views about a lot of different things. The world is complicated, and we have a moderately limited understanding of it. One of the things which may make us better equipped for the future is trying to reduce our limits on our&nbsp;understanding.</p><p>What can individuals do? I think consider personal comparative advantage. You can ask yourself, \"Could I seriously be a professional researcher in this?\" Check with others as well. I think people vary in their levels of self-confidence, so I actually think that others' opinions often can be more grounding than our own opinion for this. It's a pretty specialized skillset that I think is useful for doing technical safety research. Most people in the community are not gonna end up with that skillset and that's fine. They should not be quitting their jobs, and going to try and work on safety research. They could be saying, \"Well, I want to give money to support this,\" or they could be aiming at other parts of this portfolio. They could say, \"Well, I want to help develop our institutions to build something where we're gonna be better placed to deal with some of the longer timeline&nbsp;scenarios.\"</p><p>You could also diversify around those original assumptions that I made. I think that each of them is pretty likely to be true. But I don't think we should assume that they are all definitely true. We can check whether in fact there are worlds where they're not true that we want to be putting some significant weight onto. I think also just helping promote good community epistemics is something that we can all play a part in. By this I mean pay attention to why we believe things and communicate our real reasons to people. Sometimes you believe a thing because of a reason like: \"Well, I read this in a blog post by Carl Shulman, and he's really smart.\" He might provide some reasons in that blog post, and I might be able to pallet the reasons a little bit. But if the reason I really believe it is I read that, that's useful to communicate to other people because then they know where the truth is grounded in the statements I'm making, and it may help them to be able to better see things for themselves, and work things out. I also think we do want to often pay attention to trying to see the underlying truth for ourself. Good community epistemics is one of these institutions which I think are helpful for the longer timelines, but I think they're also helpful for our community over shorter periods. If we want to have a portfolio, we are going to have to coordinate and exchange views on what the important truths&nbsp;are.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/Arf2UlJPoWSoCq28KE8ui/3f9f4b6f049af1c5664d6a0eedbee5cb/AI_in_EA_8.png?w=1800&amp;q=70 1800w\"></figure><p>What does AI mean for effective altruism? My view is that it isn't the one thing that everyone has to pay attention to, but it is very plausibly a big part of this uncertain world stretching out in front of us. I think that we collectively should be paying attention to that and working out what we can do, so we can help increase the likelihood of good outcomes for the long term&nbsp;future.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "iDYt2e4skogJEn946", "title": "Potential Risks from Advanced AI", "postedAt": "2017-08-13T07:00:00.000Z", "htmlBody": "<p><i>In this 2017 talk, </i><a href=\"https://www.openphilanthropy.org/about/team/daniel-dewey\"><i>Daniel Dewey</i></a><i> presents Open Philanthropy's work and thinking on advanced artifical intelligence. He also gives an overview over the field, distinguishing between strategic risks - related to how influential actors will react to the rise of advanced AI systems - and misalignment risks - related to whether AI systems will reliably do what we want them to&nbsp;do.</i></p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=Nfrh4K3d_Z0\"><div><iframe src=\"https://www.youtube.com/embed/Nfrh4K3d_Z0\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure><p><i>The transcript below is lightly edited for&nbsp;readability.</i></p><p>I'm the program officer at the Open Philanthropy Project in charge of potential risks from advanced AI. This is an area we're spending a lot of our senior staff time on recently, so I wanted to give an update on the work that we're doing in this area, how we think about it, and what our plans are going&nbsp;forward.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/1DHrHc3ixWg4Ky6os64QOu/8568872113559da99a502558e7e88937/advanced_ai_1.png?w=1800&amp;q=70 1800w\"></figure><p>So, there are four basic concepts that I want to really make sure to drive home during the course of this talk, and if you watch out for these, I think they'll help you understand how we're thinking about this&nbsp;area.</p><p>I think there are a lot of different ways to frame potential risks from advanced AI that can inform different kinds of approaches and interventions and activities. And it can be a bit hard to understand why we're doing the things we're doing without understanding the way we're thinking about them. Also, I should mention, I didn't really frame this talk up as the perfect introduction to this area if you're not already somewhat&nbsp;familiar.</p><p>These are the four basic&nbsp;concepts:</p><ol><li>Transformative AI, which is how we think broadly about the impacts that AI could have in the future that we care most about affecting our&nbsp;activities;</li><li>Strategic risks, having to do with how the most influential actors in the world will react to the prospect of transformative&nbsp;AI;</li><li>Misalignment risks, which have to do with being able to build AI systems that reliably do what their operators want them to&nbsp;do;</li><li>Our strategy in this area. The way we're currently planning on making a difference, which is field&nbsp;building.</li></ol><h3><strong>Transformative AI</strong></h3><p>So, to start off, there's this idea of transformative AI. Basically looking ahead at the kinds of impacts we expect AI to have. We think there are a lot things that could happen and there's a lot of uncertainty about precisely what is going to happen. But something that seems reasonable is to expect AI to have an impact that is comparable to or larger than that of the Industrial or Agricultural Revolutions. And that's intended to capture a lot of possible sorts of scenarios that could&nbsp;happen.</p><p>So, we might see AI progress lead to automated science and technology development, which could lead to a really rapid increase in technological progress. We might see artificial general intelligence (sometimes abbreviated AGI), meaning AI systems that can do anything that a human can do, roughly. And that would really change the dynamics of the economy and how the economy functions. We might see systems that can do anything that a human or a group of humans can do. So AI systems could operate organizations autonomously. Maybe companies, non-profits, parts of&nbsp;government.</p><p>And then sort of looming over all of this is the idea that we shouldn't really expect AI to stop at the point of human-level competence, but we should expect the development of super-intelligent AI systems. It's not clear exactly what the distribution of capabilities of these systems would be and there are a lot of different&nbsp;possibilities.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/rqlTWjqDduQswsgM2U4GW/5e785d7a118c04fd765689f90be04224/advanced_ai_2.png?w=1800&amp;q=70 1800w\"></figure><p>The reason I've chose this picture in the slides is because it shows the change in the way human influence was wielded on the world during the Industrial Revolution. You can see this traditional set of biofuel usage down at the bottom and then over the course of the Industrial Revolution, that became a very small percentage of the overall influence that humanity wielded. Most of what we were doing in the world came to depend on these new energy&nbsp;sources.</p><p>The idea of transformative impact comes from AI becoming a really large percentage of how humanity influences the world. That most of the influence we have could be via AI systems that are hopefully acting on our&nbsp;behalf.</p><p>Based on the conversations we've had with a lot of AI researchers, it's pretty reasonable to think that this could happen sometime in the next 20 years. I'm saying greater than 10% chance by 2036 because we said 20 years last year and so we don't want to always be saying 20 years later as years&nbsp;continue.</p><p>So there\u2019s this really big change in the world, there's a lot of variation in what could happen, and it's hard to predict exactly what is going to be most critical and what kinds of things we might want to make a difference&nbsp;on.</p><p>So here is our general strategy in this area. We can imagine two different worlds. One of them is a world where transformative AI comes somewhat by surprise, maybe it comes relatively early. And there aren't a lot of people who have been spending much of their career thinking full time about these problems, really caring about longterm outcomes for humanity. And then there's an alternate world where those professional people have existed for a while. They're working in fields with each other. They're critiquing each other's&nbsp;work.</p><p>And we think that the prospect of good outcomes is a lot more likely in cases where these fields have existed for a while, where they're really vibrant. They have some of the best people in policies, some of the best people in machine learning and AI research in them. And where those people have been thinking really specifically about how transformative AI could affect the long run trajectory of human&nbsp;civilization.</p><p>So, our basic plan is to affect field building. To try to move these fields ahead, in terms of quality and in terms of size. And a really useful thing about this is that if you wanna affect the longterm trajectory of civilization, you don't really get to run several experiments to see which interventions are going to work well. So it's really hard to get feedback on whether what you're doing is&nbsp;helping.</p><p>So, what we'd like to do is start really keeping track of how these fields grow over time so that we can tell which kinds of interventions are making a difference. And it's not a sure thing that field growth is the correct strategy to pursue but it at least gives us something to measure and track to see if what we're doing is making a&nbsp;difference.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/4St7fd1BTGOCoyqSCikOm6/38831f548bd67238e1e82bd7abc946d9/advanced_ai_3.png?w=1800&amp;q=70 1800w\"></figure><h3><strong>Strategic Risks</strong></h3><p>I'm starting with strategic risks because I think they have historically been less emphasized in the EA community. By strategic risks, I mean risks that could be caused by the way major, influential actors in the world react to the prospect of artificial general intelligence, or super-intelligence, or other kinds of transformative AI. And the way that they choose to use these technologies to affect the world. So sort of the policies and strategies they&nbsp;adopt.</p><p>For example, if you expect this big curve of human influence in the world to be mostly about artificial intelligence in the future, then that's a big opportunity for different actors to have more influence in the future than they do today or an opportunity for that influence to be rebalanced. Maybe between different countries, between different industries. It feels like there's a strong chance that as influential actors start noticing that this might happen, that there could be preemptive conflict. There could be arms races or development races between governments or between&nbsp;companies.</p><p>If a government or company gains a really strong advantage in artificial intelligence, they might use it in a way that isn't in the best interest of the most people. So we could see a shift in the way resources and rights are distributed in the future. I classify that as a misuse of artificial intelligence. We want to make sure that transformative AI is used in a way that benefits the most people the&nbsp;most.</p><p>And then a final thing to think about is the possibility of accidental risks, risks of building AI systems that malfunction and do things that don't really benefit anyone, that weren't intentional. Then racing to develop artificial intelligence could be a big increase in that risk, because if you spend time and money and resources on making systems safer, you're spending less on&nbsp;racing.</p><p>What we'd like to do is build up a field of people who are trying to answer the key question of what should influential actors do in different scenarios depending on how AI development plays out. Its important to consider different scenarios because there\u2019s a lot of variation in how the future could&nbsp;go.</p><p>And there are a lot of existing relevant areas of expertise, knowledge and skill that seem like they're really relevant to this problem. So, geopolitics, global governance. It seems important for AI strategists to have pretty good working knowledge of AI and machine learning techniques and to be able to understand the forecasts that AI developers are making. And there's a lot of history in technology policy and the history of transformative technologies such that I hope that there are lessons that we could take from those. And of course, there's existing AI risk thought. So, Nick Bostrom's Superintelligence, things that have been done by other groups in the effective altruist&nbsp;community.</p><p>And so, our activities in this area of AI strategic risk right now, how are they going? I think that the frank summary is that we're not really sure how to build this field. Open Philanthropy Project isn't really sure. It's not really clear where we're going to find people who have the relevant skills. There's not, as far as we can tell, a natural academic field or home that already has the people who know all of these things and look at the world in this way. And so, our activities right now are pretty scattered and experimental. We're funding the Future of Humanity Institute and I think that makes sense to do, but we're also interacting a lot with government groups, think tanks, companies, people who work in technology policy, and making a few experimental grants to people in academia and elsewhere just to see who is going to be productive at doing this&nbsp;work.</p><p>I think it's really unclear and something I'd love to talk to people about more. Like how are we going to build this AI strategy field so that we can have professional AI strategists who can do the important work when it's most&nbsp;timely?</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/7sC8pFLLwsoO6Se00O6IW4/197b9272844c237990435ec1adc9118d/advanced_ai_4.png?w=1800&amp;q=70 1800w\"></figure><h3><strong>Misalignment Risks</strong></h3><p>So, the other category of risk that I wanna talk about is misalignment risks. I've used a picture of a panda. This is an adversarial example. It's a crafted image that's designed to make an AI system make an incorrect decision. And it's been sort of a recent, really hot topic in machine learning because it shows the fragility of some kinds of machine learning models that are really popular right&nbsp;now.</p><p>This kind of fragility is not a full picture of the problems of AI misalignment. It\u2019s not a full picture of when AI systems don't reliably do the things that their operators want them to do, but I think it's a good simple, straightforward example. The intent of training a neural network on these images was to get the neural network to make the same classifications that humans would. And it turns out to not be very hard to come up with a situation where the neural network will just do something completely different from what any human would&nbsp;say.</p><p>So, broadly speaking, misalignment risks refer to situations where we can make really influential AI systems and most of our influence over the world is flowing through these AI systems, but we can't make these systems reliably pursue the objectives that their operators intend. So, if we see this, a similar shaped graph as ended the Industrial Revolution where almost everything that humans are doing in the world is going through AI systems, and most of the way the world goes in the future depends on those decisions sort of lining up well with what humans want, then it's a really bad situation if we're not really sure if AI systems are going to do the things we want them to do, if they misinterpret what we want them to do, if they're gonna act unreliably when they're in situations we haven't anticipated&nbsp;before.</p><p>So, we've been talking a lot to groups like the Machine Intelligence Research Institute, to the Future of Humanity Institute, and also to technical advisors of ours who are at industrial research labs like OpenAI and Deep Mind and then also to people in academia, machine learning&nbsp;researchers.</p><p>And there are a couple of priority areas of research that we think are really important if you want to advance the technical capability of building AI systems that reliably do the things that their operators want them to do: reward learning and&nbsp;reliability.</p><p>So reward learning is this idea that it would be quite bad if we could build AI systems that can pursue easily specifiable goals like things you can measure in the world that are like how much money is in this bank account or how rewards come in through this particular channel that's flowing back to the AI. Most of the things humans care about in the world aren't easily measured in that way. So, there's a question of whether we can get AI systems to learn a task by interacting with humans in a way that makes them sort of cooperatively refine their understanding of what our goals are and act conservatively in cases where they have a lot of uncertainty and where the impact on the world could be very great if they've made the wrong evaluation of what their operator's objectives&nbsp;are.</p><p>And then on the reliability side, there's this question of how we train AI systems in really limited subsets of the situations that they'll eventually be functioning in. So if we want AI systems to make important decisions in the world, especially if the world is changing rapidly and dramatically, we need to be really sure that AI systems are not going to function dramatically differently in those situations than they did in&nbsp;training.</p><p>At Open Philanthropy Project, we've encountered a bunch of different models and ideas about how hard AI alignment will be. There's some people we've talked to who think that AI alignment is like really, really closely related to all of the things that we'll need to do in order to make AI systems effective in the world in the first place. Those problems are just gonna be solved along the way. On this view, maybe it doesn't hurt to get started ahead of time, but it's not an urgent issue. And we've talked to other people who think that there are a ton of open, unsolved problems that we have no idea how to make traction on. And that we need to get started yesterday on solving these problems. And there are a lot of people in the middle. Probably the majority of people are somewhere in between, in terms of AI and machine learning&nbsp;researchers.</p><p>So, we're highly uncertain about how hard alignment will be and we think that it makes a lot of sense to get started on this academic field building in this area. If the worst case scenario is that we build this field and the problems turn out to be easier than we expected, that seems pretty&nbsp;good.</p><p>I think we're a lot clearer how misalignment field building will go than we are about how strategic risk field building will go. In reward learning and reliability, and then in AI alignment more broadly, I think that the academic field of AI and machine learning research contains the people who have the kinds of skills and capabilities that we need for AI alignment research already. And this is an area where philanthropic funding can just directly have an impact. There's a bit of a funding puzzle to do with having all these different chickens and eggs that you need in order to get a good research field up and running. And that includes having professors who can host students, having students who are interested in working on these problems and having workshops and venues that can coordinate the research community and kind of weave people together so that they can communicate about what questions are most&nbsp;important.</p><p>I think it's obvious that this kind of field building work could pay off in the longer term. If you imagine this AI alignment community building up over many decades, it's obvious. But actually, I think that even if we want to develop experts who will be ready to make essential contributions on short timelines, this is among the best ways to do that, because we're finding PhD students who have a lot of the necessary skills already and getting them to start thinking about and working on these problems as soon as we&nbsp;can.</p><p>So, this is a scenario where we've done a pretty significant amount of grant making so far and we have some more in the works. There have been a couple big grants to senior academics in artificial intelligence and machine learning. The biggest ones being to Stuart Russell and his co-investigators, several other professors, at the Center for Human Compatible AI, which is based in Berkeley and also has branches at couple of their universities. There's another big grant that went to Joshua Bengio and bunch of his co-investigators at The Montreal Institute for Learning Algorithms. And that's a fairly recent grant. There are more students coming into that institute in the fall who we're hoping to get involved with this&nbsp;research.</p><p>With other professors, we're making some planning grants so that we can spend time interacting with those professors and talking with them a lot about their research interests and how they intersect with our interests in this area. Overall, we're taking a really personal, hands-on approach with grants to academic researchers in this area because I think our interests and the research problems we think are most important are a little bit unusual and a little bit difficult to communicate&nbsp;about.</p><p>So, I think it's important for us to do these sort of relationship-based grants and to really spend the time talking to the students and professors in order to figure out what kinds of project would be most effective for them to&nbsp;do.</p><p>So far, the main support that we've lent to students is via their professors. So often academic grants will support a professor, part of a professor's time and much of several of their students' times. But this fall we're hoping to offer a fellowship for PhD students, which is a major way that machine learning PhD students are&nbsp;supported.</p><p>I'm quite bullish on this. I think that it's reasonable to expect a lot of the really good research and ideas to come from these PhD students who will have started thinking about these things earlier in their careers and had more opportunity to explore a really wide variety of different problems and approaches. But again, offering a PhD fellowship is not something we've done before so I think it's going to be sort of experimental and iterative to figure out how exactly it's going to&nbsp;work.</p><p>As far as workshops, we've held a workshop at Open Philanthropy Project for a bunch of grantees and potential grantees. Basically, as an experiment to see what happens when you bring together these academics and ask them to give talks about the AI alignment problem. We were quite happy with this. I think that people quickly jumped on board with these problems and are exploring a set of ideas that are closely related to the fields that they were working on before, but are approaching them from an angle that's closer to what we think might be required to handle AI&nbsp;alignment.</p><p>There are also workshops like Reliable Machine Learning in the Wild that have been in academic machine learning conferences, which are the major way that academics communicate with each other and publish results. Conferences dominate over journals in the field of machine learning. So we think supporting workshops at conferences is a good way to build up this&nbsp;community.</p><p>And it really depends on being able to communicate these problems to professors and students because they're the primary organizing force in these&nbsp;workshops.</p><p>There are other developments that I think you guys might be especially interested in. There's the Open Philanthropy Project partnership with OpenAI, which I think Holden talked about a little bit yesterday. We're quite excited about this. It's an unusual grant because it's not the case that we're just contributing money to a group and then letting them pursue the activities that they were going to pursue anyway. It's like a really active partnership between us and them to try to pool our talents and resources to pursue better outcomes from transformative&nbsp;AI.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/2RBUtbklN6ismSgaW4WEWI/01da3ce50015b3679af64b22a25e145b/advanced_ai_5.png?w=1800&amp;q=70 1800w\"></figure><p>So, I'm really excited about that. It's not clear exactly what kinds of results and updates and communications it makes sense to expect from that because it's still pretty early, but I have high hopes for it. We funded the Machine Intelligence Research Institute last year and we're still in a lot of conversations with them about their particular outlook on this problem and the work that they're&nbsp;doing.</p><p>There's a collaboration between OpenAI and Deep Mind. So this is something that the Open Philanthropy Project isn\u2019t funding or playing a role in directly, but I think it's an exciting development just for people who care about this area. So, OpenAI's a nonprofit and Deep Mind is part of Google, but in theory they could be viewed as competitors for producing artificial general intelligence. So I think it's really encouraging to see their safety teams working together and producing research on the alignment problem. I think that's a robustly positive thing to&nbsp;do.</p><p>I also happen to think that the research that they did jointly publish, which is about learning from human feedback - so, having an AI system demonstrate a series of behaviors and having a human rate those behaviors and using those ratings to guide the learning of the AI system - I think this is a really promising research direction. A lot of this research is related to Paul Christiano's concept of act-based agents, which personally I'm really optimistic about as a new direction in the AI alignment&nbsp;problem.</p><figure class=\"image image_resized\" style=\"width:750px;\"><img src=\"https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=100&amp;q=50\" srcset=\"https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=100&amp;q=50 100w, https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=480&amp;q=70 480w, https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=768&amp;q=70 768w, https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=992&amp;q=70 992w, https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=1200&amp;q=70 1200w, https://images.ctfassets.net/ohf186sfn6di/13aDKg1GmSis2oogGS4UeE/fede2dfbd83552b0742b9d486d332e6b/advanced_ai_6.png?w=1800&amp;q=70 1800w\"></figure><h3><strong>Our Strategy in this&nbsp;Area</strong></h3><p>So, overall, the takeaway here: last year we published a blog post on the philanthropic opportunity that we saw from transformative AI. And looking back on that a year later, I think that short timelines still look plausible. This greater than 10% chance over the next 20 years of developing transformative AI seems really real. And additionally, we increasingly think that Open Philanthropy Project can make the biggest difference in the world where timelines are short in that way. So, a major criterion that we apply to the work that we're doing is: would this be useful if AGI were developed within the next 20 years or&nbsp;so.</p><p>Neglectedness still looks really high. We haven't seen a lot of other funders jumping into this space over the next year and I think it was really possible given the increase in attention to artificial general intelligence, that this space would become much more crowded. I think Open Philanthropy Project and this community are still in a pretty unusual position to influence outcomes in this area just because it is so&nbsp;neglected.</p><p>And after having done some experiments in strategy and field building in technical AI alignment research, I think tractability looks higher than it did before. It's probably within the general range that we thought it was in, but maybe more concentrated in the high end. Just as we've gone on and talked to more and more AI researchers, it's been easier than expected to communicate the things that we're interested to find common ground between what they think they could do productive research on and what we think would make the biggest difference for the future trajectory of human&nbsp;civilization.</p><p>So those are the continued high-priorities for us. We're still spending a lot of senior staff time on it and I think it's a cause area that it makes sense to pay attention to if you're interested in the long-term trajectory of human&nbsp;civilization.</p><p>I'll take questions now, and thanks for your&nbsp;time.</p><p><strong>Question:</strong> Do you think that we should or if it is even possible to slow the advance of AI until some of these areas can mature that you're investing&nbsp;in?</p><p><strong>Daniel Dewey:</strong> I think that's a good question. My current guess is that we don't have very good levers for affecting the speed of AI development. I think there's so much money and so much pressure in the rest of society to develop artificial intelligence that it\u2019s not in a place where we have a particularly strong advantage. Slowing down technology is, I think, quite difficult to do and it would take a really concerted effort on the part of a much larger&nbsp;community.</p><p>But on top of that, I think it's a really open question how much it makes sense to think of this as like a race between two totally separate technologies, which are like capabilities and safety. My experience has been that you need a certain amount of capability in order to really do a lot of the research on AI&nbsp;safety.</p><p>So, yeah. It doesn't seem that tractable to me and even if it were more tractable, I think it's still sort of an open strategic&nbsp;question.</p><p><strong>Question:</strong> Okay. Great. Next&nbsp;question.</p><p>Given the massive advantage that someone or some group could gain from winning the AI race, let's say, it seems to this questioner that the strategic considerations are perhaps the biggest risk. So, how does the field building that you're engaged in help us avoid this sort of arms race scenario in&nbsp;AI?</p><p><strong>Daniel Dewey:</strong> I don't want to express too much confidence about this, but the way that I currently see the strategic field building work playing out is that we don't really want people making up their strategies on the fly, in a panic at the last minute. And if there are people who have done work ahead of time and gained expertise in the strategic considerations that are going on here, I think that we can have much better, more detailed, more well worked out plans for groups to coordinate with each other to achieve their shared&nbsp;interests.</p><p>And then also if there are some groups that we think will use AI more responsibly, or some governmental structures that we think would be more conducive to overall flourishing, I think that's not something you can work out at the last minute. So, I see developing a strategy for mitigating harms from misuse or from racing as something that we need these strategy experts to do. I don't think it's something that we can do in our spare time or something that people can do casually while they're working on something else. I think it's something that you really want people working on full&nbsp;time.</p><p>So I guess that's my perspective. Since we don't know what to do, that we should develop these&nbsp;experts.</p><p><strong>Question:</strong> Another question that touches on several of the themes that you just mentioned there. How do you expect that AI development will impact human employment and how do think that will then impact the way that governments choose to engage with this whole&nbsp;area?</p><p><strong>Daniel Dewey:</strong> Yeah. This a super good&nbsp;question.</p><p>I don't have a good answer to this question. I think that there are interesting lessons from self-driving cars where I think most people who have been keeping up with self-driving cars, with the raw technological progress, have been a little bit surprised by the slowness of this technology to roll out into the&nbsp;world.</p><p>So, I think one possibility that's worth considering is that it takes so long to bring a technology from a proof of concept in the lab to a broad scale in the world. That there could be this delay that causes a big jump in effective capabilities in the world where maybe we have, in the lab, the technology to replace a lot of human labor but it takes a long time to restructure the marketplace or to pass regulatory barriers or handle other mundane obstacles to applying a new&nbsp;technology.</p><p>But I think it's absolutely worth considering and it's an important strategic question if there going to be things like employment or things like autonomous weapons that will cause governments to react dramatically to AI in the really short term. In the US the big example is truck driving. Is autonomous truck driving going to cause some concerted reaction from the US government? I don't really know. I think this is a question we would like to fund to&nbsp;answer.</p><p><strong>Question:</strong> Obviously, there's a lot of debate between openness and more closed approaches in AI&nbsp;research.</p><p><strong>Daniel Dewey:</strong> Yeah.</p><p><strong>Question:</strong> The grant to OpenAI's a big bet, obviously, on the open side of that ledger. How are you thinking about open and closed or that continuum between those two extremes and how does your bet on OpenAI fit into&nbsp;that?</p><p><strong>Daniel Dewey:</strong> So, I don't actually think that the bet on OpenAI is a strong vote in favor of openness. I think that their philosophy, as I understand it in this area, is that openness is something that they think is a good heuristic. Like it's a good place to start from in some sense. That if one of the things you're worried about is uneven distribution of power, there's this powerful mechanism of distributing information and capabilities and technology more&nbsp;widely.</p><p>But if you go and look at what they've written about it, especially more recently, they've been pretty clear that they're going to be pragmatic and flexible and that if they're sitting around a table and they've developed something and their prediction is that releasing it openly would cause horrible consequence, they're not going to be like, \"Well, we committed to being open. I guess we have to release this even though we know it's going to be awful for the&nbsp;world.\"</p><p>My perspective on openness is that, I mean, this is a boring answer. I think it's one of these strategic questions that like you can do a shallow analysis and say like, if you're worried about the risk of a small group of people taking a disproportionate chunk of influence and that that would be really bad, then maybe you want to be more open. If you're mostly worried about offense beating defense and only one hostile actor could cause immense harm, then you're probably gonna be more excited about closedness then&nbsp;openness.</p><p>But I think we need to move past this shallow strategic analysis. Like, we need people working in a real way on the detailed, nitty-gritty aspects of how different scenarios would play out, because I don't think there's a simple conceptual answer to whether openness or closedness is the right&nbsp;call.</p><p><strong>Question:</strong> Well, we'll have it to leave it there for today. Round of applause for Daniel&nbsp;Dewey.</p><p><strong>Daniel Dewey:</strong> Cool. Thank&nbsp;you.</p>", "user": {"username": "The Centre for Effective Altruism"}}, {"_id": "MrCjtdTxHvRzqu8fp", "title": "The Hidden Cost of Shifting Away from Poverty", "postedAt": "2017-10-09T15:58:02.154Z", "htmlBody": "<html><body><p>&#xA0;</p>\n<div>&#xA0;</div>\n<div>The Center for Effective Altruism and effective altruists active in online spaces have for a while now been shifting away from a focus on poverty toward a focus on the far future and meta-level work (and if not that, animal advocacy). Interestingly, the rank and file of effective altruism&#xA0;<a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\">does not seem to have made this shift</a>&#xA0;(or at least completed it). I generally agree with CEA and the online community on this. I think it&apos;s a shift with solid reasoning behind it. I think there&apos;s reason to pause, though, and appreciate some of what EA loses by making this shift.<br><br>Much of what EA loses by making this shift has been discussed: things become very abstract in a way that may not be compelling to as many people, and there are concerns about an&#xA0;<a href=\"http://everydayutilitarian.com/essays/why-im-skeptical-about-unproven-causes-and-you-should-be-too/\">overly speculative cause</a>.<br><br>I believe there are other concerns to be had, though. In particular, there is an immense amount that EAs can learn from the global poverty space and apply to other spaces, and I see very few EAs doing that. The things I see EAs missing out on are a drive toward rigor, institutional capital, and organization.<br><br><strong>Drive Toward Rigor</strong></div>\n<div>&#xA0;</div>\n<div>The &quot;randomista&quot; movement in poverty alleviation illustrated many of the basic concepts that motivate EAs in a concrete and extremely persuasive way. What &quot;randomista&quot; economists such as Esther Duflo and Michael Kremer did in the 1990s and early 2000s was to make rigorous and scientific a field that had been dominated by sentimentality and false hopes. It&apos;s easy today to look back and see as obvious the idea of comparing randomly assigned treatment and control groups for poverty alleviation programs, but this was not obvious. This sort of thing was just generally not the way social science was done, because economics is messy, and studying it the way we study medicine would be too difficult. The randomistas blew that idea out of the water.<br><br>EAs are increasingly working in theoretical spaces similar to pre-2000s development economics. Animal advocacy, EA movement-building, and cause prioritization could likely learn from the nearly neurotic desire to be empirically rigorous that created the randomization movement in poverty alleviation. Things that appear unmeasurable may actually be measurable with the right amount of determination and inventiveness. Far future causes may be genuinely unmeasurable, although some of the ingredients to improving the far future (such as effectively recruiting technical researchers and persuading others) are not. To learn how to measure those things, though, we need to learn from the greatest, and the global poverty space has a lot to offer there.<br><br><strong>Institutional Capital</strong></div>\n<div>&#xA0;</div>\n<div>There is a large network of organizations and donors in the poverty space who share virtually all EA values except neutrality with respect to generation and species. Dean Karlan, one of the randomistas, regularly cites Peter Singer in his speeches. The World Bank, the Gates Foundation, the Ford Foundation, and many other powerful bodies are invested in evidence-based poverty work and place high value on shifting their funding based on where the evidence points rather than ideology.<br><br>As I said, these organizations do not share values many EAs hold with respect to the far future and anti-speciesism, but they do share most of the values that differentiate EAs from the rest of the world, and maintaining relationships with these organizations offers institutional, intellectual, and human capital.<br><br><strong>Organization</strong></div>\n<div>&#xA0;</div>\n<div>The evidence-based organizations in the global poverty space now have two decades of experience researching effective policies and putting them into action.&#xA0;<a href=\"https://www.evidenceaction.org/dewormtheworld/#putting-deworming-on-the-global-agenda\">Evidence Action has efficiently spread deworming to a number of countries based on a growing body of evidence</a>. There are established academic pipelines to get trained in this space for both research and for effective policymaking.<br><br>No doubt the greater amount of money in this area has a large role in its organization, but time plays a significant role as well. Other EA cause areas can speed up progress by learning from the organization that poverty alleviation charities and researchers have developed.<br><br>In short, I think that at the very least a larger number of effective altruists interested in non-poverty causes should develop experience in the poverty arena. The level of rigor and institutional knowledge in that area offers something to which other cause areas could aspire.\n<div>&#xA0;</div>\n</div>\n<div>&#xA0;</div>\n<div>\n<div><em>(Cross-posted on zachgroff.com)</em></div>\n</div></body></html>", "user": {"username": "zdgroff"}}, {"_id": "FuzoJwhPtaJTCkSzr", "title": "The Effective Altruism Equality and Justice Project", "postedAt": "2017-10-06T20:56:08.241Z", "htmlBody": "<html><body><p><!-- [if gte mso 9]><xml>\n<w:WordDocument>\n<w:View>Normal</w:View>\n<w:Zoom>0</w:Zoom>\n<w:TrackMoves/>\n<w:TrackFormatting/>\n<w:PunctuationKerning/>\n<w:ValidateAgainstSchemas/>\n<w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n<w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n<w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n<w:DoNotPromoteQF/>\n<w:LidThemeOther>EN-GB</w:LidThemeOther>\n<w:LidThemeAsian>X-NONE</w:LidThemeAsian>\n<w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript>\n<w:Compatibility>\n<w:BreakWrappedTables/>\n<w:SnapToGridInCell/>\n<w:WrapTextWithPunct/>\n<w:UseAsianBreakRules/>\n<w:DontGrowAutofit/>\n<w:SplitPgBreakAndParaMark/>\n<w:EnableOpenTypeKerning/>\n<w:DontFlipMirrorIndents/>\n<w:OverrideTableStyleHps/>\n</w:Compatibility>\n<w:DoNotOptimizeForBrowser/>\n<m:mathPr>\n<m:mathFont m:val=\"Cambria Math\"/>\n<m:brkBin m:val=\"before\"/>\n<m:brkBinSub m:val=\"&#45;-\"/>\n<m:smallFrac m:val=\"off\"/>\n<m:dispDef/>\n<m:lMargin m:val=\"0\"/>\n<m:rMargin m:val=\"0\"/>\n<m:defJc m:val=\"centerGroup\"/>\n<m:wrapIndent m:val=\"1440\"/>\n<m:intLim m:val=\"subSup\"/>\n<m:naryLim m:val=\"undOvr\"/>\n</m:mathPr></w:WordDocument>\n</xml><![endif]--></p>\n<p><!-- [if gte mso 9]><xml>\n<w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"\nDefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"\nLatentStyleCount=\"267\">\n<w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\"/>\n<w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\"/>\n<w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\"/>\n<w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\"/>\n<w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\"/>\n<w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Table Grid\"/>\n<w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\"/>\n<w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\"/>\n<w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\"/>\n<w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\"/>\n</w:LatentStyles>\n</xml><![endif]--><!-- [if gte mso 10]>\n<style>\n/* Style Definitions */\ntable.MsoNormalTable\n{mso-style-name:\"Table Normal\";\nmso-tstyle-rowband-size:0;\nmso-tstyle-colband-size:0;\nmso-style-noshow:yes;\nmso-style-priority:99;\nmso-style-parent:\"\";\nmso-padding-alt:0cm 5.4pt 0cm 5.4pt;\nmso-para-margin:0cm;\nmso-para-margin-bottom:.0001pt;\nline-height:115%;\nmso-pagination:widow-orphan;\nfont-size:11.0pt;\nfont-family:\"Arial\",\"sans-serif\";\ncolor:black;}\n</style>\n<![endif]--></p>\n<p><!-- [if gte mso 9]><xml>\n<w:WordDocument>\n<w:View>Normal</w:View>\n<w:Zoom>0</w:Zoom>\n<w:TrackMoves/>\n<w:TrackFormatting/>\n<w:PunctuationKerning/>\n<w:ValidateAgainstSchemas/>\n<w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n<w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n<w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n<w:DoNotPromoteQF/>\n<w:LidThemeOther>EN-GB</w:LidThemeOther>\n<w:LidThemeAsian>X-NONE</w:LidThemeAsian>\n<w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript>\n<w:Compatibility>\n<w:BreakWrappedTables/>\n<w:SnapToGridInCell/>\n<w:WrapTextWithPunct/>\n<w:UseAsianBreakRules/>\n<w:DontGrowAutofit/>\n<w:SplitPgBreakAndParaMark/>\n<w:EnableOpenTypeKerning/>\n<w:DontFlipMirrorIndents/>\n<w:OverrideTableStyleHps/>\n</w:Compatibility>\n<w:DoNotOptimizeForBrowser/>\n<m:mathPr>\n<m:mathFont m:val=\"Cambria Math\"/>\n<m:brkBin m:val=\"before\"/>\n<m:brkBinSub m:val=\"&#45;-\"/>\n<m:smallFrac m:val=\"off\"/>\n<m:dispDef/>\n<m:lMargin m:val=\"0\"/>\n<m:rMargin m:val=\"0\"/>\n<m:defJc m:val=\"centerGroup\"/>\n<m:wrapIndent m:val=\"1440\"/>\n<m:intLim m:val=\"subSup\"/>\n<m:naryLim m:val=\"undOvr\"/>\n</m:mathPr></w:WordDocument>\n</xml><![endif]--></p>\n<p><!-- [if gte mso 9]><xml>\n<w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"\nDefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"\nLatentStyleCount=\"267\">\n<w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\"/>\n<w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\"/>\n<w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\"/>\n<w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\"/>\n<w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\"/>\n<w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Table Grid\"/>\n<w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\"/>\n<w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\"/>\n<w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\"/>\n<w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\"/>\n</w:LatentStyles>\n</xml><![endif]--><!-- [if gte mso 10]>\n<style>\n/* Style Definitions */\ntable.MsoNormalTable\n{mso-style-name:\"Table Normal\";\nmso-tstyle-rowband-size:0;\nmso-tstyle-colband-size:0;\nmso-style-noshow:yes;\nmso-style-priority:99;\nmso-style-parent:\"\";\nmso-padding-alt:0cm 5.4pt 0cm 5.4pt;\nmso-para-margin:0cm;\nmso-para-margin-bottom:.0001pt;\nline-height:115%;\nmso-pagination:widow-orphan;\nfont-size:11.0pt;\nfont-family:\"Arial\",\"sans-serif\";\ncolor:black;}\n</style>\n<![endif]--><strong><u><span>Background</span></u></strong></p>\n<p>Effective Altruism (EA) has successfully grown to become a global community of people who have made helping others a fundamental part of their lives. Its core idea &#x2014; using evidence and analysis to take actions that help others as much as possible &#x2014; appeals to a range of people across nationalities, political ideologies and religions. What its proponents have in common is they accept that we have limited resources at our disposal and that these resources should be directed where they can do the <em>most</em> good, downgrading projects that may be well-intended but would ultimately accomplish less in relative terms. While the EA community does not explicitly rely on utilitarianism, their emphasis on welfare maximisation, rationality and cause prioritisation can make EA a difficult sell to people whose ethical views are non-utilitarian. Recognising this obstacle was the motivation for this project, which sought to answer the following questions:</p>\n<p><em>How can people with non-utilitarian ethical views, such as egalitarians and justice-oriented individuals, find a place in the effective altruism community?</em></p>\n<p><em>And are effective altruism methods helpful when we seek to reduce systemic inequalities and social injustices?<br> </em></p>\n<p>To answer these questions, Sam Hilton from EA London sought to gather a group of 15-20 people who would meet 6-8 times during the summer of 2017. The group would work together to give away &#xA3;1000 of Sam&#x2019;s money to the best organisation it could find through effective value-led exploration, problem-solving research and conversations.</p>\n<p>The idea was suggested at an EA London strategy meeting and then advertised in the EA London Facebook group and on Meetup.com. In the end, only seven people (none of whom knew one another prior to the project) committed, although the initial meetings had a few more attendees. Each meeting was led by an experienced member of the EA community.</p>\n<p>Naturally, some constraints were present from the onset. First and foremost, we were a group of working professionals with limited time available to prepare for and work on the project. Furthermore, a lack of existing research on potential charities we would evaluate meant we would have to rely to a larger extent on intuitions than what would normally be the case in EA charity evaluations. We were not discouraged by these constraints, however, because while they would likely cause us to rely more on intuitions than we would prefer, we would still make a donation to an effective charity (even if not the most effective), provide a meaningful answer to a very important question for the future of EA and make a contribution to the community-building efforts of EA London.</p>\n<p>&#xA0;</p>\n<p><!-- [if gte mso 9]><xml>\n<w:WordDocument>\n<w:View>Normal</w:View>\n<w:Zoom>0</w:Zoom>\n<w:TrackMoves/>\n<w:TrackFormatting/>\n<w:PunctuationKerning/>\n<w:ValidateAgainstSchemas/>\n<w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n<w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n<w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n<w:DoNotPromoteQF/>\n<w:LidThemeOther>EN-GB</w:LidThemeOther>\n<w:LidThemeAsian>X-NONE</w:LidThemeAsian>\n<w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript>\n<w:Compatibility>\n<w:BreakWrappedTables/>\n<w:SnapToGridInCell/>\n<w:WrapTextWithPunct/>\n<w:UseAsianBreakRules/>\n<w:DontGrowAutofit/>\n<w:SplitPgBreakAndParaMark/>\n<w:EnableOpenTypeKerning/>\n<w:DontFlipMirrorIndents/>\n<w:OverrideTableStyleHps/>\n</w:Compatibility>\n<w:DoNotOptimizeForBrowser/>\n<m:mathPr>\n<m:mathFont m:val=\"Cambria Math\"/>\n<m:brkBin m:val=\"before\"/>\n<m:brkBinSub m:val=\"&#45;-\"/>\n<m:smallFrac m:val=\"off\"/>\n<m:dispDef/>\n<m:lMargin m:val=\"0\"/>\n<m:rMargin m:val=\"0\"/>\n<m:defJc m:val=\"centerGroup\"/>\n<m:wrapIndent m:val=\"1440\"/>\n<m:intLim m:val=\"subSup\"/>\n<m:naryLim m:val=\"undOvr\"/>\n</m:mathPr></w:WordDocument>\n</xml><![endif]--></p>\n<p><!-- [if gte mso 9]><xml>\n<w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"\nDefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"\nLatentStyleCount=\"267\">\n<w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\"/>\n<w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\"/>\n<w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\"/>\n<w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\"/>\n<w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\"/>\n<w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Table Grid\"/>\n<w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\"/>\n<w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\"/>\n<w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"\nUnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\"/>\n<w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\"/>\n</w:LatentStyles>\n</xml><![endif]--><!-- [if gte mso 10]>\n<style>\n/* Style Definitions */\ntable.MsoNormalTable\n{mso-style-name:\"Table Normal\";\nmso-tstyle-rowband-size:0;\nmso-tstyle-colband-size:0;\nmso-style-noshow:yes;\nmso-style-priority:99;\nmso-style-parent:\"\";\nmso-padding-alt:0cm 5.4pt 0cm 5.4pt;\nmso-para-margin:0cm;\nmso-para-margin-bottom:.0001pt;\nline-height:115%;\nmso-pagination:widow-orphan;\nfont-size:11.0pt;\nfont-family:\"Arial\",\"sans-serif\";\ncolor:black;}\n</style>\n<![endif]--><strong><u><span>Summary</span></u></strong></p>\n<p>The group defined its mission as &#x201C;For all humans, minimise the extent to which circumstances outside their control limit access to personal need, starting with the most basic needs.&#x201D; We arrived at this mission statement by consensus after various discussions and exercises to expose our core beliefs. Interestingly the group kept reverting to utilitarian principles of welfare maximisation when pressed to choose between basic needs and equality/justice, but by the same token the members were willing to trade off <em>some</em> satisfying of basic needs to make a significant improvement to equality and justice. This would create a conflict if we were to evaluate charities using a single metric or a quantified best guess measure, as that would ignore the subtleties of this trade-off and likely overstate or understate the relative importance of basic needs versus equality and justice. Therefore we decided instead to devise a rubric that considered multiple factors to <em>guide </em>our decision-making process.</p>\n<p>The rubric consisted of six categories, each worth up to five points. The categories were Basic needs, Impact (on one individual), Number of people impacted per &#xA3;1000, Quality of evidence, Levels of disadvantage, and Actively preventing discrimination. This approach seems entirely in line with EA if your cause is equality and justice; if one charity more effectively promotes equality and justice than others, without sacrificing basic needs and welfare maximisation to an unacceptable extent, we should dedicate our resources toward that charity. (Of course many EAs will argue that <em>any </em>compromising of welfare maximisation is unacceptable but this project is not targeted toward those people beyond showing them that EA can make non-utilitarians <em>more </em>effective).</p>\n<p>Populating our rubric and evaluating the effectiveness of charities we discovered that most of them were health-focused charities that were already recommended by various EA organisations (see below for details). After shortlisting our finalists, No Means No Worldwide won by one vote in a voting process and received the &#xA3;1000.</p>\n<p>For the purpose of answering whether or not non-utilitarians can be meaningful participants in and make use of the EA community and framework it seems less important what charity received the &#xA3;1000 than the fact that we were able to evaluate charities focusing on equality and justice while paying attention to the primacy of basic needs. Our charity evaluator was highly useful in the process and we were able to rely on research conducted by the EA community to work toward our mission. We believe these lessons demonstrate that the EA community can have impact on people who are not traditional utilitarians and equally that equality and justice-oriented people are able to incorporate EA methods without at all abandoning their core beliefs.</p>\n<p>While the project had a number of shortcomings, we do not believe they invalidate our positive answer to our initial questions. We are certainly not claiming that our approach is the <em>best </em>approach for non-utilitarians concerned with equality and justice, and we do not want to convince all EAs that they should incorporate those considerations when evaluating charities to the same extent that we did. What we do hope we have been able to demonstrate is that there is a place for non-utilitarians in EA that the community ought to embrace as it continues to grow. <br> _______________________________________________________</p>\n<p><em>(Keep reading for a detailed account of the various stages of the project. 1856 words. Estimated reading time: approximately 10 minutes)</em></p>\n<p>&#xA0;</p>\n<p><strong><u><span>How we did it</span></u></strong></p>\n<p><strong><em>Stage 1: Mapping values and defining the project</em></strong></p>\n<p>The first stage of the project centred around outlining the values of the group members and examine what we really mean when we talk about equality and justice. We considered a number of thought experiments designed to expose our core beliefs. This exercise served two purposes. First, it helped the group members who are not trained in or used to discussing philosophical or ethical questions to become familiar with how such questions should be approached. Second, it allowed us to see where the group as a whole fell on a spectrum ranging from completely utilitarian on one side to egalitarian/justice-oriented on the other, which would help us decide how to proceed in the later stages of the project. To visualise the result, we created a mind map of values (see <a href=\"https://realtimeboard.com/app/board/o9J_k0DKdME=/\"><span>https://realtimeboard.com/app/board/o9J_k0DKdME=/</span></a>). As expected, most members of the group displayed non-utilitarian first principles.</p>\n<p>Digging deeper into what we mean when we talk about equality and justice, we placed access to healthcare, education and employment opportunities centre stage and did not rank any concepts in order of importance at this point. We defined our common mission to be:&#xA0;</p>\n<p><em>&#x201C;For all humans, minimise the extent to which circumstances outside their control limit access to personal need, starting with the most basic needs.&#x201D;</em></p>\n<p><strong><em>Stage 2: Narrowing the scope</em></strong></p>\n<p>After defining our values and mission, we held initial discussions on charity types, methods, locations and operations that could help us best work towards our mission and eliminated a few options from our search. We did not want to choose a charity focused on sport, arts, culture, science, promoting religion or rescue/emergency as these focus areas deviate from our core mission. In setting this direction we also narrowed the scope, especially where there was group consensus around an issue. In particular we decided to focus on humans. This focus was a result of a consensus decision of the group upon considering each attendee&#x2019;s values and the current state of the world. (We recognise that setting the direction to exclude animals could be seen as a criticism of this project by some EAs. See: <a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1471898846199801/\"><span>https://www.facebook.com/groups/effective.altruists/permalink/1471898846199801/</span></a>) Furthermore, we decided to focus on a single-action charity and not a multi-action charity because for the latter, it is difficult to establish which of their interventions are the most effective and it is difficult to reliably measure impact. We also spent part of one session discussing how we could avoid cognitive biases when assessing our charities, agreeing that we would assess all charities according to the same set of criteria.&#xA0;</p>\n<p>Key factors in charity evaluation include cost effectiveness, room for more funding, transparency, impact and management. We decided to selectively apply some of these considerations to our project focusing mainly on cost effectiveness and impact, as those are comparatively easy to assess and arguably the most important.</p>\n<p>As well as eliminating a number of charity areas and deciding our evaluation criteria, we ranked the values we had associated with equality and justice on a scale of 0-10, where 0 represents death and 10 represents no material impact on the quality of your life. Prior to doing this we had researched the effects of promoting these values through charitable donations. The exercise exposed something most EAs already maintain: basic needs are perceived to be more important when measured up against secondary needs. For example, we assigned a 2-weighting to healthcare, but a 7 to education. Our challenge from here onwards then was to find out whether we were able to identify a charity that could work toward reducing systemic inequalities and social injustices without neglecting basic needs.&#xA0;</p>\n<p><strong><em>Stage 3: Using a rubric for making decisions</em></strong>&#xA0;</p>\n<p>Having noted that everyone who cared about equality and justice also cared about well-being and that utilitarian considerations were crucial to the members (all members were willing to allow some inequality to save a large number of lives, for example), it was difficult to use a single metric assigned to charities to measure effectiveness. EA charity evaluation work has often sought a single metric (e.g. QALYs) and/or a quantified best guess figure for different charities, but has avoided incorporating factors like equality and justice that make comparisons less accurate. Given the added complexity that arises when equality and justice also need to be considered, we decided to create a rubric that considered multiple factors.</p>\n<p>This part was challenging. We needed to devise a rubric that incorporated all the considerations outlined above. At the same time it had to be simple enough that we could employ it in a meaningful way with the limited time and resources we had available but sophisticated enough for us to succeed in showing non-utilitarians that EA methods are highly effective also if you are not a utilitarian. In other words, our rubric had to be a successful Proof of Concept.</p>\n<p>At first, our group thought it helpful to research some charities that could qualify and see what types of charities they were to figure out how to best evaluate them. Of course, this process was insufficient to identify the most effective charities according to our criteria, although we were able to establish that some charities, like Transparency International, would be too difficult to evaluate despite their significant contributions to promote equality and justice across the globe.</p>\n<p>The rest of our efforts at this stage concentrated on creating the rubric. As we had clearly established, it needed to 1) measure impact and 2) cost-effectiveness, 3) not ignore basic needs, 4) pay attention to the quality of evidence available, and prioritise charities that effectively 5) prevent discrimination and 6) offset people&#x2019;s levels of disadvantage. By determining how a charity scores according to these six categories, inputting the data into a spreadsheet, we were able to quantify the performance of the charity and establish its effectiveness.</p>\n<p>In our charity evaluator, all categories count equally and earn a charity 5 points, meaning charities are ranked from 0 to 30, where 30 represents the most effective and 0 the least effective.</p>\n<p>The categories are as follows:</p>\n<p>Basic needs:<br>1. Desires/Self-actualisation<br>2. Education<br>3. Community bonding / Mental health<br>4. Safety/Shelter/Healthcare<br>5. Water/Food</p>\n<p>Impact (on one individual):<br>1. Useless or harmful<br>2. Marginal gains<br>3. Noticeable improvements<br>4. Transforms life<br>5. Saves life</p>\n<p>Number of people impacted per &#xA3;1000:<br>1. Less than 1<br>2. 1 to 20<br>3. 21 to 200<br>4. 201 to 2000<br>5. More than 2000</p>\n<p>Quality of evidence:<br>1. No evidence<br>2. Reasonable assumptions / good theoretical framework<br>3. One or a few rigorous studies<br>4. One randomised controlled trial (RCT) or several quasi-experiments<br>5. Several RCTs</p>\n<p>Levels of disadvantage:<br>1. Recipients not disadvantaged<br>2. Recipients disadvantaged in 1 way (e.g. they are poor)<br>3. Recipients disadvantaged in 2 ways (e.g. they are poor and female)<br>4. Recipients disadvantaged in 3 ways<br>5. Addresses many layers of disadvantage at systemic level</p>\n<p>Actively preventing discrimination:<br>1. Entrenching current injustices<br>2. Addresses non-justice issues (e.g. giving bed nets to the poor)<br>3. Addresses non-justice issues through empowerment (e.g. giving money to the poor)<br>4. Prevents passive injustice<br>5. Prevents active injustice<span>&#xA0; </span></p>\n<p>Of course, the charity evaluator only works to the extent that people have faith in the data that goes into it. And naturally our spreadsheet is a prototype that contains omissions. However, already at this stage in the project we had illustrated that it is both feasible and entirely rational to measure how effectively a charity promotes justice and equality objectives. If we can establish that one charity more effectively creates equality and justice than another, we should not focus our resources on the one that is less effective.</p>\n<p>(See <a href=\"https://docs.google.com/spreadsheets/d/1xe8RSgK0a6DgPx7DKAUVrXjFM285lYLrsdFBfpCRJm8/htmlviewb\"><span>https://docs.google.com/spreadsheets/d/1xe8RSgK0a6DgPx7DKAUVrXjFM285lYLrsdFBfpCRJm8/htmlview</span></a><a href=\"https://docs.google.com/spreadsheets/d/1xe8RSgK0a6DgPx7DKAUVrXjFM285lYLrsdFBfpCRJm8/htmlviewb\"><span>b</span></a>)</p>\n<p><strong><em>Stage 4: Choosing a charity</em></strong></p>\n<p>Because we wanted our spreadsheet to contain as much detailed information as possible, we reached out to the EA community (on various EA Facebook group pages) to fill in information using a shared public Google document.</p>\n<p>During the two weeks we made the document available for editing, we did not receive the attention we wanted and the responses came mainly from within the group. One reason the spreadsheet did not get the attention we expected could be that we had not advertised our project well enough in advance and did not provide adequate background information when asking people to help us fill in the information. Another reason could be that it was in the middle of July and people had other priorities. Either way, we decided to organise a co-working session and the group met to work together to fill in the spreadsheet ourselves. That way, we were able to question the input data and explain why we nominated certain charities and not others.</p>\n<p>To assign values to the impact and quality of evidence evidence categories, we relied mostly on research conducted by GiveWell, thus placing a high level of confidence in the accuracy of our data.</p>\n<p>The charities that scored well according to our evaluator typically focused on health, in particular children and women&#x2019;s health, and gender equality. The charities that came out on top included Population Services International, Project Healthy Children, Maternity Worldwide, Against Malaria Foundation, Give Directly, The Deworm the World Initiative (Evidence Action) and No Means No Worldwide. (Due to an error, The Schistosomiasis Control Initiative (SCI) disappeared on the version we were using during the co-working session, something we did not discover until after the project. We cannot know if SCI would have been chosen had this error not occurred. We extend our apologies to SCI for this error.)</p>\n<p>With a shortlist of finalists following our co-working session, our final meeting was dedicated to decide on a charity. While the evaluator provided a useful indication of which charities were effective and not, we wanted to keep the final decision democratic and we revisited our mission: <em>&#x201C;For all humans, minimise the extent to which circumstances outside their control limit access to personal need, starting with the most basic needs.&#x201D;</em></p>\n<p>Although we voiced our opinions on the finalists, we decided to keep the decision democratic and choose our charity through dotmocracy, giving each group member five dots to distribute between our final charities. While this is an effective method of deciding between many options, the process would have been better blinded to avoid anchoring.</p>\n<p>When making the final decision, only five members were present. We had narrowed down our choice to two charities: Population Services International (PSI) and No Means No Worldwide (NMNW). We believed that their missions, &#x201C;[making] it easier for people in the developing world to lead healthier lives and plan the families they desire&#x201D; and &#x201C;create a rape free world&#x201D;, respectively, actively addressed our own mission and the charities were demonstrably highly effective. We held a final vote between the two giving each of the five members one vote.</p>\n<p>Ultimately, No Means No Worldwide won with 3 against 2 votes, earning it the &#xA3;1000. One key argument by those who voted for them was that PSI is a multi-action charity and that we could not be certain that donations would be going specifically to improve equality and justice to the same extent that NMNW would ensure. The group may post a separate write-up on the estimated effectiveness of NMNW.&#xA0;</p>\n<p>We look forward to your feedback.&#xA0;</p>\n<p><em>Written by Pouya Jafari, with Enrico Calvanese, Kirsten Horton, Dr Colin McClure, Ellie Karslake, Naim Sheikh and Ruth Stokes. Many thanks to Samuel Hilton, Sanjay Joshi, Holly Morgan and Saulius &#x160;im&#x10D;ikas.</em></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "PouyaJafari"}}, {"_id": "A54Lrz5vQK5kgtxA2", "title": "EA Survey 2017 Series: Have EA Priorities Changed Over Time?", "postedAt": "2017-10-06T15:51:49.329Z", "htmlBody": "<html><body><p><img src=\"https://i.imgur.com/lSCiAYt.png?2\"></p>\n<p><span>By Peter Hurford and Tee Barnett</span></p>\n<p>&#xA0;</p>\n<blockquote>\n<p><span>This is the seventh article in the EA Survey 2017 Series.</span>&#xA0;<span>You can find supporting documents at the bottom of this post, including previous EA surveys conducted by <a href=\"https://rtcharity.org/\">Rethink Charity</a>, and an up-to-date list of articles in the series. Get notified of the latest posts in this series by signing up</span>&#xA0;<a href=\"http://eepurl.com/c2MaW5\"><span>here</span></a><span>.</span></p>\n</blockquote>\n<h3><span>Summary</span></h3>\n<ul>\n<li>\n<p><span>We use past survey data to shed light on community shifts in cause area preferences over time. </span></p>\n</li>\n<li>\n<p><span>Our evidence suggests that EAs are becoming more favorable toward AI and less favorable toward politics.</span></p>\n</li>\n<li>\n<p><span>EAs in both the 2015 and 2017 surveys shifted away from viewing poverty as a &#x201C;top&#x201D; or &quot;near top&quot; cause. </span></p>\n</li>\n<li>\n<p><span>Newcomers in the 2015 survey were less accepting of global poverty than veterans. However, the reverse was true in the 2017 survey, with newcomers being more accepting of global poverty than veterans. </span></p>\n</li>\n<li>\n<p><span>There is no indication that EAs are getting less interested in animal welfare with time. </span></p>\n</li>\n</ul>\n<h3><span>Cause Preference Shifts</span></h3>\n<p><span>Our previous posts in this series were largely descriptive, often reporting on 2015 and 2016 to provide an approximate snapshot of the current EA community. As the series progresses into late 2017, we&#x2019;ll look to extract further insight from the data, which will include various longitudinal analyses, commentary on the Pledge, and potentially other angles upon request.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>We turn first to a commonly held narrative within the community &#x2013; that new EAs are typically attracted to poverty relief as a top cause initially, but subsequently branch out after exploring other EA cause areas. An extension of this line of thinking credits increased familiarity with EA for making AI more palatable as a cause area. In other words, the top of the EA outreach funnel is most relatable to newcomers (poverty), while cause areas toward the bottom of the funnel (AI) seem more appealing with time and further exposure. (For example, see Michael Plant&#x2019;s </span><a href=\"/ea/1cd/the_marketing_gap_and_a_plea_for_moral_inclusivity/\"><span>post</span></a><span> &#x201C;The marketing gap and a plea for moral inclusivity&#x201D;.) While we previously </span><a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\"><span>reported</span></a><span> higher support for global poverty as a top cause, we find reason to support some version of a narrative suggesting that EAs are shifting away from global poverty.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>There are two ways we&#x2019;ve looked at changes in preference toward causes over time. First, we took the information on what year EAs joined the community, and compared the cause preferences of earlier EAs to newcomers. Our second method involved taking the population of EAs who took the EA Survey in both 2015 and 2017 and seeing how the same people changed their opinions of their &#xA0;top cause over this two year gap. The first method has a larger sample size, while the second version captures intrapersonal attitude shifts over time. Both tell a similar tale.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Using the longitudinal method, there were 184 people who took both the 2015 and 2017 EA Surveys that we could match (using a hashed email address to preserve anonymity). To get a quick overview of cause preference change over time, we looked at the number of people who shifted toward a cause (they previously had not considered the cause to be a &#x201C;top priority&#x201D; or &#x201C;near the top priority&#x201D; in 2015, but now do as of 2017) and subtracted the number of people who shifted away from a cause (they previously had considered the cause to be &#x201C;top&#x201D; or &#x201C;near top&#x201D; and now don&#x2019;t). This gave us a number we called a &#x201C;net shift&#x201D; from a cause.</span></p>\n<p>&#xA0;</p>\n<p><span>Cause area preferences fluctuated slightly between the 2015 and 2017 EA surveys (Table 1). Poverty remains the clear community favorite, although the net shift in preference broken down by cause area reveals that interest has been waning in poverty since the 2015 EA survey, with a net shift of -8. Interestingly, politics has hemorrhaged the most interest (-13) in the wake of Brexit, Trump&#x2019;s victory, and other significant political developments in traditional EA hubs. The biggest winner in net gains is AI (+29) and non-AI far future (+14), which suggests at least some directional movement toward long-term concerns over time. </span></p>\n<p><img src=\"https://i.imgur.com/TZPxmPr.png\"></p>\n<p><span><span>We were compelled to take a closer look at the dropping interest in poverty, particularly due to its continued popularity in the aggregate and traditional status as an EA mainstay. Between the 2015 and the 2017 surveys, 14.13% of EAs in the longitudinal sample changed their mind about how much importance should be placed on the cause (Table 2), with 9.24% of these EAs no longer considering poverty as a &#x201C;top&#x201D; or &#x201C;near top&#x201D; cause, and 4.89% of EAs upgrading their estimation of poverty&#x2019;s importance.</span></span></p>\n<p><img src=\"https://i.imgur.com/9m3NXwD.png\"></p>\n<p><span><span>However, there has been more movement within the distinction between &#x201C;top&#x201D; and &#x201C;near top&#x201D;, with 19.02% of EAs in the longitudinal sample relegating poverty from being the top cause two years later and only 5.98% of EAs upgrading their estimation of poverty as the most important cause area (Table 3).</span></span></p>\n<p><img src=\"https://i.imgur.com/4quxs4e.png\"></p>\n<p><span>To look at this from another perspective, we took the 2017 EA Survey population and distinguished between whether an EA was more of a &#x201C;veteran&#x201D; who learned about EA in 2013 or earlier or was more of a relative newcomer who learned about EA in 2014 or later[1]. The hypothesis is that veteran EAs would have had more time to shift their beliefs in causes and may be predictive of how newcomers will eventually shift.</span></p>\n<p><span>&#xA0;</span></p>\n<p><span>Taking initial preferences into consideration, EAs who joined in 2013 or earlier were far less likely to rank poverty as the &#x201C;top&#x201D; or &#x201C;near top&#x201D; priority than EAs who joined in 2014 or later (Table 4), though a majority of these veteran EAs still ranked poverty as the &#x201C;top&#x201D; or &#x201C;near top&#x201D; cause. </span>&#xA0;</p>\n<p><img src=\"https://i.imgur.com/KmMoedu.png\"></p>\n<p><span><span>One potential explanation for this shift might not be a genuine change in opinion over time, but instead that veteran EAs were always less likely to be into poverty, whereas newer EAs are a lot more likely to be into poverty. To check our base assumption about whether there has been a significant influx of poverty-focused EAs in recent years, we looked back at the 2015 EA Survey and compared it to the 2017 EA Survey (Table 5). </span></span></p>\n<p><img src=\"https://i.imgur.com/3GyAqAM.png\"></p>\n<p><span>As of the 2015 Survey, newcomers were actually relatively less accepting of global poverty than the veterans, but this effect reverses as of the 2017 EA Survey. This could point to a difference in attitudes for newcomers in 2015 and 2017 skewing the data, rather than newcomers from 2015 changing their minds over time.</span></p>\n<p><span>&#xA0;</span></p>\n<p><span>The data is not entirely clear on whether initially interested EAs change their views away from poverty with time. The perceived separation between veteran EAs being less poverty-focused may be down to initial dispositions, rather than later conversions. The 2017 EA survey data does suggest that most newcomers enter the movement interested in poverty, which may have implications for movement building organizations to bear in mind. </span></p>\n<h3><span>Attitudes Toward AI</span></h3>\n<p><span><span><span>Turning to AI, not only has resistance to devoting resources to AI safety reduced substantially since the 2015 EA Survey, but </span><a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\"><span>we showed</span></a><span> that this set of concerns is now actively competing with other cause areas for top priority billing.</span></span></span></p>\n<p><img src=\"https://i.imgur.com/xSC6lzs.png\"></p>\n<p><span><span>There were more people changing their minds on AI than global poverty (Table 6), with 19.57% of EAs in our longitudinal sample choosing to upgrade the importance of AI in their view to a &#x201C;top&#x201D; or &#x201C;near top&#x201D; cause and only 3.8% of EAs choosing to downgrade it out of &#x201C;top&#x201D; and &#x201C;near top&#x201D;. When looking at just top cause area preference, the trends were roughly similar, with 13.04% of EAs in the longitudinal sample promoting AI to the top cause and 7.07% demoting AI from top cause to something else.</span></span></p>\n<p><img src=\"https://i.imgur.com/dS2RCHN.png\"></p>\n<p><span>Among those veteran EAs who joined in 2013 or earlier, the support for AI as a &#x201C;top&#x201D; or &#x201C;near top&#x201D; priority was closer to 50-50, whereas for EAs who joined in 2014 or later, there is less support for AI as a &#x201C;top&#x201D; or &#x201C;near top&#x201D; cause (Table 7). The net shift of aggregate interest toward AI (Table 1), a broad trend favoring AI (Table 6), combined with our knowledge that newer EAs favor AI relatively less (Table 7), would seem to suggest that more exposure to EA increases the likelihood of becoming more inclined to support AI safety over time. </span></p>\n<h3><span>Attitudes Toward Animal Welfare</span>&#xA0;</h3>\n<p><span>We were also curious to check the same for animal rights, to see how EA interest in helping animals as a cause has changed over the years.</span></p>\n<p><img src=\"https://i.imgur.com/qqjAfSM.png\"></p>\n<p><span><span>Here we see that among the 2017 EA Survey respondents, unlike with AI, there is no statistically significant difference between the rate at which newcomers and veterans support animal rights (Table 9). Furthermore, there has been a net shift toward animal welfare among those who took both the 2015 and 2017 EA Surveys (Table 8). Thus, suggestions that EAs are getting less interested in animal welfare over time does not seem to be confirmed by EA Survey data.</span></span></p>\n<p><img src=\"https://i.imgur.com/fRC281L.png\"></p>\n<p><span>Among the 2017 EA Survey respondents, newcomers to EA are relatively more likely to support politics than veterans, though the majority of both newcomers and veterans do not support politics as a &#x201C;top&#x201D; or &#x201C;near top&#x201D; cause (Table 11). Similarly, among those who took both the 2015 and 2017 EA Surveys, people are shifting away from thinking of politics as a &#x201C;top&#x201D; or &#x201C;near top&#x201D; cause (Table 10). This may mean that while politics is less popular as an EA cause overall, EAs tend to shift away from it over time. Likewise, it is interesting that it seems like contentious developments of late may have not had any sort of energizing effect on getting EAs interested in politics, as far as we can tell in this survey data. </span></p>\n<h3><span>Endnotes</span></h3>\n<p><span>[1]:</span><span> This effect is statistically significant at p &lt; 0.00001 for both. We chose 2013 because we felt it properly conveyed &#x201C;veteran&#x201D; status before a lot of popular growth in EA in 2014, but this effect remains the same in direction and statistical significance, with similar strength, regardless of your choice for cut-off year (tested with 2011, 2012, 2013, 2014, and 2015 as cut-off years).</span></p>\n<h3><span>Credits</span></h3>\n<p><span>Post written by Peter Hurford and Tee Barnett</span></p>\n<p>&#xA0;</p>\n<p><span>The annual EA Survey is a volunteer-led project of </span><a href=\"http://rtcharity.org\"><span>Rethink Charity</span></a><span> that has become a benchmark for better understanding the EA community. A special thanks to Ellen McGeoch, Peter Hurford, and Tom Ash for leading and coordinating the 2017 EA Survey. Additional acknowledgements include: Michael Sadowsky and Gina Stuessy for their contribution to the construction and distribution of the survey, Peter Hurford and Michael Sadowsky for conducting the data analysis, and our volunteers who assisted with beta testing and reporting: Heather Adams, Mario Beraha, Jackie Burhans, and Nick Yeretsian.</span></p>\n<p>&#xA0;</p>\n<p><span>Thanks once again to Ellen McGeoch for her presentation of the 2017 EA Survey results at EA Global San Francisco.</span></p>\n<p>&#xA0;</p>\n<p><span>We would also like to express our appreciation to the Centre for Effective Altruism, Scott Alexander via SlateStarCodex, 80,000 Hours, EA London, and Animal Charity Evaluators for their assistance in distributing the survey. Thanks also to everyone who took and shared the survey.</span></p>\n<h3><span>Supporting Documents</span></h3>\n<h3><span>EA Survey 2017 Series Articles</span></h3>\n<p><span>I -</span><a href=\"/ea/1e0/effective_altruism_survey_2017_distribution_and/\"><span> Distribution and Analysis Methodology</span></a></p>\n<p><span>II -</span><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"><span> Community Demographics &amp; Beliefs</span></a></p>\n<p><span>III -</span><a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\"> <span>Cause Area Preferences</span></a></p>\n<p><span>IV -</span><a href=\"/ea/1el/ea_survey_2017_series_donation_data/\"> <span>Donation Data</span></a></p>\n<p><span>V -</span><a href=\"/ea/1ex/demographics_ii/\"><span> &#xA0;</span><span>Demographics II</span></a></p>\n<p><span>VI -</span><a href=\"/ea/1f5/ea_survey_2017_series_qualitative_comments_summary/\"> <span>Qualitative Comments Summary</span></a></p>\n<p><span>VII - <a href=\"/ea/1fi/have_ea_priorities_changed_over_time/\">Have EA Priorities Changed Over Time?</a></span></p>\n<p><span><span>VIII - </span><a href=\"/ea/1h5/ea_survey_2017_series_how_do_people_get_into_ea/\">How do People Get Into EA?</a></span></p>\n<p>&#xA0;</p>\n<p><span>Please note: this section will be continually updated as new posts are published. </span><span>All 2017 EA Survey posts will be compiled into a single report at the end of this publishing cycle</span></p>\n<h3><span>Prior EA Surveys conducted by Rethink Charity (formerly .impact)</span></h3>\n<p><a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\"><span>The 2015 Survey of Effective Altruists: Results and Analysis</span></a></p>\n<p><a href=\"/ea/gb/the_2014_survey_of_effective_altruists_results/\"><span>The 2014 Survey of Effective Altruists: Results and Analysis</span></a></p></body></html>", "user": {"username": "Tee"}}, {"_id": "HHeaoX47aAa8dgt7a", "title": "Lessons from a full-time community builder. Part 1 of 4. Impact assessment", "postedAt": "2017-10-04T18:14:12.357Z", "htmlBody": "<p><span>NOTE: For the past year Sam Hilton has been funded by the EA community in London to grow, run and support the community. This is part 1 of a 4 part write up, broken down as follows:</span></p>\n<p><span>Part 1.</span>&nbsp;<span>Impact assessment</span></p>\n<p><a href=\"/ea/1fl/general_lessons_on_how_to_build_ea_communities/\"><span>Part 2. </span>&nbsp;<span>General lessons on how to build EA communities.</span></a></p>\n<p><span>Part 3.</span>&nbsp;<span>Specific lessons on running a large local community.</span></p>\n<p><a href=\"/ea/1g5/effective_altruism_london_strategic_plan_funding/\"><span>Part 4.</span>&nbsp;<span>Future plans and a request for funding</span></a></p>\n<p><span>[Links will be provided to future articles when they are written]</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h1>Summary</h1>\n<p><span>Background: </span><span>Sam was funded to work full time on the EA London community for a year.<br><br></span></p>\n<p><span>What was done:</span></p>\n<ul>\n<li>\n<p><span>Our theory of change was:</span>.</p>\n</li>\n<ul>\n<li>\n<p><strong><span>Raising awareness of EA -&gt; engagement in our community -&gt; positive behaviour changes</span></strong></p>\n</li>\n</ul>\n</ul>\n<ul>\n<li>\n<p><span>80% of project time went into events</span><span>, publicity and marketing. We ran 70 events with 900 unique attendees.</span></p>\n</li>\n<li>\n<p><span>20% of project time went into supporting other EA communities and projects in London, such as London's student groups, EA policy groups and one-on-one support.<br><br></span></p>\n</li>\n</ul>\n<p><span>What was the estimated impact:</span></p>\n<ul>\n<li>\n<p><span>Our events caused </span><span>12 GWWC pledges and 6 other large behaviour changes</span><span> and about 175 smaller but significant changes to behavior or beliefs.</span></p>\n</li>\n<ul>\n<li>\n<p><span>We believe we would have had 1/3 of this benefit if we were volunteer run.</span></p>\n</li>\n</ul>\n<li>\n<p><span>Our </span><span>cost per pledge or equivalent is $2000</span><span>. This is close to but not quite as effective as other EA meta-organisations (approximate average benefit of $1000 per pledge/etc).</span></p>\n</li>\n<li>\n<p><span>We have also had a </span><span>variety of other benefits</span><span> that are harder to put numbers on, such as the aforementioned sub-communities, projects created as a result of EAs networking, people retaining a engagement with EA ideas, and so on.<br><br></span></p>\n</li>\n</ul>\n<p><span>Conclusions:</span></p>\n<ul>\n<li>\n<p><span>We did not reach a strong conclusion on if it is sensible to fund full-time EA community organisers to run non-student EA groups in large cities.</span></p>\n</li>\n<li>\n<p><span>We do think there is a strong case for targeted community outreach (such as to civil servants or poker players or start-up founders).</span></p>\n</li>\n<li>\n<p><span>We think there is a reasonable case for funding EA London for another year.&nbsp;</span></p>\n</li>\n</ul>\n<h1>&nbsp;</h1>\n<h1><span><span><br></span></span>Contents</h1>\n<p><span><span><span><strong>- Background and initial plans</strong></span></span></span></p>\n<p><span><span><span><strong>- Breakdown of activities and costs</strong></span></span></span></p>\n<p><span><span><span><strong>- Impact \u2013 key data</strong></span></span></span></p>\n<p><span><span><span><strong>- Impact \u2013 Further analysis</strong></span></span></span></p>\n<p><span><span><span><strong>- Impact \u2013 Other benefits and stories of success</strong></span></span></span></p>\n<p><span><span><span><strong>- Update on June 2017 \u2013 Sept 2017</strong></span></span></span></p>\n<p><span><span><span><strong>- Conclusions</strong></span></span></span></p>\n<p><span><span><span><strong>- Annex A: Impact \u2013 elaboration &amp; data collection</strong></span></span></span></p>\n<p><span><span><span><strong>- Annex B: London\u2019s other EA communities</strong></span></span></span></p>\n<p><span><br>This document can be read as a Google Doc&nbsp;<a href=\"https://docs.google.com/document/d/1A9c4gfMe135zpyus1foEkgSkWSYqn7tsqjbtIumViGE/edit?ts=59cf85b1\">here</a></span><span><span>.<br>This is part 1 of 4. Other parts cover community building, lessons learned and future plans.&nbsp;</span></span><span>[Links pending] </span>&nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h1>Background and initial plans</h1>\n<p><span>This impact assessment covers the activities done by Effective Altruism London from 3 June 2016 until 7 June 2017. <br></span></p>\n<p><span>HISTORY</span></p>\n<p><span>In April 2013, the first Effective Altruism London social event was held. The community grew slowly but steadily, drawing from outreach efforts in London and growth in the wider EA community. In June 2016 the community began funding Sam Hilton to act as a full time movement builder for a year.<br><br></span></p>\n<h2><span>THE VISION</span></h2>\n<p><span>Our aspirational vision was written collectively by the community and has helped guide actions. It is:</span></p>\n<p><span>Everyone in London working effectively towards a better world<br><br></span></p>\n<h2><span>MODEL / THEORY OF CHANGE</span></h2>\n<p><span>We had a 3 stage model of how we create impact. Awareness of the community leads to people engaging which leads to people changing their behaviour to be more altruistic and effective:</span></p>\n<div>\n<table>\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>\n<p><span>Awareness (of EAL)</span></p>\n</td>\n<td>\n<p><span>\u2192</span></p>\n</td>\n<td>\n<p><span>Engagement</span></p>\n</td>\n<td>\n<p><span>\u2192</span></p>\n</td>\n<td>\n<p><span>Changing behaviour</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Happens through:</span></p>\n</td>\n<td>\n<p><span>Meetup.com, word of mouth, wider EA</span></p>\n</td>\n<td>&nbsp;</td>\n<td>\n<p><span>Event attendance, (some online activity)</span></p>\n</td>\n<td>&nbsp;</td>\n<td>\n<p><span>Event attendance,&nbsp;(some personal support)</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Measured by:</span></p>\n</td>\n<td>\n<p><span>Numbers on Meetup, Facebook, email, etc</span></p>\n</td>\n<td>&nbsp;</td>\n<td>\n<p><span>Event attendance, Facebook conversation</span></p>\n</td>\n<td>&nbsp;</td>\n<td>\n<p><span>Surveys on self-reported change, projects started</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><span><br>This model has remained largely unchanged for the year, although we also tried to recognise (but do not have good measures for) alternative paths to impact. Including:</span></p>\n<p><span>\u2022 Community benefits, such as networks and EA friendships leading to projects and actions taken.</span></p>\n<p><span>\u2022 Benefits of keeping existing EAs within the EA community<br><br></span></p>\n<h2><span>AIMS AND PLANS</span></h2>\n<p><span>The aims set for this year were:</span></p>\n<ol>\n<li>\n<p><span>Work out the best ways of building an engaged EA community in London</span></p>\n</li>\n<li>\n<p><span>Secondary aim: Support and inspire those in the EA London community to have a greater positive impact on the world.</span></p>\n</li>\n<li>\n<p><span>Tertiary aim: Grow the EA movement in London<br><br></span></p>\n</li>\n</ol>\n<p><span>Plans were made about how these aims would be achieved. &nbsp;For example, we planned to:</span></p>\n<ul>\n<li>\n<p><span>Experiment with online marketing</span></p>\n</li>\n<li>\n<p><span>Experiment with a variety of events (Workshops, Giving Circles, Discussions, etc)</span></p>\n</li>\n<li>\n<p><span>Support individuals with their EA projects.</span></p>\n</li>\n<li>\n<p><span>Measure and track impact<br><br></span></p>\n</li>\n</ul>\n<p><span>Details of the initial plans can be found at: </span><a href=\"/ea/t4/effective_altruism_london_a_request_for_funding/\"><span>February 2016 _ Request for funding</span></a></p>\n<p><span>Plans, adjusted following our mid-year progress review, are at: </span><a href=\"https://drive.google.com/file/d/0BwwvTgW1FiT_V2ZBNHdMZVhTSWs/view?usp=sharing\"><span>Jan 2017 _ Six month review<br><br></span></a></p>\n<p><span>We had a budget of just over \u00a330,000 to carry out this work<br><br></span></p>\n<h2><span>PREDICTIONS</span></h2>\n<p><span>Prior to the year beginning we predicted the following counterfactual impacts</span></p>\n<ul>\n<li>\n<p><span>Awareness: New contacts (email, meetup, etc) of 1,500-2,500 people</span></p>\n</li>\n<li>\n<p><span>Engagement: Unique new event attendees of: 400-800</span></p>\n</li>\n<li>\n<p><span>Behaviour change: New regular attendees: 50-250. New GWWC pledges: 20-55. New significant career changes: 10-30</span><span>\f</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h1>Breakdown of activities and costs<span><br>ACTIVITIES</span></h1>\n<p><span>The following chart shows the breakdown of key activities carried out in the past year.</span></p>\n<div>\n<table>\n<tbody>\n<tr>\n<td>\n<p><span>Time (apx)</span></p>\n</td>\n<td>\n<p><span>Activity</span></p>\n</td>\n<td>\n<p><span>Details</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>22%</span></p>\n</td>\n<td>\n<p><span>Events</span></p>\n</td>\n<td>\n<p><span>socials, talks, workshops</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>22%</span></p>\n</td>\n<td>\n<p><span>Awareness raising</span></p>\n</td>\n<td>\n<p><span>newsletter, advertising, website, auto-emails, speaking</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>16%</span></p>\n</td>\n<td>\n<p><span>Subgroups</span></p>\n</td>\n<td>\n<p><span>LSE, Imperial, UCL, policy x 2, finance, animals, others</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>12%</span></p>\n</td>\n<td>\n<p><span>Strategy</span></p>\n</td>\n<td>\n<p><span>Quarterly strategy reviews</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>7%</span></p>\n</td>\n<td>\n<p><span>Impact measurement</span></p>\n</td>\n<td>&nbsp;</td>\n</tr>\n<tr>\n<td>\n<p><span>7%</span></p>\n</td>\n<td>\n<p><span>Operations</span></p>\n</td>\n<td>\n<p><span>Charity, accounts, insurance, volunteer management</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>7%</span></p>\n</td>\n<td>\n<p><span>Collaboration</span></p>\n</td>\n<td>\n<p><span>Networking, writing up, talking to other EA orgs,</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>6%</span></p>\n</td>\n<td>\n<p><span>Experiments / campaigns</span></p>\n</td>\n<td>\n<p><span>Humanist experiment, buddies, pledge campaign </span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>1%</span></p>\n</td>\n<td>\n<p><span>Community</span></p>\n</td>\n<td>\n<p><span>One-on-one support, Facebook groups, interns</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><span><br>This is roughly as expected</span><span> from the initial plans, although perhaps with:</span></p>\n<p><span> \u2022 less time going into structured experiments</span></p>\n<p><span> \u2022 more time than expected spent on impact measurement </span></p>\n<p><span> \u2022 more time than expected spent on growing EA London\u2019s subgroups<br><br><br></span></p>\n<h2><span>COSTS</span></h2>\n<p><span>The following chart shows the breakdown of financial costs for the past year.</span></p>\n<div>\n<table>\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>\n<p><span>Money Spent</span></p>\n</td>\n<td>\n<p><span>Inc gifts in kind and lost wages</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>TOTAL</span></p>\n</td>\n<td>\n<p><span>\u00a319,510</span></p>\n</td>\n<td>\n<p><span>\u00a334,800</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Staff time</span></p>\n</td>\n<td>\n<p><span>\u00a317,860</span></p>\n</td>\n<td>\n<p><span>\u00a332,860</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Marketing</span></p>\n</td>\n<td>\n<p><span>\u00a3855</span></p>\n</td>\n<td>\n<p><span>\u00a3925</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Events</span></p>\n</td>\n<td>\n<p><span>\u00a3630</span></p>\n</td>\n<td>\n<p><span>\u00a3850</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Overheads</span></p>\n</td>\n<td>\n<p><span>\u00a3165</span></p>\n</td>\n<td>\n<p><span>\u00a3165</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><span><br>We spent \u00a319,510 from 1 June 2016 until 1 June 2017. Adding in estimates for gifts in kind (books from CEA, printing) this brings the cost to \u00a319,760. Sam took a low wage for the year. If accounting for the loss in salary of \u00a315,000 the cost becomes approximately \u00a335,000 (assumes 100% of additional salary earned would have been donated).<br><br></span></p>\n<p><span>Note: We have not accounted for time spent by volunteers. This is difficult to do as it is unclear:</span></p>\n<ul>\n<li>\n<p><span>how time would have been spent otherwise,</span></p>\n</li>\n<li>\n<p><span>how much they benefited in terms of career capital (overall a benefit is likely)</span></p>\n</li>\n<li>\n<p><span>what should be counted as volunteering as opposed to community engagement or an individual\u2019s personal EA outreach actions.</span></p>\n</li>\n</ul>\n<p><strong><br></strong><span>\f<br><br></span></p>\n<h1>Impact \u2013 key data</h1>\n<h2><span>HEADLINE FIGURES<br></span><span><br>The total impact of Effective Altruism London this year is estimated to be:<br></span></h2>\n<ul>\n<li>\n<p><span>18 people had large behaviour changes </span><span>including</span><span> 12 GWWC pledges</span><span>.<br><br></span></p>\n</li>\n</ul>\n<ul>\n<li>\n<p><span>174 people had some significant, but not large, changes to belief or behaviour</span></p>\n</li>\n</ul>\n<ul>\n<ul>\n<li>\n<p><span>Including 10 taking Giving What We Can\u2019s \u2018Try Giving\u2019 pledge</span></p>\n</li>\n<li>\n<p><span>Including 14 impact-adjusted career plan changes at 80,000 Hours workshops<br><br></span></p>\n</li>\n</ul>\n</ul>\n<ul>\n<li>\n<p><span>900 unique event attendees</span><span> over the year of which </span><span>240 attended multiple events</span></p>\n</li>\n<li>\n<p><span>3000 new unique individuals</span><span> contactable on email list, Meetup and Facebook<br><br></span></p>\n</li>\n</ul>\n<ul>\n<li>\n<p><span>7 new EA subgroups supported </span><span>(LSE, UCL, animal welfare, finance, 2x policy, quakers)<br><br></span></p>\n</li>\n</ul>\n<ul>\n<li>\n<p><span>Additional reported but unquantified </span><span>benefits for community members, in particular:</span></p>\n</li>\n<ul>\n<li>\n<p><span>Networking effects.</span><span> Eg. meeting a contact who can help with work / a decision.</span></p>\n</li>\n<li>\n<p><span>EA Retention.</span><span> Helping people retain an EA mind-set after university.<br><br></span></p>\n</li>\n</ul>\n</ul>\n<p><span>The above figures are calculated estimates rather than numbers known with certainty. We explain how they were calculated below in </span><span>Annex A: Impact \u2013 elaboration &amp; data collection.<br><br><br></span></p>\n<h2><span>IMPACT OF PROVIDING FUNDING<br></span><span><br></span></h2>\n<p><span><span><span><strong>We estimate that if not funded we would have had 33% of the impact </strong></span></span></span><span><span><span>on behaviour and attendance, 25% of the impact on contactable individuals, and the finance subgroup would have been started. This is based on the amount of and size of events run in by volunteers in previous years.</span></span></span><span><span><span><br><br></span></span></span><span><span>This means that the impact on behaviour of funding EA London, rather than letting it be run by volunteers, is about 2/3 of the impact set out above, ie. </span><span><span><strong>12 large behaviour changes of which 8 are GWWC pledges. <br><br></strong></span><span>Additionally not being volunteer run means a reduced risk of community collapse, and the additional benefits created, as set out in the section: </span><span>Impact \u2013 Other benefits and stories of success.</span><br></span></span></p>\n<p><span><br></span><strong><br><br></strong></p>\n<h2><span>HOW THIS COMPARES TO OTHER EA ORGS<br></span></h2>\n<h2><span><br>Our measured impact on behaviour changes suggests we are in the s<strong>ame order of magnitude</strong> as other meta EA organisations but <strong>somewhat less effective</strong>.</span></h2>\n<h2><span>The returns to funding other EA orgs is in the ballpark of $1000 per pledge or equivalent (am uncertain estimate based on talking to people who work in EA organisations). We are getting a pledge level change in behaviour for about $2000 of funding (depends on how measured, see Cost benefit analysis section in Appendix A below)</span><span>.</span></h2>\n<p><strong>&nbsp;</strong></p>\n<h2>&nbsp;</h2>\n<h1>Impact \u2013 further analysis</h1>\n<h2><span>GROWTH<br></span><span><br>Growth rates are unclear but generally positive. Noise is added as the kinds of events being run has changed throughout the year as we have experimented with different things.</span></h2>\n<ul>\n<li>\n<p><span>Monthly attendance numbers have grown by 60%.</span><span> We have moved from about 90 monthly attendees (average Jun-Sep 2016) to about 145 monthly attendees (average Mar-Jun 2017).</span></p>\n</li>\n<ul>\n<li>\n<p><span>If we look at data beyond the end of the year (</span><span>in orange</span><span>) monthly attendance has risen to 190 attendees (Average June-Sept 2017, staff time still being invested).</span></p>\n</li>\n</ul>\n<li>\n<p><span>Attendance at the regular monthly social has risen about 100%</span><span>. These have gone from about 25 people to about 50 people.</span></p>\n</li>\n</ul>\n<p><img src=\"http://res.cloudinary.com/cea/image/upload/v1667995110/mirroredImages/HHeaoX47aAa8dgt7a/inwnedlz4tmafkpczkez.png\">&nbsp;&nbsp;<img src=\"http://res.cloudinary.com/cea/image/upload/v1667995110/mirroredImages/HHeaoX47aAa8dgt7a/n0okf4eaoxxdptg1bwit.png\"></p>\n<p><span>On the other hand, </span><span>we did not find additional evidence that we are changing behaviour or getting more new people engaged at an increasing rate</span><span>. This maybe a sign that we are not successfully engaging more new attendees despite larger events, but might also be because the data set is noisy, too small and has missing data.<br></span></p>\n<p>&nbsp;</p>\n<p><span>HOW OUR ACTIONS CREATED IMPACT</span></p>\n<p><span>Roughly</span><span> 80% of project time went into marketing and running events, which led to the behaviour changes we saw (18 large and 174 significant behaviour changes)</span><span>. It is difficult to know exactly which actions lead to which changes as people often do not change their behaviour straight away. <br><br></span></p>\n<p><span>We can see from the event reports that </span><span>social events</span><span> led to the most new attendees (people for whom this was their very first event) returning (attending another event with 50 days):</span></p>\n<div>\n<table>\n<tbody>\n<tr>\n<td rowspan=\"2\">\n<p><span>&nbsp;</span></p>\n</td>\n<td colspan=\"4\">\n<p><span>Number of attendees at events by type</span></p>\n</td>\n<td rowspan=\"2\">\n<p><span>Total number of each type of event</span></p>\n</td>\n<td rowspan=\"2\">\n<p><span>Average returning new attendees</span></p>\n</td>\n</tr>\n<tr>\n<td colspan=\"2\">\n<p><span>Total </span><span>(not unique) </span><span>event</span> <span>attendees</span></p>\n</td>\n<td>\n<p><span>New attendees</span></p>\n</td>\n<td>\n<p><span>Returning new attendees</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Career networking</span></p>\n</td>\n<td colspan=\"2\">\n<p><span>199</span></p>\n</td>\n<td>\n<p><span>106</span></p>\n</td>\n<td>\n<p><span>8</span></p>\n</td>\n<td>\n<p><span>10</span></p>\n</td>\n<td>\n<p><span>0.8</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Social</span></p>\n</td>\n<td colspan=\"2\">\n<p><span>594</span></p>\n</td>\n<td>\n<p><span>263</span></p>\n</td>\n<td>\n<p><span>48</span></p>\n</td>\n<td>\n<p><span>23</span></p>\n</td>\n<td>\n<p><span>2.1</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Talk</span></p>\n</td>\n<td colspan=\"2\">\n<p><span>417</span></p>\n</td>\n<td>\n<p><span>221</span></p>\n</td>\n<td>\n<p><span>15</span></p>\n</td>\n<td>\n<p><span>16</span></p>\n</td>\n<td>\n<p><span>0.9</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Workshop</span></p>\n</td>\n<td colspan=\"2\">\n<p><span>240</span></p>\n</td>\n<td>\n<p><span>156</span></p>\n</td>\n<td>\n<p><span>19</span></p>\n</td>\n<td>\n<p><span>14</span></p>\n</td>\n<td>\n<p><span>1.4</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Other</span></p>\n</td>\n<td colspan=\"2\">\n<p><span>130</span></p>\n</td>\n<td>\n<p><span>23</span></p>\n</td>\n<td>\n<p><span>11</span></p>\n</td>\n<td>\n<p><span>11</span></p>\n</td>\n<td>\n<p><span>1.0</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>TOTAL</span></p>\n</td>\n<td colspan=\"2\">\n<p><span>1580</span></p>\n</td>\n<td>\n<p><span>769</span></p>\n</td>\n<td>\n<p><span>101</span></p>\n</td>\n<td>\n<p><span>74</span></p>\n</td>\n<td>\n<p><span>1.4</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><span>NOTES:</span><span> \u2022 Not adjusted to account for imperfect attendance data, total numbers returning is likely to be higher.</span><span><br></span><span>\u2022 Dates covered in the above table: </span><span>01 Aug 16 to 31 Jul 17</span><span>. &nbsp;&nbsp;\u2022 Returning means attended another event </span><span>within 50 days</span><span>.</span></p>\n<p><span>More detail on exactly which events, campaigns and actions were more impactful than others is covered in part 3 of the write-up on </span><span>Specific lessons on running a large local community</span><span> [link pending]</span><span>.</span></p>\n<p><span>Roughly </span><span>20% of the time went into supporting the sub groups, supporting individuals</span><span> with their projects, and so forth. We believe </span><span>this had other benefits</span><span>, such as the existence of student EA groups at London universities. (Events at student groups and any resultant behaviour changes are not captured by our metrics, eg. we know of and have not counted 3 pledges at student groups). </span></p>\n<p><strong><br></strong><span>WHY COUNTERFACTUAL IMPACT OF FUNDING WAS LOWER THAN PREDICTIONS</span></p>\n<p><span>How our predictions match our results:</span></p>\n<div>\n<table>\n<tbody>\n<tr>\n<td>&nbsp;</td>\n<td>\n<p><span>Metric</span></p>\n</td>\n<td>\n<p><span>Prediction</span></p>\n</td>\n<td>\n<p><span>Result*</span></p>\n</td>\n<td>\n<p><span>Comparison to prediction</span></p>\n</td>\n<td>\n<p><span>Within range predicted</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Awareness</span></p>\n</td>\n<td>\n<p><span>Contactable individuals</span></p>\n</td>\n<td>\n<p><span>1,500-2,500</span></p>\n</td>\n<td>\n<p><span>2250</span></p>\n</td>\n<td>\n<p><span>Better</span></p>\n</td>\n<td>\n<p><span>Yes</span></p>\n</td>\n</tr>\n<tr>\n<td rowspan=\"2\">\n<p><span>Engagement</span></p>\n</td>\n<td>\n<p><span>Unique attendees</span></p>\n</td>\n<td>\n<p><span>400-800</span></p>\n</td>\n<td>\n<p><span>600</span></p>\n</td>\n<td>\n<p><span>Same</span></p>\n</td>\n<td>\n<p><span>Yes, mid range</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Attending multiple events</span></p>\n</td>\n<td>\n<p><span>50-250</span></p>\n</td>\n<td>\n<p><span>160</span></p>\n</td>\n<td>\n<p><span>Same</span></p>\n</td>\n<td>\n<p><span>Yes, mid range</span></p>\n</td>\n</tr>\n<tr>\n<td rowspan=\"3\">\n<p><span>Behaviour</span></p>\n<p><span>Change</span></p>\n</td>\n<td>\n<p><span>Made a significant change</span></p>\n</td>\n<td>\n<p><span>Not predicted</span></p>\n</td>\n<td>\n<p><span>174</span></p>\n</td>\n<td>\n<p><span>-</span></p>\n</td>\n<td>\n<p><span>-</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>Made a huge change</span></p>\n</td>\n<td>\n<p><span>30-85</span></p>\n</td>\n<td>\n<p><span>12</span></p>\n</td>\n<td>\n<p><span>Lower</span></p>\n</td>\n<td>\n<p><span>No</span></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>GWWC pledge</span></p>\n</td>\n<td>\n<p><span>20-55</span></p>\n</td>\n<td>\n<p><span>8</span></p>\n</td>\n<td>\n<p><span>Lower</span></p>\n</td>\n<td>\n<p><span>No</span></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><span>* Predictions made comparing to if volunteer run. Results are are the marginal impact, so 33% lower than headline figures..</span></p>\n<p><span>We </span><span>overpredicted the expected behaviour change</span><span>. We have a good understanding of why this happened. The main problem was that we based predictions on</span><span> self-reported behaviour change, but this did not match up to actual behaviour changes</span><span>. For example, people said on surveys that attending EA London events has already had a drastic (8 out of 8) effect on their behaviour but upon questioning failed to give clear evidence of such changes.</span></p>\n<p><span>Also, </span><span>our behaviour change metric does not capture future behaviour changes.</span><span> For example, changing a career plan (or donation plan, etc) would not be captured by our metric, unless we believe the person could give evidence that they had already changed their career (or donations etc). Anecdotal evidence does however suggest that people who are attending regular events and report an intention to change their behaviour may well actually do so.<br><br></span></p>\n<p><span>Predictions of attendance and repeat attendance very closely matched results.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h1>Impact \u2013 Other benefits and stories of success</h1>\n<p><span>As well as introducing new people to EA we have had significant other successes, as illustrated by the anecdotes below:<br><br></span></p>\n<p><span>Founding and supporting sub-communities. </span><span>Some examples:</span></p>\n<ul>\n<li>\n<p><span>Starting EA student groups at LSE and UCL</span><span>. The leaders of both these student groups decided to found them as a direct result of being involved in EA London. </span></p>\n</li>\n<li>\n<p><span>Supporting student groups. </span><span>Also supported groups at SOAS, Imperial and Queen Mary. Grew students\u2019 email lists by about 2000 emails. Arranged career workshops for 250+ students (with over 20 IA-SPC). &nbsp;NOTE: Student group attendees are excluded from our metrics.</span></p>\n</li>\n<li>\n<p><span>Founding HIPE</span><span> (High Impact Policy Engine) a highly promising initiative to support civil servants who want to have a positive impact with their careers.<br><br></span></p>\n</li>\n</ul>\n<p><span>Networking effects.</span><span> Attendees have commented that they have benefited from the networking effects of the EA London community. Some examples:</span></p>\n<ul>\n<li>\n<p><span>One of our past attendees used contacts she met at EA London events to help her set up an All Party Parliamentary Group (APPG) on Future Generations.</span></p>\n</li>\n<li>\n<p><span>We connected an EA climate scientist to a wealthy entrepreneur looking for his next project.<br><br></span></p>\n</li>\n</ul>\n<p><span>Retention effects. </span><span>Attendees have commented that the EA London community helps make sure they continue to engage with EA ideas. However, this effect is hard to measure and we have no good evidence or stories of this. We have seen a few people attending events who had been involved in EA at university, not done much since and EA London gets them re-engaging with EA ideas. <br><br></span></p>\n<p><span>Paved the way for future behaviour change. </span><span>We have seen people who got involved in EA in part through EA London, prior to this year, play a bigger role in the EA community including:</span></p>\n<ul>\n<li>\n<p><span>Michael Plant (likely to have come across EA anyway) is working on measuring happiness, drug policy reform, mental health issues and regularly engaging with the EA community.</span></p>\n</li>\n<li>\n<p><span>David Nash, now doing EA community building work for EA London</span></p>\n</li>\n<li>\n<p><span>Sanjay Joshi, founded SoGive - a social enterprise to give comprehensive charity evaluation data.</span></p>\n</li>\n</ul>\n<p><span>These are not benefits created this year but are evidence that impressive actions and significant life changes may take longer than a year to manifest. Already we have seen:</span></p>\n<ul>\n<li>\n<p><span>A senior communications official connecting HNWs to CEA</span></p>\n</li>\n<li>\n<p><span>A significant number of EAs begin working at Founders Pledge in London, some following EA London career advice.</span></p>\n</li>\n<li>\n<p><span>Someone who got involved through the EA London community begin work at CEA<br><br></span></p>\n</li>\n</ul>\n<p><span>Paved the way for future growth of the London community. </span><span>We should grow more quickly in the future as we have gained:</span></p>\n<ul>\n<li>\n<p><span>Size</span><span>. As evidenced above, more people are engaging with us each month.</span></p>\n</li>\n<li>\n<p><span>Infrastructure</span><span> We are a registered charity and now have a regular email newsletter. The student groups and subgroups are providing excellent outreach opportunities.</span></p>\n</li>\n<li>\n<p><span>Lessons learnt. </span><span>As we set out in parts 2 and 3</span><span> [links pending], </span><span>we have learned a significant amount about community building</span></p>\n</li>\n<li>\n<p><span>Contacts and relationships </span><span>with other philanthropy groups across London.<br><br></span></p>\n</li>\n</ul>\n<p><span>Downsides.</span><span> It is likely some of the people who have put time and effort to advising and supporting EA London may have had significant benefit elsewhere.</span></p>\n<p>&nbsp;</p>\n<p><strong><br><br></strong></p>\n<h1>Update on June \u2013 Sept</h1>\n<p><span>In the last 3 months we have continued to put time into EA London and to grow (captured by the </span><span>orange</span> <span>bars in the charts in the section on growth). We have also been training new staff. Sam took a step back (and will shortly be moving into a Government role) and Holly Morgan and David Nash took over the EA London work. In these 4 months we have seen:</span></p>\n<ul>\n<li>\n<p><span>The EA London Retreat 33 people attended of which 9 changed their mind on the top cause and 5 changed their career plans</span></p>\n</li>\n<li>\n<p><span>Our month with the highest total event attendance in a while (230) including our largest ever monthly social (66 people).</span></p>\n</li>\n<li>\n<p><span>500 new people signed up to the email list at Just V show and about 3000 more have signed up to student groups at London freshers\u2019 fairs (including in early October).</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<h1><strong><br></strong>Conclusions</h1>\n<p><span>Overall the conclusions from this year are rather weak. As set out above we believe:</span></p>\n<ul>\n<li>\n<p><span>EA London has been in the same magnitude as effectiveness as other EA movement building organisations, although </span><span>our measured impact seems low</span><span>er.</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p><span>SHOULD EAS CONTINUE TO FUND THE LONDON COMMUNITY</span></p>\n<p><span><span>&nbsp;</span>As a result of our limited success this year we intend to change our plans. We hope that with slightly different plans and more focus we can have an even greater impact in future. <br><br></span></p>\n<p><span>Against funding:</span></p>\n<ul>\n<li>\n<p><span>If Effective Altruism London is less effective than other organisations why fund it?</span></p>\n</li>\n<li>\n<p><span>It is hard to imagine that a local organisation has the same potential to grow as organisations doing mass outreach.</span></p>\n</li>\n<li>\n<p><span>It is possible that the people we would fund (especially Holly Morgan) could be doing more impactful work with their time.</span></p>\n</li>\n</ul>\n<p><span>For funding:</span></p>\n<ul>\n<li>\n<p><span>A year is a short space of time in the philanthropy world. It is not obvious that it makes sense to pull the plug on a new organisation within a year if it is not as effective as more established organisations. </span></p>\n</li>\n<li>\n<p><span>Updated plans could mean more impact in future.<br><br></span></p>\n</li>\n</ul>\n<p><span>Overall, we think it depends on what the other opportunities for funding meta EA work such as movement building work are. We believe such opportunities are limited and that CEA are not considerably funding-constrained. We have, collectively as a community, taken the decision to develop future plans and seek funding for another year. <br><br></span></p>\n<p><span>See part 4 for our future plans </span><span>[link pending]</span><span>.</span></p>\n<p><strong>&nbsp;</strong></p>\n<p><span>SHOULD EAS FUND FULL TIME LOCAL COMMUNITY BUILDERS IN MORE CITIES</span></p>\n<p><span>We believe the larger EA organisations providing small amounts of funding and support to local communities is useful. That said, this year has not made a strong case either for or against funding full time local community organisers. </span><span>Overall, we have no strong recommendation either way </span><span>and it may depend on what other opportunities are available.<br><br></span></p>\n<p><span>We do however think there is value in funding outreach to innovative community groups with specific targets, </span><span>such as HIPE which is reaching UK civil servants, REG which reached top poker players or Founders Pledge which is reaching founders. This is defended by our ideas of how communities grow (see part 2 of write up on: </span><span>General lessons on how to build EA communities</span><span>) and analysis of our subgroups (see section below: </span><span>Annex B: Subgroups</span><span>). We will look to do more of this in London next year.<br><br></span></p>\n<p><span>If people in the EA movement do fund local city organisers they should be prepared for the fact that compared to other movement building actions that involve mass marketing, growing a local community will produce less rigorous quantitative data but will provide a greater insight into the individuals impacted.</span>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><span>RECOMMENDATIONS FOR CEA / LEAN&nbsp; </span></p>\n<p><span>A few things we think that CEA and/or LEAN should be considering in their work to build the EA movement:</span></p>\n<ul>\n<li>\n<p><span>Funding. Continue to provide small levels of funding to local groups. Especially for things like copies of books or occasional events.</span></p>\n</li>\n<li>\n<p><span>Retention research. Build understanding on how people stay or leave the EA community over time and the impact of this would be useful. </span></p>\n</li>\n<li>\n<p><span>Joining up of groups in different locations. For example, connecting students to groups in cities following graduation.</span></p>\n</li>\n<li>\n<p><span>More high-quality support with the administration work for running a local group. A lot of the admin work done feels easily replicable. Building on the fantastic work already done, more support could be centrally provided, including support: with handling data, measuring impact, emails, newsletters, websites, support engaging with HNWs, etc.</span></p>\n</li>\n</ul>\n<p><span><br>\f<br><br></span></p>\n<h1>Annex A: Impact \u2013 elaboration &amp; data collection</h1>\n<p>You can read this <a href=\"https://docs.google.com/document/d/1A9c4gfMe135zpyus1foEkgSkWSYqn7tsqjbtIumViGE/edit?ts=59cf85b1#heading=h.9ind8nlxl728\">here</a>.</p>\n<p><br><br></p>\n<p>&nbsp;</p>\n<h1>Annex B: Subgroups</h1>\n<p><strong>You can read this <a href=\"https://docs.google.com/document/d/1A9c4gfMe135zpyus1foEkgSkWSYqn7tsqjbtIumViGE/edit?ts=59cf85b1#heading=h.ku4v1fdytws6\">here</a>.<br></strong></p>\n<p>&nbsp;</p>", "user": {"username": "weeatquince"}}, {"_id": "4kRPYuogoSKnHNBhY", "title": "An intervention to shape policy dialogue, communication, and AI research norms for AI safety", "postedAt": "2017-10-01T18:29:15.685Z", "htmlBody": "<html><body><p>A PDF version of this article can be found <a href=\"https://drive.google.com/open?id=0B_diIvF4w7F_TG9DM3R1ckYxdDg\">here</a>.<sup>1</sup></p>\n<h1>Abstract</h1>\n<p>Discourse on AI safety suffers from heated disagreement between those sceptical and those concerned about existential risk from AI. Framing discussion using strategic choice of language is a subtle but potentially powerful method to shape the direction of AI policy and AI research communities. It is argued here that the AI safety community is committing the dual error of frequently using language that hinders constructive dialogue and missing the opportunity to frame discussion using language that assists their aims. It is suggested that the community amend usage of the term &#x2018;AI risk&#x2019; and employ more widely the &#x2018;AI accidents&#x2019; frame in order to improve external communication, AI policy discussion, and AI research norms.&#xA0;</p>\n<h1>Contents</h1>\n<ul>\n<li>Abstract</li>\n<li>The state of public discourse on AI safety</li>\n<li>Why to care about terminology</li>\n<li>Introducing &#x2018;AI accidents&#x2019; and why use of &#x2018;AI risk&#x2019; can be inaccurate</li>\n<li>Why use of &#x2018;AI risk&#x2019; is problematic and why use of &#x2018;AI accidents&#x2019; is helpful</li>\n<ul>\n<li>From the perspective of sceptics</li>\n<li>From the perspective of the newcomer to the subject</li>\n<li>Shaping policy discussion and research norms</li>\n</ul>\n<li>Seizing the opportunity</li>\n<li>Footnotes</li>\n<li>Works Cited</li>\n</ul>\n<h1><a></a><span>The state of public discourse on AI safety</span></h1>\n<p>Contemporary public discourse on AI safety is often tense. Two technology billionaires have engaged in a regrettable <a href=\"http://fortune.com/2017/07/25/elon-musk-just-dissed-mark-zuckerbergs-understanding-of-artificial-intelligence/\"><span>public spat</span></a> over existential risks from artificial intelligence <!-- [if supportFields]><span\nstyle='mso-element:field-begin'></span><span\nstyle='mso-spacerun:yes'>\u00a0</span>CITATION Kat17 \\l 2057 <span\nstyle='mso-element:field-separator'></span><![endif]-->(Samuelson, 2017)<!-- [if supportFields]><span style='mso-element:\nfield-end'></span><![endif]-->; high profile AI experts have volleyed loud <a href=\"https://www.technologyreview.com/s/602410/no-the-experts-dont-think-superintelligent-ai-is-a-threat-to-humanity/\"><span>opinion</span></a> <a href=\"https://www.technologyreview.com/s/602776/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/\"><span>pieces</span></a> &#xA0;making contradictory calls for concern or for calm <!-- [if supportFields]><span\nstyle='mso-element:field-begin'></span><span\nstyle='mso-spacerun:yes'>\u00a0</span>CITATION Daf16 \\l 2057 <span\nstyle='mso-element:field-separator'></span><![endif]-->(Dafoe &amp; Russell, 2016)<!-- [if supportFields]><span\nstyle='mso-element:field-end'></span><![endif]--> <!-- [if supportFields]><span style='mso-element:field-begin'></span><span\nstyle='mso-spacerun:yes'>\u00a0</span>CITATION Ore16 \\l 2057 <span\nstyle='mso-element:field-separator'></span><![endif]-->(Etzioni, 2016)<!-- [if supportFields]><span style='mso-element:\nfield-end'></span><![endif]-->; both factions (the group sceptical of existential risk posed by AI and the group concerned about the risk) grow larger as interest in AI increases, and more voices join the debate. The divide shows little sign of narrowing. If surviving machine superintelligence will require strong coordination or even consensus, humanity&#x2019;s prospects currently look poor.</p>\n<p>In this polarised debate, both factions, especially the AI safety community, should look to ways to facilitate constructive policy dialogue and shape safety-conscious AI research norms. Though it insufficient on its own, framing discussion using strategic choice of language is a subtle but potentially powerful method to help accomplish these goals<!-- [if supportFields]><span style='mso-element:field-begin'></span>CITATION\nSet16 \\l 2057 <span style='mso-element:field-separator'></span><![endif]-->&#xA0;(Baum, 2016)<!-- [if supportFields]><span\nstyle='mso-element:field-end'></span><![endif]-->.&#xA0;</p>\n<h1><a></a><span>Why to care about terminology</span></h1>\n<p>Language choice frames policy debate, assigns the focus of discussion, and thereby influences outcomes. It decides whether the conversation is &#x201C;Gun control&#x201D; (liberty reducing) or &#x201C;Gun violence prevention&#x201D; (security promoting); &#x201C;Red tape&#x201D; or &#x201C;Safety regulations&#x201D;; &#x201C;Military spending&#x201D; or &#x201C;Defence spending&#x201D;. If terminology does not serve discussion well, it should be promptly rectified while the language, the concepts it signifies, and the actions, plans, and institutions guided by those concepts are still relatively plastic. With that in mind, the below advocates that the AI safety community revise its use of the term <strong>&#x2018;AI risk&#x2019; </strong>and employ the <strong>&#x2018;AI accidents&#x2019; frame</strong> more widely.</p>\n<p>It will help first to introduce what is argued to be the substantially better term, &#x2018;AI accidents&#x2019;. The inaccuracy of current language will then be explored, followed by discussion of the problems caused by this inaccuracy and the important opportunities missed by only rarely using the &#x2018;AI accidents&#x2019; frame.</p>\n<h1><a></a><span>Introducing &#x2018;AI accidents&#x2019; and why use of &#x2018;AI risk&#x2019; can be inaccurate</span></h1>\n<p>An AI accident is &#x201C;unintended and harmful behavior that may emerge from poor design of real-world AI systems&#x201D; <!-- [if supportFields]><span\nstyle='mso-element:field-begin'></span><span\nstyle='mso-spacerun:yes'>\u00a0</span>CITATION Amo16 \\l 2057 <span\nstyle='mso-element:field-separator'></span><![endif]-->(Amodei, et al., 2016)<!-- [if supportFields]><span\nstyle='mso-element:field-end'></span><![endif]-->. The earliest description of misaligned AI as an &#x2018;accident&#x2019; appears to be in Marvin Minsky&#x2019;s 1984 afterword to Vernor Vinge&apos;s novel, <em>True Names</em>:</p>\n<blockquote>\n<p><em>&#x201C;The first risk is that it is always dangerous to try to relieve ourselves of the responsibility of understanding exactly how our wishes will be realized. Whenever we leave the choice of means to any servants we may choose then the greater the range of possible methods we leave to those servants, the more we expose ourselves to accidents and incidents. When we delegate those responsibilities, then we may not realize, before it is too late to turn back, that our goals have been misinterpreted, perhaps even maliciously. We see this in such classic tales of fate as Faust, the Sorcerer&apos;s Apprentice, or the Monkey&apos;s Paw by W.W. Jacobs.&#x201D;</em><!-- [if supportFields]><span\nstyle='mso-element:field-begin'></span> CITATION Min84 \\l 2057 <span\nstyle='mso-element:field-separator'></span><![endif]-->&#xA0;(Minsky, 1984)<!-- [if supportFields]><span\nstyle='mso-element:field-end'></span><![endif]--></p>\n</blockquote>\n<p>The term &#x2018;AI accident&#x2019; seems to emerge publicly later, with Huw Price&#x2019;s 2012 quotation of Jaan Tallin:</p>\n<blockquote>\n<p><em>&#x201C;He (Tallinn) said that in his pessimistic moments he felt he was more likely to die from an AI accident than from cancer or heart disease,&#x201D;</em><!-- [if supportFields]><span style='mso-element:\nfield-begin'></span> CITATION Uni12 \\l 2057 <span style='mso-element:field-separator'></span><![endif]-->&#xA0;(University of Cambridge, 2012)<!-- [if supportFields]><span style='mso-element:field-end'></span><![endif]--><em>.</em></p>\n</blockquote>\n<p>There is some evidence that the term was used in the AI safety community prior to this<!-- [if supportFields]><span\nstyle='mso-element:field-begin'></span>CITATION Les10 \\l 2057 <span\nstyle='mso-element:field-separator'></span><![endif]-->&#xA0;(LessWrong commenter &quot;Snarles&quot;, 2010)<!-- [if supportFields]><span\nstyle='mso-element:field-end'></span><![endif]-->, but other written evidence proved elusive through online search.</p>\n<p>The first definition of &#x2018;accidents in machine learning systems&#x2019; appears to be provided in the well-known paper Concrete Problems in AI Safety (Amodei, et al., 2016). This is the definition for &#x2018;AI accident&#x2019; given above and used here throughout.&#xA0;</p>\n<p>Some examples of AI accidents may be illustrative: A self-driving car crash where the algorithm was at fault would be an AI accident; a housekeeping robot cooking the cat for dinner because it was commanded to &#x201C;Cook something for dinner&#x201D; would be an AI accident; using algorithms in the justice system that have inadvertently been trained to be racist would be an AI accident; the 2010 Flash Crash or similar future incidents would be an AI accident; deployment of a paperclip maximiser would be an AI accident. There is no presupposed upper bound for the size of AI accidents. AI safety seeks to reduce the risk of AI accidents.</p>\n<p>&#xA0;</p>\n<p><img src=\"https://drive.google.com/uc?id=0B_diIvF4w7F_WTM5eUJ6bTM1d3c\" alt=\"AI accidents\">&#xA0;</p>\n<p><span>Figure: AI accidents. The relative placement of instances of AI accidents may be subject to debate; the figure is intended for illustration only.</span>&#xA0;</p>\n<p>At significant risk of pedantry, close examination of terminology is worthwhile because, despite the appearance of hair-splitting, it yields what will emerge to be useful distinctions.</p>\n<p>&#x2018;AI risk&#x2019; has at least three uses.</p>\n<ol>\n<li><strong>&#x2018;An AI risk&#x2019; - The &#x2018;count noun&#x2019; sense,</strong><span> meaning a member of the set of all risks from AI, &#x2018;AI risks&#x2019;, which can be used interchangeably with &#x2018;dangers from AI&#x2019;, &#x2018;potential harms of AI&#x2019;, &#x2018;threats from AI&#x2019;, etc. Members of the set of AI risks include:</span></li>\n<ol>\n<li>AI accidents</li>\n<li>Deliberate misuse of AI systems (e.g. autonomy in weapons systems)</li>\n<li>Risks to society deriving from intended use of AI systems, which may result from coordination failures in the deployment of AI (e.g. mass unemployment resulting from automation).</li>\n</ol>\n<li><strong>&#x2018;AI risk&#x2019; &#x2013; The &#x2018;mass noun&#x2019; sense</strong>, meaning some amount of risk from AI. In practice, this means to discuss at least one member of the above set of risks, but the source of risk is not implied. &#xA0;It can be used interchangeably with &#x2018;danger from AI&#x2019;, &#x2018;potential harm of AI&#x2019;, &#x2018;AI threat&#x2019;, etc.</li>\n<li>The third, inaccurate sense is employing &#x2018;AI risk&#x2019; to mean specifically <strong>&#x2018;Risk of a catastrophic AI accident&#x2019;</strong>.</li>\n</ol>\n<p>Observe that in the third usage, the label used for the second (mass noun) sense is used to refer to an instance of the first (count noun) sense. It would be easy to overlook this small discrepancy of &#x2018;crossed labels&#x2019;. Nevertheless, below it is argued that using the third sense causes problems and missed opportunities.</p>\n<p>Before exploring why use of the third sense might cause problems, note that it has been employed frequently by many of the major institutions in the AI safety community (although the accurate senses are used even more commonly)<sup>2</sup>:</p>\n<ul>\n<li><span>Cambridge Centre for the Study of Existential Risk (CSER) examples </span><a href=\"http://cser.org/stuart-russell-argues-for-a-new-approach-to-ai-risk/\">1</a><span>, </span><span><a href=\"https://opinionator.blogs.nytimes.com/2013/01/27/cambridge-cabs-and-copenhagen-my-route-to-existential-risk/?mcubz=0\">2</a><sup>3</sup></span><span>;</span></li>\n<li><span>Future of Humanity Institute (FHI) examples </span><a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/Policy-Desiderata-in-the-Development-of-Machine-Superintelligence.pdf\">1</a><span>, </span><a href=\"https://www.fhi.ox.ac.uk/edge-article/\">2</a><span>;</span></li>\n<li><span>Future of Life Institute (FLI) example </span><a href=\"https://futureoflife.org/background/aimyths/\">1</a><span>;</span></li>\n<li><span>Foundational Research Institute example </span><a href=\"https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering?gclid=EAIaIQobChMI26X07Myv1gIV7rXtCh05hQebEAAYASAAEgJxDvD_BwE#Caring_about_the_AIs_goals\">1</a><span>;</span></li>\n<li><span>Global Challenges Report 2017 example </span><a href=\"http://comcapint.com/wp-content/uploads/2017/05/Global-Catastrophic-Risks-2017.pdf\">1</a><span>;</span></li>\n<li><span>Machine Intelligence Research Institute (MIRI) examples </span><a href=\"https://intelligence.org/why-ai-safety/\">1</a><span>, </span><a href=\"https://intelligence.org/2017/02/28/using-machine-learning/\">2</a><span>;</span></li>\n<li><span>Open Philanthropy example </span><a href=\"http://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/openai-general-support\">1</a><span>;</span></li>\n<li><span>Wikipedia entries: </span><a href=\"https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence\">1</a><span>, </span><a href=\"https://en.wikipedia.org/wiki/Mind_uploading#Emulation_timelines_and_AI_risk\">2</a><span>, </span><a href=\"https://en.wikipedia.org/wiki/Future_of_Humanity_Institute\">3</a><span>;</span></li>\n<li><span>And others in popular blogs, community blogs, or media: </span><a href=\"http://slatestarcodex.com/2015/05/22/ai-researchers-on-ai-risk/\">Slatestar codex</a><span>, </span><a href=\"https://jack-clark.net/\">Import AI</a><span>, </span><a href=\"http://lesswrong.com/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">LessWrong</a><span>, </span><a href=\"http://www.overcomingbias.com/2017/08/foom-justifies-ai-risk-efforts-now.html\">Overcoming Bias</a><span>, a high profile AI expert in the </span><a href=\"https://www.newyorker.com/tech/elements/why-we-should-think-about-the-threat-of-artificial-intelligence\">New Yorker</a><span>, </span><a href=\"https://www.weforum.org/agenda/2014/12/two-mistakes-about-the-threat-from-artificial-intelligence/\">WEF</a><span>.</span></li>\n</ul>\n<div>\n<h1><a></a><span>Why use of &#x2018;AI risk&#x2019; is problematic and why use of &#x2018;AI accidents&#x2019; is helpful</span></h1>\n<p>Use of the third sense could be defended on several grounds. It is conveniently short. In a way, it is not even especially inaccurate; if, like many in the AI safety community, one believes that the <em>vast</em> majority of AI risk comes from catastrophic AI accidents, one could be excused for equivocating the labels.</p>\n<p>Problems arise in the combination of the <strong>generality</strong> of the mass noun sense and the <strong>inaccuracy</strong> of the third use. An additional issue is the <strong>missed opportunity</strong> of not using &#x2018;AI accidents&#x2019;<em>.</em></p>\n<p><em><strong>Generality:</strong> </em>A key issue is that general terms like &#x2018;AI risk&#x2019;, &#x2018;AI threat&#x2019;, etc., when used in their mass noun senses, conjure the most <a href=\"http://lesswrong.com/lw/j5/availability/\">available</a> instances of &#x2018;AI risk&#x2019;, thus summoning in many listeners images of existential catastrophes induced by artificial superintelligence &#x2013; this is perhaps one reason why the AI safety community came to employ the third, inaccurate use of &#x2018;AI risk&#x2019;. The generality of the term permits the psychological availability of existential risks from superintelligent AI to overshadow less sensational risks and accidents. A member of the AI safety community will not necessarily find this problematic; a catastrophic AI accident is indeed their main concern, so they might understandably not care much if general terms like &#x2018;AI risk&#x2019;, &#x2018;AI threat&#x2019;, etc. conjure their highest priority risk specifically. There are two groups for which this usage may cause problems: (1) sceptics of risks from catastrophic AI accidents and (2) newcomers to the subject who have not yet given issues surrounding AI much serious consideration. Aside from causing issues, not using a strategically selected frame misses opportunities to influence how groups such as policymakers and AI researchers think about existential risks from AI; using the AI accident frame should prove beneficial.</p>\n<h2>From the perspective of sceptics</h2>\n<p><em><strong>Inaccuracy:</strong> </em>Most informed sceptics of catastrophic AI accidents are plausibly still somewhat concerned about small AI accidents and other risks, but they may find it difficult to agree that they are concerned with what the AI safety community might, by the third sense, refer to as &#x2018;AI risk&#x2019;. The disagreement with &#x2018;AI risk&#x2019; (third sense) does not reflect the fact that the two groups are in broad agreement on most risks, disagreeing only on risk from a part of the AI accidents scale. The crossed labels creates the illusion of discord regarding mitigation of &#x2018;AI risk&#x2019;. The confusion helps drive the chorus of retorts that safety-proponents are wrong about &#x2018;AI risk&#x2019; and that AI accidents are the &#x2018;wrong risk&#x2019; to focus on <!-- [if supportFields]><span style='mso-element:field-begin'></span><span\nstyle='mso-spacerun:yes'>\u00a0</span>CITATION Car17 \\l 2057 <span\nstyle='mso-element:field-separator'></span><![endif]-->(Sinders, 2017)<!-- [if supportFields]><span style='mso-element:\nfield-end'></span><![endif]--><!-- [if supportFields]><span\nstyle='mso-element:field-begin'></span> CITATION Bia16 \\l 2057 <span\nstyle='mso-element:field-separator'></span><![endif]-->&#xA0;(Nogrady, 2016)<!-- [if supportFields]><span\nstyle='mso-element:field-end'></span><![endif]--><!-- [if supportFields]><span style='mso-element:field-begin'></span>\nCITATION Ale15 \\l 2057 <span style='mso-element:field-separator'></span><![endif]-->&#xA0;(Madrigal, 2015)<!-- [if supportFields]><span style='mso-element:field-end'></span><![endif]--><!-- [if supportFields]><span style='mso-element:\nfield-begin'></span> CITATION Ore16 \\l 2057 <span style='mso-element:field-separator'></span><![endif]-->&#xA0;(Etzioni, 2016)<!-- [if supportFields]><span style='mso-element:field-end'></span><![endif]-->, and presents AI safety work, which in fact mitigates risk of AI accidents of any size, as the domain of the superintelligence-concerned uniquely.</p>\n<p>With the &#x2018;AI accidents&#x2019; frame, otherwise-opposing factions can claim to be concerned with different but overlapping areas on the scale of AI accidents; the difference between those concerned about catastrophic AI accidents and those who are not is simply that the former camp sees reason to be cautious about the prospect of AI systems of <em>arbitrary levels of capability or misalignment</em>, while the latter chooses to discount perceived risk at higher levels of these scales. To observers of the debate, this partial unity is much easier to see within the AI accident frame than when the debate concerned &#x2018;AI risk&#x2019; or &#x2018;existential risk from AI&#x2019;. There does not need to be agreement about the probability of accidents on the upper-end of the scale to have consensus on the need to prevent smaller ones, thereby facilitating agreement to prioritize research that prevents AI accidents in general.</p>\n<p>Both factions now working within the same conceptual category, the result is that the <em>primary </em>disagreement between groups becomes only the scope of their concerns rather than on the existence of a principal concept. Using the &#x2018;AI accidents&#x2019; frame helps find common ground where &#x2018;AI risk&#x2019; struggles.</p>\n<h2>From the perspective of the newcomer to the subject</h2>\n<p><em><strong>Missed opportunity:</strong></em> We should conservatively assume that a newcomer to the subject holds priors that are sceptical of existential risks from artificial superintelligence. For these individuals, current language misses an opportunity for sound communication. What &#x2018;AI risk&#x2019; and even &#x2018;existential risk from artificial superintelligence&#x2019; omits to communicate is the <em>fundamental nature</em> of the risk: that the true risk is of the simple accident of deploying a singularly capable machine with a poorly designed objective function &#x2013; not something malicious or fantastical. This central point is not communicated by the label, giving the priors of the newcomer free reign over the interpretation, facilitating the &#x2018;dismissal by science fiction&#x2019;.</p>\n<p>Using &#x2018;AI accidents&#x2019;, it is directly implied that the risk involves no malicious intent. Moreover, one can point to existing examples of AI accidents, such as racist algorithms or the 2010 Flash Crash. AI accidents slightly higher on the capability scale are <em>believable</em> accidents: a housekeeping robot cooking the cat for dinner is an accident well within reach of imagination; likewise the AI that fosters war to maximise its profit objective. Using &#x2018;AI accidents&#x2019; thus creates a continuous scale populated by existing examples and facilitates arrival at the comprehension of misaligned superintelligence by simple, believable steps of induction. The framing as an accident on the upper, yet-to-be-realised part of a scale arguably makes the idea feel more tangible than &#x2018;existential risk&#x2019;.</p>\n<h2>Shaping policy discussion and research norms</h2>\n<p><em><strong>Missed opportunity: </strong></em>This reframing should confer some immediate practical benefits. Since most policy-making organisations are likely to be composed of a mix of sceptics, the concerned, and newcomers to the subject, it may be socially difficult to have frank policy discussion on potential risks from artificial superintelligence; an ill-received suggestion of existential risk from AI may be dismissed as science fiction or ridiculed. If it exists, this difficulty would be especially marked in organizations with pronounced hierarchy (a common attribute of e.g. governments), where there is a greater perceived social cost to making poorly received suggestions. In such organizations, concerns of existential risk from artificial superintelligence may thus be omitted from policy discussion or relegated to a weaker mention than if framed in terms of AI accidents involving arbitrary levels of intelligence. The &#x2018;AI accidents&#x2019; frame automatically introduces large scale AI accidents, making it an opt-out discussion item, rather than opt in.</p>\n<p>&#xA0;</p>\n<p><strong><em>Missed opportunity:</em> </strong>Stuart Russell advocates a culturally-oriented intervention to improve AI safety:</p>\n<blockquote>\n<p><em>&#x201C;I think the right approach is to build the issue directly into how practitioners define what they do. No one in civil engineering talks about &#x201C;building bridges that don&apos;t fall down.&#x201D; They just call it &#x201C;building bridges.&#x201D; Essentially all fusion researchers work on containment as a matter of course; uncontained fusion reactions just aren&apos;t useful. Right now we have to say &#x201C;AI that is probably beneficial,&#x201D; but eventually that will just be called &#x201C;AI.&#x201D; [We must] redirect the field away from its current goal of building pure intelligence for its own sake, regardless of the associated objectives and their consequences.&#x201D; </em><!-- [if supportFields]><span\nstyle='mso-element:field-begin'></span><span\nstyle='mso-spacerun:yes'>\u00a0</span>CITATION Boh17 \\l 2057 <span\nstyle='mso-element:field-separator'></span><![endif]-->(Bohannon, 2017)<!-- [if supportFields]><span style='mso-element:\nfield-end'></span><![endif]--><em>.</em></p>\n</blockquote>\n<p>How to realise Russell&#x2019;s edict? Seth Baum discusses framing as an &#x2018;intrinsic measure&#x2019; to influence social norms in AI research to pursue beneficial designs and highlights the importance not only of <em>what </em>is said, but <em>how</em> something is said <!-- [if supportFields]><span\nstyle='mso-element:field-begin'></span><span\nstyle='mso-spacerun:yes'>\u00a0</span>CITATION Set16 \\l 2057 <span\nstyle='mso-element:field-separator'></span><![endif]-->(Baum, 2016)<!-- [if supportFields]><span style='mso-element:field-end'></span><![endif]-->. For engineers, it would be strangely vague to talk about &#x2018;car risk&#x2019;, &#x2018;bridge risk&#x2019; or other broad terms. Instead, they talk about reducing the risk of car accidents or bridge collapses &#x2013; referring explicitly to the event that they are responsible for mitigating and precluding competing ideas, e.g. the risks from mass use of cars on air pollution, or from disruption to a horse-driven economy. The same should be true for AI. The &#x2018;AI accidents&#x2019; frame moves thinking away from abstract argument and analogy and brings the salient concepts closer to the material realm. Giving AI researchers a clear, available, and tangible idea of the class of events they should design to avoid will be important to engender safe AI research norms.</p>\n<h1><a></a><span>Seizing the opportunity</span></h1>\n<p>The count noun and mass noun senses of &#x2018;AI risk&#x2019; and &#x2018;existential risk from AI&#x2019; etc. still have their place. But opportunities should be sought for the &#x2018;AI accidents&#x2019; frame where it is appropriate. Without being prescriptive (and cognisant that not all catastrophic AI risks are of catastrophic AI accidents), instead of &#x2018;reducing AI risk&#x2019; or &#x2018;reducing existential risk from AI&#x2019;, the policy, strategy, and technical AI safety community would claim to work on reducing the risk of AI accidents, at least where they are not also working on other risks.</p>\n<p>Shifting established linguistic habits requires effort. The AI safety community is relatively small and cohesive, so establishing this subtle but potentially powerful change in frame at a community level could be an achievable aim. By driving a shift in terminology, a goal of wider adoption by other groups such as policy makers, journalists, and AI researchers is within reach.</p>\n<p>&#xA0;</p>\n<h1>Footnotes</h1>\n<p>[1] For comment and review, I am grateful to Nick Robinson, Hannah Todd, and Jakob Graabak.</p>\n<p>[2] In some links, other AI risks are discussed elsewhere in the texts, but nevertheless the sense in which &#x2018;AI risk&#x2019; was used was actually the third sense. The list is not exhaustive.&#xA0;</p>\n<p>[3] By a co-founder - not an institutional use.</p>\n<p>&#xA0;</p>\n<h1><a></a><span>Works Cited</span></h1>\n<p><!-- [if supportFields]><span\nstyle='mso-element:field-begin'></span><span\nstyle='mso-spacerun:yes'>\u00a0</span>BIBLIOGRAPHY <span style='mso-element:field-separator'></span><![endif]-->Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., &amp; Man&#xE9;, D. (2016, July 25). <em>Concrete Problems in AI Safety</em>. Retrieved from arXiv:1606.06565v2 [cs.AI]: https://arxiv.org/abs/1606.06565</p>\n<p>Baum, S. (2016). On the promotion of safe and socially beneficial artificial intelligence. <em>AI &amp; Society</em>, 1-9. Retrieved from https://link.springer.com/article/10.1007/s00146-016-0677-0</p>\n<p>Bohannon, J. (2017, July 17). Fears of an AI pioneer. <em>Science</em>, Vol. 349, Issue 6245, pp. 252. doi:DOI: 10.1126/science.349.6245.252</p>\n<p>Bostrom, N., Dafoe, A., &amp; Flynn, C. (2016). <em>Policy Desiderata in the Development of Machine Superintelligence</em>.</p>\n<p>Dafoe, A., &amp; Russell, S. (2016, November 2). <em>Yes, We Are Worried About the Existential Risk of Artificial Intelligence</em>. Retrieved from MIT Technology Review: https://www.technologyreview.com/s/602776/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/</p>\n<p>Etzioni, O. (2016, September 20). <em>No, the Experts Don&#x2019;t Think Superintelligent AI is a Threat to Humanity</em>. (MIT Technology Review) Retrieved from https://www.technologyreview.com/s/602410/no-the-experts-dont-think-superintelligent-ai-is-a-threat-to-humanity/</p>\n<p>LessWrong commenter &quot;Snarles&quot;. (2010, May 19). <em>Be a Visiting Fellow at the Singularity Institute</em>. Retrieved from LessWrong: http://lesswrong.com/lw/29c/be_a_visiting_fellow_at_the_singularity_institute/</p>\n<p>Madrigal, A. (2015, February 27). <em>The case against killer robots, from a guy actually working on artificial intelligence</em>. Retrieved from Splinternews: http://splinternews.com/the-case-against-killer-robots-from-a-guy-actually-wor-1793845735</p>\n<p>Minsky, M. (1984). <em>Afterword to Vernor Vinge&apos;s novel, &quot;True Names&quot;.</em> Retrieved from http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html</p>\n<p>Nogrady, B. (2016, November 10). <em>The Real Risks of Artificial Intelligence</em>. Retrieved from BBC: http://www.bbc.com/future/story/20161110-the-real-risks-of-artificial-intelligence</p>\n<p>Samuelson, K. (2017, July 25). <em>Elon Musk Just Dissed Mark Zuckerberg&#x2019;s Understanding of Artificial Intelligence</em>. (Fortune) Retrieved from http://fortune.com/2017/07/25/elon-musk-just-dissed-mark-zuckerbergs-understanding-of-artificial-intelligence/</p>\n<p>Sinders, C. (2017, August 25). <em>Dear Elon &#x2013; Forget Killer Robots. Here&#x2019;s What You Should Really Worry About</em>. Retrieved from Fastcodedesign: https://www.fastcodesign.com/90137818/dear-elon-forget-killer-robots-heres-what-you-should-really-worry-about</p>\n<p>University of Cambridge. (2012, November 25). <em>Humanity&apos;s last invention and our uncertain future</em>. Retrieved from http://www.cam.ac.uk/research/news/humanitys-last-invention-and-our-uncertain-future</p>\n<!-- [if supportFields]><b><span style='font-size:11.0pt;line-height:107%;\nfont-family:\"Calibri\",sans-serif;mso-ascii-theme-font:minor-latin;mso-fareast-font-family:\n\"Times New Roman\";mso-fareast-theme-font:minor-fareast;mso-hansi-theme-font:\nminor-latin;mso-bidi-font-family:\"Times New Roman\";mso-bidi-theme-font:minor-bidi;\nmso-ansi-language:EN-GB;mso-fareast-language:EN-US;mso-bidi-language:AR-SA'><span\nstyle='mso-element:field-end'></span></span></b><![endif]--></div></body></html>", "user": {"username": "Lee_Sharkey"}}, {"_id": "org8QYG3gqgDK7St6", "title": "Effective Altruism Grants project update", "postedAt": "2017-09-29T20:05:53.514Z", "htmlBody": "<html><body><p>The Centre for Effective Altruism has distributed its first round of grants through its new <a href=\"https://www.effectivealtruism.org/grants/\">Effective Altruism Grants</a> program. The aim of the project is to solve funding constraints for high-impact projects. You can read more about our <a href=\"https://www.effectivealtruism.org/grants/#motivation-and-aims\">motivation and aims</a>.</p>\n<p>&#xA0;</p>\n<p>This post details (1) the grants we&#x2019;ve made, (2) our assumptions, (3) the grant methodology, (4) cost-benefit considerations, (5) mistakes, (6) difficulties, (7) project changes, and (8) our plans for EA Grants going forward.</p>\n<h1><span>Grants</span></h1>\n<p>We are sharing information about our grant this round to give people a better sense of what kinds of projects we look for, should we run EA Grants rounds in the future. You can see the <a href=\"https://docs.google.com/spreadsheets/d/1iBy--zMyIiTgybYRUQZIm11WKGQZcixaCmIaysRmGvk/edit?usp=sharing\">grants</a> we made.</p>\n<p>We have allocated &#xA3;369,924 for distribution, withholding the remainder of the allotted &#xA3;500,000 to further fund some of the current recipients, contingent on performance.</p>\n<p>We also facilitated the funding of grants by the Open Philanthropy Project and a couple of private donors.</p>\n<h1><span>Assumptions</span></h1>\n<p>There were many implicit assumptions we made in deciding that and how to run EA Grants. A few of the major ones include:</p>\n<h2><span>Many good projects are hamstrung by small funding gaps.</span></h2>\n<p>We believe some high-value projects have unmet funding needs. The individuals and small organizations we decided to fund are generally too small to get on the radar of foundations like the Open Philanthropy Project, and small donors rarely have time or expertise to evaluate many small projects. But there are high returns to funding them.</p>\n<h2><span>Value alignment is useful for maintaining project relevance.</span></h2>\n<p>In order to be comfortable with this arrangement, we placed particular emphasis on evaluating value alignment, altruistic motivation, and judgment. Value alignment was particularly important, even more than ostensibly well-defined projects, since some autonomy is inevitable. All else equal we preferred projects by applicants who have a track record of doing this or other projects well and on a voluntary or selflessly motivated basis. (One exception to this rule is that we must stipulate that funding is not used for certain activities that don&#x2019;t fit within our charitable objects.)</p>\n<h2><span>At this funding level, a hefty application process would be more costly than useful.</span></h2>\n<p>Many grantmaking processes require multi-page proposals. Since our grants were both smaller and more speculative than many of the grants foundations distribute, applications of that length felt unnecessarily costly, both for the applicants and for us as evaluators. This had costs: projects that are hard to describe briefly suffer from insufficient space to make their cases. We tried to get the best of both worlds by requesting additional information where we found them hard to assess with just what we had. We leave open the possibility of longer proposals in the future should we run subsequent rounds.</p>\n<h1><span>Grant methodology</span></h1>\n<p>The grants application process had three rounds, and is best described as a <a href=\"http://www.openphilanthropy.org/blog/projects-people-and-processes\">process-based approach</a>.</p>\n<h2><span>First round</span></h2>\n<p>In the first round the three grants associates eliminated applications which we could clearly assess would not meet our selection criteria. We received 722 applications and desk rejected 413 of them, about 57% of the applicants.</p>\n<h2><span>Second round</span></h2>\n<p>The second round involved taking the remaining applications and assessing applicants based on their track record, values, and plans. This assessment adhered to a rubric, weighting each category in accordance with its predictive power for project success. The scores combined into one weighted score per applicant, which we used to rank the remaining applicants.</p>\n<p>We then went through the list by rank and chose applicants to interview, discussing applicants about which there was large divergence in scores or general opinion. Given our &#xA3;500,000 budget and most of three staff members&#x2019; time for two weeks, we decided to interview 63 candidates.</p>\n<h2><span>Third round</span></h2>\n<p>Most candidates had three, 10-minute interviews, which we used to further assess their achievements, values, and plans. Candidates we knew well received only one interview. For candidates with skillsets we couldn&#x2019;t evaluate internally we arranged a fourth interview with a relevant technical expert. We then used the data from these interviews, as well as any additional information requested from references and/or the applicants themselves, to adjust their written application scores. While each interviewer could make modifications to scores in all three categories, interviewers each had a category of focus, so their assessments in their respective area received the most weight.</p>\n<p>Finally, we went through the new rank-ordered list and decided who to fund and how much. We initially assigned grants values to candidates in rank order until we&#x2019;d exhausted the funding pool, then adjusted amounts to fit the particular circumstances of the grantees. Such considerations include our credence in the score given, the counterfactuals of funding each candidate, the potential risks associated with the candidate and/or their proposal, and what candidates could do with money on the margin. We passed promising candidates who did not fit our charitable objects or who requested money out of scope of our funding capacity onto some private donors associated with CEA and/or the relevant program officer at the Open Philanthropy Project.</p>\n<p>Through this process we selected 22 candidates to fund, partially or in full, and passed another 11 on to the Open Philanthropy Project.</p>\n<h1><span>Cost-benefit considerations</span></h1>\n<p>An important consideration in our thinking is whether or not the costs of running EA Grants exceed its benefits. Since the counterfactual is likely a future grant made by the Open Philanthropy Project, one angle for evaluating EA Grants is to compare its costs and benefits relative to the distribution(s) Open Phil might have made otherwise. CEA distributed &#xA3;600&#xA0;per hour worked by the grants team, whereas we estimate Open Phil distributes&#xA0;~&#xA3;20,000 per hour. However, we think a comparison made in this way&#xA0;has limitations.</p>\n<h2><span>Costs</span></h2>\n<p>The costs are the &#xA3;500,000 disseminated, plus ~740 CEA staff hours thus far. We expect to spend another 100 hours on activities related to this round of grantees, mostly arranging mentors and ensuring financial regulatory compliance. There have also been costs to other EA organizations &#x2014; mostly the Open Philanthropy Project, who has decided to evaluate and fund some of the grantees who went through the application process.</p>\n<p>An Open Phil staff member made a rough guess that it takes them 13-75 hours per grant distributed. Their average grant size is quite a bit larger, so it seems reasonable to assume it would take them about 25 hours to distribute a pot the size of EA Grants. This, of course, ignores the time costs of institution-building. Much of the time we spent in this funding round went to building the internal grants infrastructure and relationships with other funders. Should we run this project again, we expect to be able to run a similar grants process in a fraction of the time.</p>\n<p>This ratio has limited meaning, most notably ignoring that Open Phil found this project compelling enough to fund. While they can distribute more funding per hour having achieved scale, we find it plausible that the additional costs for these smaller projects were still worth it on the margin. The cost-per-dollar distributed is less than that of other impact-focused foundations, likely on par if we were to factor in staff overhead time.</p>\n<h2><span>Benefits</span></h2>\n<p>The benefits are notably quite complicated to calculate. Any individual project is itself going to be challenging to evaluate, since most of the value is likely to come from hard-to-track, long-term changes to sentiments and behavior. Rather than try to compute the value of each grant with a common base metric, we have instead opted for projects that seem robustly positive should they work. This, again, is not unlike Open Phil&#x2019;s strategy; the real question is how effective we think our distributions were compared to theirs.</p>\n<p>It seems likely that we&#x2019;ve picked up on value they would not have, given the scale of interventions they generally consider, which is more valuable per dollar than where they would have given. Reasons to believe this include:</p>\n<ul>\n<li>\n<p>Scaling potential. By funding early-stage projects, many with plans to grow, the returns to funding at this stage are higher variance but also higher potential upside.</p>\n</li>\n<li>\n<p>Inexpensive salaries. Most people requested living wages at or below nonprofit employee salaries.</p>\n</li>\n<li>\n<p>Funding individuals. Not only were salaries cheaper, but individuals are cheaper. Organizations often spend 1.7x an employee&#x2019;s salary on overhead.</p>\n</li>\n</ul>\n<p>CEA&#x2019;s counterfactuals are unclear. We are unsure if CEA would have received the additional money were EA Grants not in our plans. Assuming that to be the case, Open Phil might have later granted the money to some other community-building activity. Had CEA staff not worked on this program, we would have accelerated progress on writing collated EA content, built out the EA events infrastructure, and worked on plans for EA academic engagement. As for the projects we funded, we estimate that about one quarter of the projects wouldn&#x2019;t have happened at all, and the rest would have received less time, since the grantees would have pursued other funding (from the Open Philanthropy Project, or elsewhere) or self-funded by working or going into personal debt.</p>\n<h1><span>Mistakes</span></h1>\n<p>Our communication was confusing. We initially announced the process with little advertisement. Then, we advertised to the EA Newsletter, but only shortly before the application deadline, and extended the deadline by two days.</p>\n<p>We underestimated the number of applications we would receive, which gave us less time per candidate in the initial evaluation than we would have liked. It also caused delays, which we did not adequately communicate to applicants. &#xA0;We should have been less ambitious in setting our initial deadlines for replying, and should have communicated all changes in our timetable immediately and in writing to all applicants.</p>\n<p>Our advertisement did not make sufficiently clear that we might not be able to fund educational expenses through CEA. Fortunately, the Open Philanthropy Project was receptive to considering some of the academic applicants.</p>\n<h1><span>Difficulties</span></h1>\n<h2><span>Project evaluation</span></h2>\n<p>We found it hard to make decisions on first-round applications that looked potentially promising but were outside of our in-house expertise. Many applicants had proposals for studies and charities we felt under-qualified to assess. Most of those applicants we turned down; some we deferred to the relevant Open Phil program manager. We are in the process of establishing relationships with domain experts who can help us do this in the future.</p>\n<h2><span>Conflicts of interest</span></h2>\n<p>One difficulty in running this program is its susceptibility to conflicts of interest (COIs).</p>\n<p>Many of the most promising applications came from people who are already deeply involved with the community. Involvement with the community gives us evidence of value-alignment, and the community also provides a context within which it is easier to come up with proposals that we think are important.</p>\n<p>Unfortunately, since many applicants, and particularly many of the best, were deeply involved with the community, our assessing staff tended to have many COIs. This includes one of the team members, who was both an grant evaluator and an applicant.</p>\n<p>Rather than avoid giving where COIs existed, we adopted a view much like that of the Open Philanthropy Project. You can see the details articulated in Holden Karnofsky&#x2019;s post on&#xA0;<a href=\"http://www.openphilanthropy.org/blog/hits-based-giving#Anti-principles_for_hits-based_giving\">hits-based giving</a>. We recognized and tried to mitigate the effects of COIs by asking for expert input, expecting domain expertise to help correct for personal domain-irrelevant sentiments. However, given our process-based approach and comparatively limited internal capacity, it was both less necessary and less feasible for us to develop in-house expertise on areas about which we formerly knew little, another&#xA0;means of reducing the impact of COIs.</p>\n<p>The measures we took include:</p>\n<ul>\n<li>\n<p>Blinding applications during the first and second rounds of the application process, such that all written applications received scores while anonymized.</p>\n</li>\n<li>\n<p>Asking staff members to declare conflicts of interest with finalists, where they existed. The team then found replacement interviewers and asked the associated staff member to step out of decisionmaking for those candidates.</p>\n</li>\n<li>\n<p>Deferring applications to staff of the Open Philanthropy Project when the project proposals were outside our domains of expertise.</p>\n</li>\n<li>\n<p>Establishing scores in our rubric associated with observable measures, tying applicants&#x2019; scores to specific features of their abilities and plans rather than our general impression.</p>\n</li>\n<li>\n<p>For the grant applicant who was also an assessor, removing him from all discussions about his application, obscuring his score and ranking, and subjecting him to the same evaluation process as all other grantees.</p>\n</li>\n</ul>\n<h1><span>Project changes</span></h1>\n<h2><span>We can&#x2019;t fund educational expenses.</span></h2>\n<p>We adhered closely to our <a href=\"https://www.effectivealtruism.org/grants/#what-projects-could-get-funded\">grant areas</a>, funding nothing out of scope of what we described on the website. However, we have since determined that we cannot fund all of the projects for which we encouraged people to apply. Most notably, CEA&#x2019;s charitable objects do not allow us to pay for education expenses, making it impossible for us to give grants for Masters or PhD programs. However, the Open Philanthropy Project is able to do so and has started to consider funding candidates pursuing research in their priority areas.</p>\n<h2><span>We are unlikely to make grants for longer than a year.</span></h2>\n<p>While we offered opportunities for grant renewal, we didn&#x2019;t make any grants lasting more than a year. This was more a result of happenstance than an intentional decision. For the few finalists who requested more than a year of funding, we were sufficiently unsure of either their proposal or their future funding situation so as to not want to commit more than a year upfront. That being said, we&#x2019;re still open to doing so in the future.</p>\n<h1><span>Plans going forward</span></h1>\n<p>It seems likely that we will run a similar program in the future. Kerry Vaughan has just taken over ownership of this project, and will be in charge of deciding on and implementing changes. That being said, the initial EA Grants team has many ideas of how to improve the scheme, and in particular how to solve the mistakes discussed above. We will coordinate with Kerry and post again when we have more information.</p>\n<p>As I will no&#xA0;longer work&#xA0;on this project after October 6th, please direct questions and comments to <a href=\"mailto:eagrants@centreforeffectivealtruism.org\">eagrants@centreforeffectivealtruism.org</a>.</p>\n<p>&#xA0;</p>\n<p>Thanks to Ryan Carey, Rohin Shah, and Vipul Naik for corrections to this post.</p></body></html>", "user": {"username": "Roxanne_Heston"}}, {"_id": "ifkFqcnw3u2p6EScB", "title": "Update on Envision: progress thus far and next steps", "postedAt": "2017-09-27T20:51:52.409Z", "htmlBody": "<html><body><p><span>I&#x2019;m the next president of Envision, a student group working to impart a safety-conscious mindset towards technology to future leaders. Last year my predecessor Luca Rade wrote a </span><a href=\"/ea/10b/introducing_envision_a_new_eaaligned_organization/\"><span>post</span></a><span> which laid out Envision&#x2019;s mission and how we intend to accomplish it. This post will provide a summary of what Envision accomplished this past year, present the challenges we faced in implementing the plan Luca laid out, show how we&#x2019;re refining our strategy to confront these challenges, and give our plan for the future. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>In an effort to make this post self-contained, I&#x2019;ll provide a summary of Envision&#x2019;s mission as laid out in Luca&#x2019;s post. I highly recommend reading his original post for the complete picture on our mission. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Envision&#x2019;s mission is to imbue a safety-conscious mindset towards technology in industry, political, and academic leaders. We strive to do so by exposing young people likely to be leaders in these domains to the risks advanced technologies present and convincing them that these risks necessitate action to ensure technology is developed prudently. Our events attract young future domain leaders by cultivating prestige, bringing leadership in fields they&#x2019;re interested in, and focusing on how a fundamental concern for safety will allow domain leaders to achieve positive results. Luca&#x2019;s post expands on the specific mechanisms we use to instill a safety-conscious mindset.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>What we accomplished last year</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Envision Conference</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Last year Envision held our first annual Envision Conference. The conference brought together 150 participants from 38 institutions and 8 countries to analyze the future implications of technology. Andrew Critch, Robin Hanson, and Anders Sandberg all spoke at the first Envision Conference and will be returning this year. Additional speakers included Luke Nosek, a co-founder of PayPal and Founder&#x2019;s Fund; Andreas Mershin, a physicist at MIT; Jeremy Kashin, a professor at Princeton and scientist at NASA; and other accomplished researchers and professionals actively thinking about the risks and possibilities in our future. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Envision Conference&#x2019;s mission is to select young participants who are likely to be leaders in developing, implementing, and/or regulating future technology and teach them the importance of developing technology with a safety conscious mindset. We believe last year&#x2019;s Conference made progress towards achieving this goal based on the following data points:</span></p>\n<p><strong>&#xA0;</strong></p>\n<ul>\n<li>\n<p><span>The conference had a counterfactual impact by targeting individuals who are not typically reached by the EA community. </span></p>\n</li>\n<ul>\n<li>\n<p><span>We advertised the conference heavily to groups with technically excellent participants, and these groups are primary focused on developing technology rather than analyzing its implications. The prestige and positive word of mouth publicity our conference developed helped us also passively attract qualified participants to our applicant pool. These individuals are likely to have a role in developing technology, and have likely not previously thought deeply about the risks associated with technology. Targeting young participants also allows us to influence their belief system while they&#x2019;re still developing it, embedding a concern for prudence more deeply than if they are exposed to it once already on the path towards domain leadership. </span></p>\n</li>\n<li>\n<p><span>Travel reimbursement enabled us to reach qualified participants who are unable to attend other conferences, or require a strong incentive to find a conference worthwhile to attend. Our competitive and holistic admissions process helped us ensure that these participants are also likely to become future leaders. </span></p>\n</li>\n<li>\n<p><span>By coupling the positive potential of technology to its risks, we were able to attract entrepreneurs and others who are not attracted to EA due to the excessive sense of obligation it can engender.</span></p>\n</li>\n</ul>\n<li>\n<p><span>Our conference events reached many people who were unable to attend the conference. The live stream of our closing ceremony had 1400 viewers and our panel on AI safety had close to 250 live participants (including Princeton students). </span></p>\n</li>\n<li>\n<p><span>48% of conference participants reported an increased sense of agency towards shaping the future.</span></p>\n</li>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<p><span>Many of our participants showed that their opinions were changed or took measurable steps to change their actions after attending the conference. Here are some of their quotes and stories:</span></p>\n<ul>\n<li>\n<p><span>&#x201C;I used to think that the path toward progress was taking place in lock-step with a clear goal at the end, but now I realize that the future could very easily be hijacked and taken in a different, harmful direction.&#x201D;</span></p>\n</li>\n<li>\n<p><span>Participants report that the conference helped engage them with a new community. Engagement in this community remains high after the conference; many repeat applicants to the conference wrote &#xA0;that they still are in contact with individuals they met at the first conference. This community helps sustain the connection our participants have with the mission of Envision. As I will explain in the &#x201C;Leverage the Matrix&#x201D; section, we&#x2019;re going to take steps to continue building this community. </span></p>\n</li>\n<li>\n<p><span>I fit the demographic the conference attempts to target well: my primary interest is in technical businesses. I&#x2019;ve worked as a programmer since I was a sophomore in high school, have developed numerous technical projects that are used by thousands, and did an internship at a major security company. &#xA0;Before the conference I was primarily interested in developing technology with no regard to safety. I joined Envision after attending more of their events and quickly became heavily involved. The conference personally changed the way I view the future, and catalyzed my strong desire to help mitigate existential risk. </span></p>\n</li>\n</ul>\n<p><strong>&#xA0;</strong></p>\n<p><span>While we believe our conference last year was successful, we do have substantial room to grow. One of our primary areas for growth is to establish better data-gathering procedures and a process for measuring the longer-term impact of the conference on conference participants. To acquire better data on the long term impact of the conference we&#x2019;re going to expand our post-conference survey and continue to follow up with participants in the medium and long terms. We&#x2019;ll increase the number of participants that participate in the survey by cultivating a community of conference participants and incentivizing participation. The &#x201C;Leverage the Matrix&#x201D; section explains this plan in further detail. &#xA0;</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The conference&#x2019;s longevity is still unclear, and we are working to ensure that it will continue to be successful throughout leadership transitions. I&#x2019;ll detail our plan to increase the conference&#x2019;s and Envision&#x2019;s longevity in the &#x201C;Secure Longevity&#x201D; section. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Engaging the Princeton community</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>We hosted many events throughout the year at Princeton in addition to the conference. The events we hosted include panels on technology policy with professors with governmental experience, talks with top researchers who spoke about their work to confront the potential negative consequences of technology, discussions among students on the implications of artificial intelligence and other technologies, a book club, and trips to Boston and Silicon Valley where Princeton students met with top companies and researchers about their work. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The trips to Boston and Silicon Valley had the largest overall impact of our Princeton-based events. Both trips had 15 participants, but because of the high value and intensity of the trip the application process was very competitive and many of the members of these trips made dramatic shifts in their professional and collegiate plans after attending the trip. One participant switched their research focus and ultimately switched careers after learning what technologies will be most impactful, another student who&#x2019;s now running our entrepreneurship competition narrowed their general interest in venture capital to identifying businesses that leverage a successful business strategy to catalyze technical and social change, and a participant interested in software engineering and finance now actively works for Envision to promote our mission. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The trips to Silicon Valley and Boston were able to effectively attract participants and bring a competitive pool, but many of our other events were unable to attract a significant number of participants. This accurately depicts one of the primary challenges Envision faces: inability to attract future leaders to our smaller scale events. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Challenges</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Attracting Future Leaders</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Luca addressed the concern that Envision will be unable to attract domain leaders in his post. We &#xA0;believe, based on the quality of our participants, that we&#x2019;ve have so far broadly succeeded in attracting domain leaders at the Conference and our trips. Our smaller events have mostly failed to do so. Most undergraduates are busy, and the undergraduates that we target are generally busier than average. As a result our panels, book groups, discussions, and other smaller events have struggled to attract a meaningful number of participants.</span></p>\n<p>&#xA0;</p>\n<p><span>We believe that the larger events are able to attract quality participants because of their prestige and the high value they provide to participants. Our conference is able to bring world-class researchers and speakers, helping us attract world class participants. Based on early bird applications, the conference this year has also benefited from a substantial network effect: previous participants promote the conference and its mission to their peers, spreading Envision and its mission. The trips have experienced similar success because of the value that they provide participants. </span></p>\n<p><span>We will continue to hold some smaller events since they provide value for the people who do attend, but we will shift to spending more time developing a smaller number of high value events. In addition to our main conference and trips, we&#x2019;re planning on hosting a smaller conference in Boston &#xA0;to help establish a second hub for Envision and engage another community. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Effective Officers</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Envision has been successful because a small number of officers commit a disproportionate amount of effort. We&#x2019;ve recruited heavily and receive more than fifty applications to become an officer every year, but we struggle with maintaining long-term, effective officers that grow Envision. Envision&#x2019;s success is dependent on a substantial amount of work from its officers, and few of the people we recruit are willing to commit the necessary effort. </span></p>\n<p>&#xA0;</p>\n<p><span>Our interviews with ineffective officers show us that many capable officers that believe in Envision&#x2019;s mission are more passionate about integrating it into their lives than spreading it to others. These officers were interested in joining Envision, but shifted their effort to ventures where they see a greater potential for personal benefit. We&#x2019;ve used this information to change how we select officers. Previously, we preferred candidates with previous initiative and technical expertise--we thought that those factors were indicative of highly capable officers. We now see that while these officers are likely highly capable, we can&#x2019;t guarantee that they&#x2019;ll direct their talent towards Envision. We now seek to identify officers that will dedicate time and effort to Envision by favoring applicants passionate about spreading Envision&#x2019;s mission. We can train officers and help develop aptitude if they have passion, but we can&#x2019;t effectively use capable officers that aren&#x2019;t passionate about working for the organization. Specifically, we&#x2019;ve substantially reduced the weight of technical competence, and instead look for passion for spreading Envision&#x2019;s mission. These adjustments will be an iterative process. Genuine passion for spreading Envison&#x2019;s mission will be hard to identify and we&#x2019;re actively refining our processes and exploring new ways of doing so. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Growing other Envision Chapters</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>After the conference several participants from different schools expressed interest in starting an Envision chapter at their school. While several of these participants started chapters at their school, none had more than a few events. Our conversations with the founders lead us to believe that they were two primary reasons for these chapters&#x2019; failures: </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>-Lack of a framework for Envision chapters. We actively supported the chapter founders and helped them plan events, but we did not provide a detailed framework on how to create an Envision chapter. We thought that the founders were best suited to understand what events would be successful and beneficial to their schools. A detailed framework on how to create a chapter felt pedantic and like it robbed the founders of the ability to take ownership of their chapters. We also felt that we would be unable to provide a framework that would guarantee the success of an Envision chapter since our chapter&#x2019;s success was partially based on context, luck, and the Herculean effort of a few individuals. </span></p>\n<p><span>-Lack of sufficient interest from chapter founders. The chapter founders were very interested in the principles of Envision and genuinely wanted to help, but creating a successful chapter requires an obsessive group of founders who devote almost all of their time to growing the chapter. The challenges Envision continues to overcome are hard, and we haven&#x2019;t found a scalable way for chapters without Envision&#x2019;s central leadership to overcome them. </span></p>\n<p><span>-Envision currently isn&#x2019;t well-established enough to inspire competent participants to create a subsidiary organization. Many of the most capable participants of the conference who were interested in continuing to work on Envison&#x2019;s mission were hesitant to create an organization that&#x2019;s derivative of Envision. They wanted to create distinct organizations that could achieve a similar level of success, and in one notable case they failed to do so because they tried to partially reproduce Envision without adopting key aspects that drew sponsors and participants.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Our plan going forward</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Leverage the Matrix</span></p>\n<p><span>We have learned that it&#x2019;s not a feasible short term goal to create a network of active Envision chapters at different universities. Envision chapters require dedicated leadership and we aren&#x2019;t able to effectively train conference participants to become the leader a chapter requires. We will continue to support members if they want to create a chapter, but we&#x2019;re going to shift our effort to leverage our network in more effective ways. Envision has grown a substantial network of collaborators across the world that are passionate about our mission, and we intend to leverage this network to address our challenges. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Envision&#x2019;s network can be used to keep participants outside of Envision engaged with our mission and help them apply our principles to their work. We&#x2019;re going to help our network connect by creating an internet forum (likely a private subreddit) where participants in Envision events are encouraged to post updates, discuss ideas, and collaborate on projects. We have empirical evidence that suggests that this resource will be utilized: the Facebook page for last year&#x2019;s Envision conference is still moderately active despite little effort on our part in encouraging this almost nine months after the conference.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>We&#x2019;re going to focus our effort on establishing a second Envision &#x201C;hub&#x201D; in one geographical location. By focusing on expanding to one area, our officers will be able to actively work to plan events with members of the local community. We&#x2019;ll host a smaller conference in the community to engage community members and help build momentum. Right now Boston is our target for the next hub of Envision. We have heavily involved members and an officer in Boston, have partnerships with several schools, and believe that Boston&#x2019;s combination of prestigious schools and innovative startups will make it a high value target. Envision will complement the value provided by the Boston-based Future of Life Institute by using our unique position to engage with students, and young professionals.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Secure Longevity</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Leading Envision requires an effective leader who&#x2019;s willing to devote all of their time. Finding an undergraduate who combines high competence with extreme work ethic and passion is incredibly challenging, even at top schools. Finding a leader is challenging, but we&#x2019;ve had promising success in identifying and training individuals who may fit the role. We look to identify candidates to lead Envision by selecting participants who are extremely passionate about the mission and have some previous leadership experience. Many of these candidates have underdeveloped resumes, but their passion and potential is clear from speaking with them. Our trips have proven to be the most successful way to attract these candidates and show them the importance of the issues Envision addresses: they give us a unique ability to intimately interact with people interested in Envision&#x2019;s mission and learn more about their background while providing intense exposure. n. There is no absolute formula for identifying capable leaders. Envision&#x2019;s mission may dilute over time, and one extremely misaligned leader could bring the end. We&#x2019;ll mitigate this risk by having a Board, consisting of former presidents, select the new president, provide mentorship, and in extreme circumstances step in if Envision is not making progress towards its mission. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Beyond leadership the biggest obstacle to Envision&#x2019;s success is monetary support for our events. We&#x2019;re fairly confident that our large events provide significant value and are able to effectively attract domain leaders and will increase this confidence in the future as we gather better data, but these events are expensive. Even with leaders who devote all of their time outside of classwork to Envision, we struggle to fund the events. Our conference last year was made possible partly by a donation by a member of the EA community, but we can&#x2019;t become dependent on private donations without a clear prospect for continuity.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Envision is seeking large corporate sponsors for the conference to provide recurring contributions to help secure its financial longevity. We&#x2019;ve had promising initial success--we&#x2019;ve already confirmed Microsoft and Milliporesigma (Sigma-Aldrich) as sponsors of this year&#x2019;s conference. &#xA0;Although we have yet to reach our funding target, we believe we are on track to raise the budget we need to run the Conference with similar events as last year. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>However, the fundraising process is currently the primary focus of Envison&#x2019;s current leadership, and given the difficulty of raising the funds last year and this year, the capability to re-raise the money for the conference every year is not clear. Additionally, we are not able to spend time developing new events and growing the organization because we are focused on raising the funds for the conference and do not have sufficient funds beyond what is necessary for running the conference in its current form.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Therefore we&#x2019;re raising an endowment to help secure the financial future of Envision and help it weather a temporary absence of competent and/or aligned leadership. An endowment will help secure our monetary future by significantly reducing the burden on leadership to raise substantial sums of money for every individual event so that we can focus on developing the organization and increasing our impact. It will provide initial momentum to the leadership of events to raise the remaining funds, enabling us to expand our reach. The endowment will help ensure that Envision is able to survive leadership transitions by reducing the burden on the new leader to spend their time raising money, and provide a catalysis for potential leaders to feel like joining Envision will have a substantial impact. The interest from the Endowment will initially be used primarily to help Envision establish a second hub in Boston, sustain our conference, and continue to run our high impact trips. The endowment will not fully fund any of these activities, but it will provide vital initial momentum the organizers of these events can use to raise additional funds and run the events. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Spending from the endowment will be regulated to ensure that it is used effectively every year. The president will create a spending plan for the annual interest from the endowment that will be approved by the board of previous presidents. No more than 50% of the interest can be used for a single event and the principal will remain untouched, and additional funds from events will contribute to the endowment&#x2019;s principal. </span></p>\n<p><strong><br><br></strong></p>\n<p><span>To ensure that the endowment continues to be used effectively, the charter will mandate that the endowment will be donated to </span><span>an organization within Effective Altruism dealing with existential risk, to be determined by the Board of Trustees,</span><span> if Envision fails to host a successful Envision Conference with success defined by the following criteria: </span></p>\n<ul>\n<li>\n<p><span>At least 100 participants from 10 or more institutions </span></p>\n</li>\n<li>\n<p><span>At least 10 speakers, with the following conditions:</span></p>\n</li>\n<ul>\n<li>\n<p><span>At least half are not from Princeton. </span></p>\n</li>\n<li>\n<p><span>At least two are affiliated in some way with EA.</span></p>\n</li>\n<li>\n<p><span>At least one is speaking on existential risk.</span></p>\n</li>\n</ul>\n<li>\n<p><span>At least $15,000 raised from corporate sponsorship and Princeton support</span></p>\n</li>\n<ul>\n<li>\n<p><span>Excess funds will be redirected to the endowment if not necessary for the conference. </span></p>\n</li>\n</ul>\n</ul>\n<p><span> &#xA0;</span></p>\n<p><strong><br><br></strong></p>\n<p><span>Envision Conference 2017</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Envision Conference 2017&#x2019;s initial progress shows exciting evidence that Envision has been effective in engaging young leaders and cultivating prestige. The momentum from the first Envision Conference enabled us to acquire a better speaker roster than last year including: three accomplished professors at MIT; the former CTO of the FTC and professor at Harvard; an executive at a major government contractor with hundreds of millions in annual revenue; accomplished researchers in Artificial Intelligence; the CEO of a successful government-backed startup; and a former member of the National Security Council. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>We&#x2019;ve also adjusted our focus to more explicitly separate the trends in technology that we&#x2019;re focused on from their collective implications -- these can be found on our </span><a href=\"https://envision-conference.com/\"><span>website</span></a><span>. Additionally, this year&#x2019;s conference theme is Action, to reflect an increased concern with concrete positive results from both Envision&#x2019;s and our participants&#x2019; efforts and to fill what we see as the main gap in current discourse. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>We&#x2019;ve expanded from 150 participants to 200 and have already attracted many highly accomplished young leaders before we&#x2019;ve started publicizing the conference. Applicants in our early bird round include Ph.D candidates from top universities, founders of funded startups with up to 7 figures in revenue, winners of prestigious science competitions including Intel ISEF, founders of nonprofits with hundreds of members, student partners at VC firms, and other qualified applicants. The majority of applicants previously unaffiliated with Envision reported that they heard of it from a recommendation from a friend. We will begin full-scale marketing within the next week.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The theme of Action for Envision Conference 2017 reflects our increasing focus on providing measurable value: speakers will give concrete proposals on what we can collectively do or what participants can individually do to ensure the safe and beneficial development of technology.. Participants will have access to resources like speaker office hours, high-profile companies and startups seeking recruits, and entrepreneurial mentorship to help them take definite action on what they learn at Envision Conference. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>How EA can help</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>By continuing to grow our flagship conference, hosting trips to engage members of the Princeton community with industry leaders, and establishing another hub of Envision we&#x2019;re confident that, with the necessary resources, Envision can continue to have a high counterfactual impact on how future leaders develop, implement, and regulate technology. Our ability to effectively do these things is contingent upon finding support. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The easiest way effective altruists can support Envision is by helping us spread news of our conference to qualified participants. The </span><a href=\"http://envision-conference.com\"><span>application</span></a><span> is currently open and will close on October 9th. We welcome telling peers about our conference, posting on other forms about Envision, helping us reach relevant mailing lists, or other ways of spreading Envision. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Envision Conference benefits from partnerships with both nonprofits and corporate sponsors. If you&#x2019;re involved with or know of an institution that would be a good match to partner with Envision, please connect us. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The most impactful way you can support Envision is by contributing to our endowment. The endowment will enable us to run our events better, expand and better measure our impact, and ensure longevity. As I previously wrote, the endowment will be donated to EA if Envision no longer effectively accomplishes its mission, as determined by specific, measurable criteria. As an officially registered 501c3 your donations are tax deductible. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>If you&#x2019;d like to support Envision through any of these ways, I can be reached at ajs9 &lt; a t &gt; princeton ( d o t ) edu or by direct message. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Summary </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Envision is an undergraduate group working to fundamentally change how young leaders view research and innovation. We recognize that the difference between technology bringing human extinction and improving our lives will depend on how it&#x2019;s developed and implemented. We target young people likely to be leaders to maximize the impact we can have on how technology will be developed in the future, and work to cultivate a safety-conscious mindset.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>This post described what Envision has accomplished so far, what we&#x2019;ve learned about the best ways to carry out our mission, and how we&#x2019;re adjusting our strategy to maximize on the areas we&#x2019;re effective. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Envision has momentum and is at the point where we have the potential for a significant, lasting impact. Our ability to secure this impact will depend on if we are able to secure Envision&#x2019;s longevity. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>We welcome any feedback on Envision&#x2019;s plans or suggestions on the best way we can accomplish our mission. </span></p>\n<p><br><br></p></body></html>", "user": {"username": "aspencer"}}, {"_id": "uR6nqHS568dGfYKa2", "title": "The AIDS/malaria puzzle: bleg", "postedAt": "2017-09-26T15:55:37.077Z", "htmlBody": "<html><body><p>A few months ago, I <a href=\"https://www.facebook.com/photo.php?fbid=10211678488933677\">posted to Facebook</a> a puzzle about the difference in funding for AIDS and malaria. Here is the puzzle:</p>\n<ol>\n<li><strong>DAH spending for AIDS is much more than malaria</strong>:&#xA0;Development assistance for health (DAH) numbers from the Institute for Health Metrics and Evaluation (IHME)&#xA0;<a href=\"http://ihmeuw.org/426i\">show that</a> development assistance spending on AIDS significantly exceeds spending on malaria. For instance, if you click on the link and switch to &quot;Trends&quot; and &quot;Health focus&quot; you&apos;ll see that AIDS DAH spending in 2016 was estimated as $9.5 billion whereas malaria DAH spending was estimated as $2.5 billion. Most of this difference comes from government spending (breakdown by source also at the link). In fact, total annual spending by the US government on HIV/AIDS is <a href=\"http://www.kff.org/global-health-policy/fact-sheet/u-s-federal-funding-for-hivaids-trends-over-time/\">around $33 billion</a>, more than the National Institutes of Health (NIH) budget, though most of it is domestic spending.</li>\n<li><strong>Effective altruists and allied groups have focused much more on malaria than AIDS throughout their history</strong>: Malaria has been identified by GiveWell as a promising area <a href=\"http://blog.givewell.org/2006/12/28/dont-let-elie-rip-you-off/\">since 2006</a>, and the Against Malaria Foundation has been a GiveWell top-rated charity <a href=\"http://www.givewell.org/about/impact\">since 2011</a>, excepting one year. Giving What We Can has also recommended the Against Malaria Foundation since before it became a GiveWell top charity, and&#xA0;it has also been the poster boy of effective altruism for fundraising groups like The Life You Can Save and Charity Science. In contrast, HIV/AIDS hasn&apos;t been a major focus, with GiveWell getting around to reviewing a HIV/AIDS-related intervention only <a href=\"http://www.givewell.org/international/technical/programs/antiretroviral-therapy-to-treat-hiv-aids\">in 2017</a>.</li>\n<li><strong>Crude estimates of the toll of the two diseases paints a picture of fairly comparable impact</strong>: Malaria affects five or more times as many people as AIDS. But on the other hand, once you get AIDS, you are stuck with it, whereas you can usually recover from malaria in a few weeks. On the third hand,&#xA0;the agony per unit time of having AIDS is lower than that of malaria. The annual death toll of AIDS is about double that of malaria (a million versus 400,000), though estimates for both have huge error bars.</li>\n</ol>\n<p>Interestingly the Gates Foundation, which can be considered intermediate between a government donor and an &quot;effective altruist&quot;, has an AIDS/malaria spending split in between the two: it spends roughly equally on the two; see <a href=\"https://en.wikipedia.org/wiki/Bill_%26_Melinda_Gates_Foundation#Funds_for_grants_in_developing_countries\">breakdown of funds for grants in developing countries</a>.</p>\n<p>The tension between (1) and (2) is an interesting puzzle. It could be that:</p>\n<ul>\n<li>DAH spenders are wrong about their focus on AIDS, and in an ideal world would be directing more resources toward malaria.</li>\n<li>Effective altruists are wrong about their focus on malaria, and in an ideal world would be directing more resources toward HIV/AIDS.</li>\n<li>They are both right &quot;in their own way&quot;; HIV/AIDS spending is the right call to make for DAH spenders whereas malaria spending is the right thing to do for effective altruists. While the most conciliatory to all sides, this also demands the most explanation, since relativism challenges&#xA0;some of the implicit and explicit ideas of effective altruism.</li>\n</ul>\n<p>I have&#xA0;explored some more specific hypotheses in a <a href=\"https://www.facebook.com/vipulnaik.r/posts/10211678551295236?comment_id=10212596422721448\">comment on&#xA0;my Facebook post</a>, which I shall not repeat here for brevity.</p>\n<p>I&apos;ve already spent a fair amount of effort collating the history of malaria, including funding a bunch of malaria-related timelines such as <a href=\"https://timelines.issarice.com/wiki/Timeline_of_malaria\">timeline of malaria</a>, <a href=\"https://timelines.issarice.com/wiki/Timeline_of_mosquito_net_distribution\">timeline of mosquito net distribution</a>, <a href=\"https://timelines.issarice.com/wiki/Timeline_of_The_Global_Fund_to_Fight_AIDS%2C_Tuberculosis_and_Malaria\">timeline of the Global Fund</a>, <a href=\"https://timelines.issarice.com/wiki/Timeline_of_malaria_vaccine\">timeline of malaria vaccine</a>, <a href=\"https://timelines.issarice.com/wiki/Timeline_of_Against_Malaria_Foundation\">timeline of Against Malaria Foundation</a>, and timeline of malaria in <a href=\"https://timelines.issarice.com/wiki/Timeline_of_malaria_in_2014\">2014</a>, <a href=\"https://timelines.issarice.com/wiki/Timeline_of_malaria_in_2015\">2015</a>, <a href=\"https://timelines.issarice.com/wiki/Timeline_of_malaria_in_2016\">2016</a>, and <a href=\"https://timelines.issarice.com/wiki/Timeline_of_malaria_in_2017\">2017</a>. I intend to spend similar effort on HIV/AIDS, and return to the puzzle after that. However, I&apos;m curious about any thoughts readers here have on the puzzle, including whether you find it interesting, potential resolutions or directions to explore, or refutations of the premises of the puzzle.</p>\n<p><em>Thanks to Sebastian Sanchez and Issa Rice for working on the linked timelines. Thanks to Howie Lempel for commenting with thoughts on my original Facebook post. And thanks to IHME for collating Development Assistance for Health (DAH) spending, looking at which inspired this post.</em></p></body></html>", "user": {"username": "vipulnaik"}}, {"_id": "J49YemrTzXGtnri9n", "title": "EA Survey 2017 Series: Qualitative Comments Summary ", "postedAt": "2017-09-21T01:36:50.004Z", "htmlBody": "<html><body><p><img src=\"https://i.imgur.com/lSCiAYt.png?2\"></p>\n<blockquote>\n<p><span>The annual EA Survey is a volunteer-led project of</span>&#xA0;<a href=\"http://rtcharity.org/\"><span>Rethink Charity</span></a><span> that has become a benchmark for better understanding the EA community. This is the sixth article in our multi-part EA Survey 2017 Series.</span><span> You can find supporting documents at the bottom of this post, including prior EA surveys, and an up-to-date list of articles in the EA Survey 2017 Series. Get notified of the latest posts in this series by signing up</span>&#xA0;<a href=\"http://eepurl.com/c2MaW5\"><span>here</span></a><span>. </span></p>\n</blockquote>\n<h3><span>Could you, however loosely, be described as an &#x201C;Effective Altruist&#x201D;?</span></h3>\n<p><span>Several respondents support the underlying principles of the EA movement, but many suggested that they did not consider themselves part of the community because of their disagreement with some of the ideas, or their lack of donations to effective charities (often due to financial difficulties or perceived lack of commitment). Various respondents also seemed to view EA as a lofty, principle-based lifestyle that they had not yet attained and were therefore hesitant to label themselves &#x201C;effective altruists.&#x201D; A few comments suggested that the term &#x201C;effective altruist&#x201D; implied an underlying pretentiousness that respondents were unwilling to associate with.</span></p>\n<h3><span>If there was a local group near your home, would you attend?</span></h3>\n<p><span>For this question, people tended to respond in one of two ways: respondents in the first group tended to be active participants and/or leaders in their local EA group. Those that did not live in an area with a local EA group expressed interest in starting such a community. Respondents in the second group showed interest in attending occasional meetings. At the same time, these respondents also expressed some ambivalence about attending meetings. Distance and scheduling were common concerns; people also wanted to know how effective and structured the group meetings would be in reaching practical outcomes.</span></p>\n<h3><span>How welcoming do you find the EA community?</span></h3>\n<p><span>Responses varied widely based on the region and the particular forum being referenced. People generally commented that the online community feels off-putting to new members as the topics discussed are very specialized and members tend to be very well-informed. As a typical response went: &#x201C;</span><span>Sometimes the jargon and in depth conversations can be a bit alienating to someone without a philosophy or economics background</span><span>.&#x201D; Relating to this concern, a few respondents commented that it would be best to create a separate, more open space dedicated to bringing new members up to speed on EA ideas.</span></p>\n<p><span>Another common theme was that the EA community tends to attract members with similar ethnic, socioeconomic, and educational backgrounds. Respondents noted that the lack of diversity often made it difficult for those outside the demographic to feel comfortable in the EA community. </span></p>\n<h3><span>Do insecurities about not being &#x2018;EA enough&#x2019; sometimes prevent you from taking action or participating more in the EA community?</span></h3>\n<p><span>Many respondents expressed a certain degree of guilt for not having &#x201C;done enough&#x201D; as an effective altruist, especially when compared to more dedicated members of the EA community. This insecurity seems to largely be the result of internal sentiments (e.g. feeling that they do not have anything worthwhile to contribute), and at least partly attributable to a dynamic inside EA groups that does not fully accommodate new members.</span></p>\n<p><span>Others expressed satisfaction with their current level of giving and the extent to which they had embraced EA ideas in their daily life. </span></p>\n<h3><span>How can we improve the EA survey? </span></h3>\n<p><span>In this question, respondents highlighted four critical areas of improvement for the survey content. First, they were concerned that so many of the questions asked about donations and participants&#x2019; income. According to responses, these questions were tedious and reflected poorly on the nature of EA. Second, several respondents raised serious concerns that the multiple choice questions did not account for all possible answers; for instance, one person noted that the careers list did not include a &#x201C;retail&#x201D; option but did have a &#x201C;business&#x201D; and &#x201C;manual labor&#x201D; option, appearing to exclude individuals of lower income classes. These respondents suggested that more multiple choice questions include an option for &#x201C;other.&#x201D; Furthermore, responses noted that many of the questions did not distinguish between EA as a set of principles for doing good and the EA community. Finally, respondents consistently noted that the survey was much longer than advertised and actually took 30-45 min.</span></p>\n<p><span>Respondents also had specific complaints about the formatting of the survey. First, several voiced frustration that the positioning and color coding of the &#x201C;Exit &amp; Clear survey&#x201D; caused them to mistake it for the &#x201C;next&#x201D; button and accidentally delete their responses. Others noted that it would be very convenient, both for the respondents and the writers of the survey, to sync individuals&#x2019; data from the GWWC My Giving website, eliminating the need for all the questions about donations and income. The survey also caused some problems for active participants of the EA movement. For questions that gauged respondents&#x2019; interest in setting up an EAHub profile or subscribing to a newsletter, there was no option for those who had already completed these items. </span></p>\n<h3><span>How did you hear about this survey?</span></h3>\n<p><span>The vast majority of respondents heard about the survey via the Slate Star Codex blog and open threads. Respondents frequently recalled accessing the survey via Facebook group pages such as the GWWC Community page, the Effective Animal Advocacy Discussion page, local EA group pages, and the Dank EA Memes page. A significant number heard about the survey directly from EA-affiliated organizations, including 80000 Hours, Rethink Charity (formerly known as Dot Impact), Students for High-Impact Charity, and Giving What We Can; leaders of these organizations either sent out email newsletters with the survey link or directly contacted individuals with information about the survey.</span></p>\n<h3><span>Credits</span></h3>\n<p><span>Post written by June Lee, with edits from Tee Barnett and analysis from Peter Hurford.</span></p>\n<p><span>A special thanks to Ellen McGeoch, Peter Hurford, and Tom Ash for leading and coordinating the 2017 EA Survey. Additional acknowledgements include: Michael Sadowsky and Gina Stuessy for their contribution to the construction and distribution of the survey, Peter Hurford and Michael Sadowsky for conducting the data analysis, and our volunteers who assisted with beta testing and reporting: Heather Adams, Mario Beraha, Jackie Burhans, and Nick Yeretsian.</span></p>\n<p><span>Thanks once again to Ellen McGeoch for her presentation of the 2017 EA Survey results at EA Global San Francisco.</span></p>\n<p><span>We would also like to express our appreciation to the Centre for Effective Altruism, Scott Alexander via SlateStarCodex, 80,000 Hours, EA London, and Animal Charity Evaluators for their assistance in distributing the survey. Thanks also to everyone who took and shared the survey.</span></p>\n<h3><span>Supporting Documents</span></h3>\n<h3><span>EA Survey 2017 Series Articles</span></h3>\n<p><span>I -</span><a href=\"/ea/1e0/effective_altruism_survey_2017_distribution_and/\"><span> Distribution and Analysis Methodology</span></a></p>\n<p><span>II -</span><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"><span> Community Demographics &amp; Beliefs</span></a></p>\n<p><span>III -</span><a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\">&#xA0;<span>Cause Area Preferences</span></a></p>\n<p><span>IV -</span><a href=\"/ea/1el/ea_survey_2017_series_donation_data/\">&#xA0;<span>Donation Data</span></a></p>\n<p><span>V -</span><a href=\"/ea/1ex/demographics_ii/\">&#xA0;&#xA0;<span>Demographics II</span></a></p>\n<p><span>VI - <a href=\"/ea/1f5/ea_survey_2017_series_qualitative_comments_summary/\">Qualitative Comments Summary</a></span></p>\n<p><span><span>VII - </span><a href=\"/ea/1fi/have_ea_priorities_changed_over_time/\">Have EA Priorities Changed Over Time?</a></span></p>\n<p><span><span>VIII - </span><a href=\"/ea/1h5/ea_survey_2017_series_how_do_people_get_into_ea/\">How do People Get Into EA?</a></span></p>\n<p>&#xA0;</p>\n<p><span>Please note: this section will be continually updated as new posts are published. </span><span>All 2017 EA Survey posts will be compiled into a single report at the end of this publishing cycle</span></p>\n<h3><span>Prior EA Surveys conducted by Rethink Charity (formerly .impact)</span></h3>\n<p><a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\"><span>The 2015 Survey of Effective Altruists: Results and Analysis</span></a></p>\n<p><a href=\"/ea/gb/the_2014_survey_of_effective_altruists_results/\"><span>The 2014 Survey of Effective Altruists: Results and Analysis</span></a></p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Tee"}}, {"_id": "aYSJu3PzTaqNBRbXe", "title": "What do DALYs capture?", "postedAt": "2017-09-20T21:18:55.546Z", "htmlBody": "<html><body><h1><span>INTRODUCTION</span></h1>\n<p><span>There are many different causes that require our attention, but because our resources are limited, we need to decide which ones should go first. Within health, we use Health-Adjusted Life Years (HALYs) to help us decide which interventions to prioritize. Health is not the only determinant of wellbeing we care about. There may be value in building broader metrics that also encompass some of the other factors, but health is definitely an important one, so that is why it is be the focus of this article.</span></p>\n<p><span>HALYs capture morbidity and mortality: morbidity is how life with that disease compares to life in full health (the amount of life years left weighted by the severity of the disease or &#x201C;Years of Life Lived with Disability&#x201C;); and mortality is the number of years by which the patient&#x2019;s life has been shortened because of the disease, taking life expectancy as a reference (&#x201C;Years of Life Lost&#x201D;). </span></p>\n<p><span>Two of the most widely used types of HALYS are Quality-Adjusted Life Years (QALYs), and Disability-Adjusted Life Years (DALYs). They are conceptually very similar, but QALYs capture the benefits of health interventions, and hence we want as many of them as possible, whereas DALYs capture the losses caused by a health state, so we want to minimize them [1]</span><span>. QALYs are more widely used, but DALYs are more relevant here because they are the ones used in development, and hence we will focus on them. Here, &#x2018;disability weights&#x2019; will be used as a synonym for DALYs.</span></p>\n<p><span>There are several methods to elicit disability weights (e.g., standard gamble, visual analogue scale, person trade-off), &#xA0;but the most popular is the Time Trade-Off (TTO). Respondents are asked to think how many years in full health (x) are equivalent to a longer time (t) in a poor health state. Utility of full health is assigned to be 1, and the utility of the poor health state is then x/t. &#xA0;These questions are posed either to the members of the general population or to experts, when the former is not possible. Health states are described using instruments such as the EQ-5D (other examples include the SF-6D, and Health Utilities Index (HUI)). EQ-5D uses five dimensions to describe health states (mobility, self-care, usual activities, pain/discomfort, and anxiety/depression). Each dimension has three levels ((1) no problems, (2) some problems, (3) extreme problems). The digits for the five dimensions make up the score that describes the health state. For instance, the best possible health state would be represented by &#x201C;11111&#x201D;. &#xA0;</span></p>\n<p><span>DALYs are useful in that they help us make comparisons across health interventions, but they have important limitations too. Before we make decisions based on them, we should make sure that we also understand what they they may be misrepresenting or not capturing at all:</span></p>\n<ol>\n<li>\n<p><span>As a result of the elicitation process, DALYs may misrepresent the relative importance of mental compared to physical health.</span></p>\n</li>\n<li>\n<p><span>DALYs do not capture indirect effects of health interventions, and thus they could be missing a very important part of the picture.</span></p>\n</li>\n</ol>\n<h1><span>1) DALYs MISREPRESENT MENTAL HEALTH</span></h1>\n<p><span>We may miss an opportunity for increasing people&#x2019;s wellbeing if we do not think critically about how well DALYs capture the prevalence and impact of mental health with respect to physical health. There are two factors that contribute towards this misrepresentation: first, the types of questions people are asked; and second, the answers they give.</span></p>\n<p><span>The National Institute for Health Care Excellence (NICE) and other agencies recommend using EQ-5D as the instrument to elicit people&#x2019;s preferences over health states. Its dimension composition may not be a good reflection of what actually matters to people (Dolan, 2011). In particular, the fact that only 1 out of its 5 dimensions is explicitly about mental health, and that anxiety and depression are pooled together into one item. When we ask people about health with preference-based methods, we get one answer (&#x201C;physical functioning and pain matter as much to people, and sometimes more, as mental health when they are asked to risk death or trade off life years&#x201D;), whereas when we ask them directly about what we are interested in, their happiness, the picture we obtain is different (&#x201C;mental health and vitality appear to be most strongly associated with happiness, whilst physical functioning and pain are not so strongly associated with happiness&#x201D;). In short, &#x201C;the dimensions of health privileged by the EQ-5D and SF-6D may not be those that most affect people&#x2019;s lives&#x201D; (direct quotes from Dolan, 2011).</span></p>\n<p><span>The second factor is linked to the elicitation of disability weights. In order to estimate DALYs, we survey people and ask them to predict how different health states would be. This prediction is susceptible to affective forecasting errors, which affect the evaluation of physical and mental health differently.</span></p>\n<p><span>The </span><span>focusing illusion</span><span> makes people give more weight in their judgement to attributes that are more notable. When people think of their lives in their current health state and compare them with a life with an illness with salient physical symptoms, the ways in which these symptoms would affect their lives are easier to think of than they would be if they had a mental illness. This makes that physical health problems are judged to be worse than they actually are, and mental health problems are judged to be less bad than they actually are. Despite of people&#x2019;s predictions, there is evidence asymptomatic conditions such as hypertension are correlated with less happiness (Blanchflower &amp; Oswald, 2008). &#xA0;</span></p>\n<p><span>The </span><span>impact bias</span><span> makes people overestimate the length and intensity of future emotional states, and so they exaggerate how bad it will be in a certain health state.</span></p>\n<p><span>Simultaneously, they ignore that after a while, their happiness levels will go back to their pre-condition levels; this is known as </span><span>hedonic adaptation</span><span>. Dolan (2011) reviews evidence on this phenomenon and quotes a study by Hurst and colleagues (1994) where they found that people with either chronic health conditions or a physical disability showed &#x201C;considerable levels of adaptation to these conditions&#x201D;.</span></p>\n<p><span>However, mental health conditions are among the most resistant to adaptation. Dolan and Kahneman (2008) attribute this to the fact that these kinds of conditions are &#x201C;part-time experiences&#x201D;, in that they only affect wellbeing when attention is drawn to the limitation they impose, whereas mental health problems are &#x201C;full-time in their attention seeking and impact on our lives&#x201D;.</span></p>\n<p><span>In addition to this, people also underestimate how much, after becoming physically or functionally disabled, they would </span><span>adapt</span><span> (&#x201C;learning and acquiring new skills in order to regain functionality&#x201D;), </span><span>cope</span><span> (&#x201C;adjusting your expectations about your performance to reduce the gap between expected and actual functionality&#x201D;), and </span><span>adjust</span><span> (&#x201C;changing one&#x2019;s life plans so that those dimensions that are not affected by the disability become more important&#x201D;) (Solomon &amp; Murray, 2002, referenced by Brock &amp; Wikler, 2006).</span></p>\n<p><span>Because of all of this, assessments of physical health issues may be overstating how bad their impact on wellbeing is, compared to mental health issues, and so more resources will be devoted to their treatment and prevention, while mental illnesses may be under-catered for.</span></p>\n<h1><span>2) DALYs MISS INDIRECT EFFECTS</span></h1>\n<p><span>DALYs capture the direct health loss of caused by a given disease, but they may be underestimating its overall detrimental effects because they don&#x2019;t account for indirect effects. If we care about the effect of health interventions to the broader society, then DALYs, which focus on the effect to the individual, may not be giving us an accurate picture. Accounting for indirect effects may change the picture of which health interventions should be prioritized.</span></p>\n<p><span>In addition to the actual disease symptoms, other health problems may be alleviated too if the disease is treated. For instance, Miller, Paschall, and Svendsen (2008) found evidence that patients with co-morbidities that involve severe mental illnesses and another condition (such as heart disease) experience higher mortality ratios than their counterparts without the co-morbidities. Hence, treating one of those diseases could make the other one less bad. &#xA0;Also, the effects of some illnesses, with time, could also cascade and affect other dimensions of patients&#x2019; health, increasing its negative consequences. For instance, losing some physical functionality could impact vitality (these dimensions are part of the SF-6D instrument). </span></p>\n<p><span>Diseases can also impact the health, lifestyle, or economic prospects of people around the patient. If the disease is transmittable, not treating it increases the chances that more people will get the disease, and that would multiply its negative effects. Severe illnesses, such as Alzheimer&#x2019;s, can significantly alter the patient&#x2019;s family and friends&#x2019; lifestyles (Dolan, 2011). Also, when patients do not survive the disease, this causes a great amount of pain and suffering to the people who knew them. </span></p>\n<p><span>There are four ways through which improved health fosters economic development (World Bank, 1993). The first one has to do with opportunity costs: better health frees up the resources that would otherwise have been used to care for the patient. Second, better health translates into gains in worker productivity, who also miss less work days, and have increased chances of obtaining better-paying jobs. Third, when some diseases are controlled, people can exploit natural resources that were inaccessible beforehand. This was the case for some areas of Sri Lanka when malaria was tackled, and Uganda when river blindness was fought with insecticides and medication. Last, better health is translated into economic gains through education: school enrollment, ability to learn, and participation by girls will be higher. </span></p>\n<p><span>These indirect effects vary across regions with economic, ethical, cultural and social differences. For example, being blind in countries like Niger will impair your ability to make a living, and that could lead to malnutrition, and premature death. In the UK, on the other side, the first years may be difficult, but after that it would not affect other areas of your health or have such an impact in your life as it would in Niger.</span></p>\n<p><span>Not accounting for these differences make that DALYs underweight health losses in poor countries. First, for the same health intervention, people in poorer countries have more potential to benefit from the indirect effects. This is because &#x201C;they are typically most handicapped by ill health and [they are the ones] who stand to gain the most from the development of underutilized natural resources&#x201D; (World Bank, 1993). Second, if the intervention is not implemented, they are also the ones that have more to lose, as their income is mostly dependent on physical labour rather than cognitive abilities, and often they do not have a savings safety net to fall back on. &#xA0;And third, indirect health negative consequences are larger for them too: &#x201C;when a family&#x2019;s breadwinner becomes ill, other members of the household may at first cope by working harder themselves and by reducing consumption, perhaps even of food. Both adjustments can harm the health of the whole family&#x201D;.</span></p>\n<h1><span>OTHER ISSUES</span></h1>\n<p><span>The reason why DALYs are estimated by surveying members of the general population is that they are intended to reflect their preferences. However, DALYs have been criticised because they capture the benefit of health interventions but disregard how they are distributed across the affected population, which is something most people care about. Focusing on maximizing health in the aggregate but disregarding equity concerns can lead to distributions that look unacceptable to most people. An example of this is the Oregon case (Brock &amp; Wikler, 2003), where treating a very prevalent but low impact condition (performing 150 teeth capping) was seen as more valuable than giving an appendectomy, which is a life-saving intervention with a great impact to the person who receives it.</span></p>\n<p><span>QALYs and DALYs are slightly different in this. QALYs do not give preferential treatment to anyone depending on the severity of their illness or personal characteristics (such as age, sex, level of deprivation, or their role in society, and others). This, known as QALY egalitarianism, is considered to be fair because everyone gets the same opportunities. Distributing QALYs according to this principle can lead to QALY losses for some, but as long as they are compensated by QALY gains for others, there will be a net efficiency gain and society as a whole will be better off (Whitehead &amp; Ali, 2010). DALYs, on the other side, do favour people in some age groups by applying age discounting.</span></p>\n<p><span>In the 2006 edition of the Disease Control Priorities (DCP) report (Jamison et al., 2006), the age weights were &#x201C;zero at birth, ignoring health losses from still birth prior to live birth; reach a maximum at age 25; and decline almost to zero at advanced age&#x201D;. In the 2013 edition, which is the latest revision of DALYs, constant age weighting (treating all years alike) was used.</span></p>\n<p><span>There is little evidence that one way of discounting is better than the other one, but some people argue in favour of having some kind of discounting for the following two reasons. First, to account for the fact that quality of life may depend on age. Second, to reflect the effect of health improvements on others. In particular, the fact that individuals in their productive years usually have young and/or elderly people that depend on them emotionally, physically, and financially. This argument has been criticised because it discriminates individuals depending in their social and economic value to others. This criterion is not linked to health, and also, it would justify outcomes that most people would consider unfair. For instance, it would justify that between a rich and a poor patient with the same medical needs, treating the rich was prioritized because they are more socially productive than the poor.</span></p>\n<p><span>Another way of incorporating distributional concerns into DALYs would be to use time discounting. This would make benefits in the future less attractive and so it would give an advantage to &#x2018;present patients&#x2019; over &#x2018;future patients&#x2019;. The first argument in favour of doing this is consistency (treating benefits in the same way that we treat costs). Discounting is also supported in order to reflect general uncertainty about the future, opportunity costs, negative health effects that could cascade if the patients are not treated immediately, and people&#x2019;s time preferences (this argument has been contested by evidence of how time preferences vary depending on the elicitation method (Frederick, 2003), and the implications that discounting would have on our preference of the past over the present &#x2013; i.e., &#x201C;discounting time at a 1% rate [&#x2026;] a single day of Tutankhamen&#x2019;s life would have been more valuable than the entire lives of all 7,000,000,000 humans alive today put together&#x201D; (Ord and Wiblin, n.d.). And finally, discounting would avoid paradoxes such as the Keeler and Cretin Paradox (1983)</span><span> [2] and the infinite benefit of eradicating diseases, which would justify any finite cost [3]</span><span>.</span></p>\n<p><span>The main criticism to discounting is that it violates intergenerational justice. Is it ethical to confer less value to increasing someone&#x2019;s wellbeing just because it happens in the distant future rather than now? Another argument against discounting is that it systematically disadvantages programs with benefits that take time to be accrued (such as vaccination programs or unhealthy behaviour change &#x2013; i.e., start exercising now to not to get a coronary disease later on). And last, there is a concern that applying a discount factor would be double-discounting, given that some of the elicitation methods (TTO, for example</span><span>) are already capturing at least some of these uncertainties.</span></p>\n<p><span>The above is not an exhaustive discussion of all the criticism to DALYs, but is intended to give an overview of some of the points that are currently being debated. Alternative approaches to value health such as using wellbeing measures have been suggested as a solution to some of these problems</span><span>. </span></p>\n<h1><span>CONCLUSION</span></h1>\n<p><span>DALYs measure health. But they miss, or misjudge, some important factors. First, DALYs are biased towards physical health. The instruments used for eliciting them and affective forecasting errors cause mental health to be underrepresented. Second, DALYs fail to capture various indirect effects. These include indirect health consequences for the patient, consequences for people around them, and economic impacts. Some of these effects have a stronger effect in poorer countries, and that is also unaccounted, biasing DALYs towards richer countries. Alternative ways of valuing health (e.g., using wellbeing measures) are currently being explored. </span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p><span>[1] &#xA0;There are two other main differences between QALYs and DALYs. First, QALYs describe health states in terms of a few dimensions and DALYs describe specific diseases. This implies that QALYs can account for co-morbidities but DALYs cannot. Second, DALYs incorporate age discounting, but for QALYs that would need to be done in an additional step. DALYs assign a different value to a year of life extension of the same quality, depending on the age at which an individual receives it; specifically, life extension for individuals during their adult productive work years is assigned greater value than a similar period of life extension for infants and young children or the elderly (Brock &amp; Wikler, 2006).</span></p>\n<p><span>[2] &#xA0;&#x201C;If you were faced with the choice of spending X dollars now to achieve a certain health benefit, or investing it and spending it a year later, you should invest it because a year from now you&#x2019;ll have more money to spend and can achieve a greater benefit. But then why not delay two years, etc? The paradox is that infinite delay is called for by this logic. Discounting of future health benefits potentially solves the problem. You have more money to spend, but if future health benefits are valued less, you aren&#x2019;t necessarily getting more for your dollar by delaying.&#x201D;</span></p>\n<p><span><span><span>[3] </span><a href=\"https://www.givingwhatwecan.org/sites/givingwhatwecan.org/files/attachments/discounting-health2.pdf\"><span>Ord and Wiblin</span></a><span> say that this would technically only be true if humanity was expected to survive until infinity and never to come up with an alternative cure for smallpox. &#x201C;A more realistic benefit appraisal of this situation is that the vaccine would contribute to eradicate it earlier, rather than preventing it to be &#x201C;a menace for billions of years&#x201D;.</span></span></span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<h1><span>REFERENCES</span></h1>\n<p><span>Blanchflower, D. G., &amp; Oswald, A. J. (2008). Hypertension and happiness across nations. </span><span>Journal of health economics</span><span>, </span><span>27</span><span>(2), 218-233.</span></p>\n<p><span>Brazier, J., &amp; Tsuchiya, A. (2015). Improving cross-sector comparisons: going beyond the health-related QALY. </span><span>Applied health economics and health policy</span><span>, </span><span>13</span><span>(6), 557-565.</span></p>\n<p><span>Brock, D., &amp; Wikler, D. (2006). Ethical issues in resource allocation, research, and new product development. </span><span>Disease control priorities in developing countries</span><span>, </span><span>2</span><span>, 259-60.</span></p>\n<p><span>Dolan, P. (2011). Using happiness to value health. </span><span>London: Office of Health Economics</span><span>.</span></p>\n<p><span>Dolan, P., &amp; Kahneman, D. (2008). Interpretations of utility and their implications for the valuation of health. </span><span>The Economic Journal</span><span>, </span><span>118</span><span>(525), 215-234.</span></p>\n<p><span>Frederick, S. (2003). Measuring intergenerational time preference: Are future lives valued less?. </span><span>Journal of Risk and Uncertainty</span><span>, </span><span>26</span><span>(1), 39-53.</span></p>\n<p><span>Hurst, N.P., Jobanputra, M., Hunter, M., Lambert, M., Lockhead, A. and Brown, H. (1994) Validity of EuroQol&#x2014;a generic health status instrument&#x2014;in patients with rheumatoid arthritis. </span><span>Rheumatology. </span><span>33(7), 655&#x2013;662. </span></p>\n<p><span>Jamison, D. T.; Breman, J. G.; Measham, A. R.; Alleyne, G.; Claeson, M.; Evans, D. B.; Jha, P.; Mills, A.; Musgrove, P. (2006). </span><span>Disease Control Priorities in Developing Countries, Second Edition</span><span>. Washington, DC: World Bank and Oxford University Press. Retrieved from: </span><a href=\"https://openknowledge.worldbank.org/handle/10986/7242\"><span>https://openknowledge.worldbank.org/handle/10986/7242</span></a></p>\n<p><span>Keeler, E. B., &amp; Cretin, S. (1983). Discounting of life-saving and other nonmonetary effects. </span><span>Management science</span><span>, </span><span>29</span><span>(3), 300-306.</span></p>\n<p><span>Miller, B. J., Paschall III, C. B., &amp; Svendsen, D. P. (2008). Mortality and medical comorbidity among patients with serious mental illness. </span><span>Focus</span><span>, </span><span>6</span><span>(2), 239-245.</span></p>\n<p><span>Ord, T., &amp; Wiblin, R. (n.d.) Should we discount future health benefits when considering cost-effectiveness? Retrieved from: https://www.givingwhatwecan.org/sites/givingwhatwecan.org/files/attachments/discounting-health2.pdf</span></p>\n<p><span>Solomon, J. A., &amp; Murray, C. J. L. (2002). A conceptual framework for understanding adaptation, coping and adjustment in health state valuations. </span><span>Summary Measures of Population Health</span><span>, </span><span>11</span><span>.</span></p>\n<p><span>Whitehead, S. J., &amp; Ali, S. (2010). Health outcomes in economic evaluation: the QALY and utilities. </span><span>British medical bulletin</span><span>, </span><span>96</span><span>(1), 5-21.</span></p>\n<p><span>World Bank (1993). </span><span>World Development Report 1993: Investing in Health</span><span>. New York: Oxford University Press. Retrieved from </span><a href=\"https://openknowledge.worldbank.org/handle/10986/5976\"><span>https://openknowledge.worldbank.org/handle/10986/5976</span></a></p>\n<p><span>&#xA0;</span></p></body></html>", "user": {"username": "Danae_Arroyos"}}, {"_id": "S7EPTq2J5tvHkSFca", "title": "Getting Nuclear Policy Right Is Hard", "postedAt": "2017-09-19T01:00:14.519Z", "htmlBody": "<html><body><p><span>Earlier this month I made a fairly <a href=\"https://theconsequentialist.wordpress.com/2017/07/07/effective-vs-harmful-anti-war-activism-and-policy-part-2/\">long post on nuclear policy</a>&#xA0;with&#xA0;many considerations for how different technologies might upset or improve nuclear deterrence, and why epistemic humility is extremely valuable as some EAs begin to focus more on policy.<br><br>To centralize discussion of this, I think it is best to post comments on the post above here on the EA forum. The post would have been made here, but unfortunately there is a lot of html, which vastly improves the readability of the post via compression, but does not work on the EA Forum. Leave any comments you have below.<br><br><br><br></span></p></body></html>", "user": {"username": "Gentzel"}}, {"_id": "K8D8hmaCYFtvhg5ZJ", "title": "Can a Transparent Idea Directory reduce transaction costs of new ideas?", "postedAt": "2017-09-18T17:31:20.770Z", "htmlBody": "<html><body><p>Would&#xA0;a transparent idea directory&#xA0;enable refinement of good&#xA0;ideas into great&#xA0;ones, help great&#xA0;ideas find a team, all the while reducing&#xA0;the overall burden of transaction costs associated with considering new ideas?</p>\n<p>Ideas are&#xA0;a resource, like money, skills and time. If EA is more <a href=\"https://80000hours.org/2015/11/why-you-should-focus-more-on-talent-gaps-not-funding-gaps/\">talent constrained</a> than funding constrained, and <a href=\"https://80000hours.org/2015/11/why-you-should-focus-more-on-talent-gaps-not-funding-gaps/#better-mechanisms-for-coordinating-people-and-solving-talent-gaps\">better mechanisms for coordinating EA&apos;s are useful</a>, then it may be worth creating the requisite digital and personnel infrastructure to manage ideas.</p>\n<p>The basics of a &quot;Transparent Idea Directory&quot;:</p>\n<p><strong>1- Write up</strong>: An idea is submitted as a semi-formal proposal according to&#xA0;a template like that used for the <a href=\"https://www.effectivealtruism.org/grants/\">EA Grants applications</a>.&#xA0;</p>\n<p><strong>2- Formal Review</strong>: Proposals would be reviewed by a committee, probably comprising quantitative skills, deep knowledge of EA principles, and experience converting theory into action such as entrepreneurship. Proposals would be categorized as follows:</p>\n<p>&#xA0; I- Ready for implementation. These are extremely well considered ideas that support EA principles and have/will contribute good evidence for effectiveness.</p>\n<p>&#xA0; II- Worth refining. These are promising ideas that can be upgraded to type I with more background research, adjustments in strategy, etc.</p>\n<p>&#xA0; III- Back to the drawing board. These are well intentioned but miss the mark in an important way, perhaps&#xA0;an over-reliance on intuition or misinformation.</p>\n<p><strong>3- Community Review</strong>: Proposals would be posted in a venue where the community could not only review the content and comment, but also offer to form part of a team to launch the idea. This could be similar to <a href=\"https://cofounderslab.com/\">CoFoundersLab</a>, where the startup community can promote themselves as having a business idea looking for a team, or being a manager of investor looking for a project. For the most promising ideas, the committee could facilitate establishing the best team.</p>\n<p>Additional Features: Additional information could be displayed, such as indicating if an idea has a full team that has begun implementation, thereby reducing redundancy. The team could post additional requests about an idea under implementation, such as consultants for specific expertise or small pieces of research. Timelines could show how long an idea has been worked on. Upon completion, reports on success or failure could be attached. &#xA0;</p>\n<p><strong>Limitations:</strong></p>\n<p>Clearly, establishing a formal review committee and developing an online directory with the features I&apos;ve laid out would require a lot of work. The committee would likely need to be several people and consume considerable time verifying proposed data&#xA0;and background information. This may be streamlined by having a single person triage proposals, and a committee only required to review the most promising.&#xA0;The proposed website would need to be fairly sophisticated to handle the multiple inputs, and would likely need nearly constant updating. The transparency could stoke&#xA0;intellectual property disputes which may consume time settling such disputes. Fortunately, these problems would only arise if the project was successful, a victim of its own success, thereby warranting the necessary attention.</p>\n<p><strong>Possibilities:</strong></p>\n<p>For individuals, this directory could serve as a&#xA0;portfolio of EA of one&apos;s work as an idea person, as a doer, as a funder, etc. For the EA community it could serve as a data pool for researching&#xA0;the common features of effective ideas, showcasing past successes and learning from failures.</p>\n<p><strong>Discussion:&#xA0;</strong></p>\n<p>Too many ideas and not enough doers increases the likelihood that doers will settle on weak ideas. Put another way, new ideas present transaction costs to doers, and more new ideas are not necessarily better if the number of doers is saturated, they only gum up the works. In this scenario, it makes sense to invoke the expectation that if you think you have a great idea, start doing it (on a super small scale akin to <a href=\"https://80000hours.org/2013/07/your-career-is-like-a-startup/\">The Lean Startup</a>). If results are favorable, then it&apos;s probably worth a high-impact doer&apos;s attention to determine if it&apos;s ready for prime time.</p>\n<p>This fits with the&#xA0;natural expectation that the person responsible for the idea should also be responsible for executing it, and &quot;idea people&quot; often do execute their own vision. However, this expectation sets up an unfortunate asymmetry, where idea people are considered a waste if they don&apos;t also execute their ideas. They&#xA0;get criticized for lacking dedication or follow through, and there is unspoken sense that it&#xA0;would be better that an idea without follow through was never voiced in the first place (ie- transaction costs). In the end, idea people can get discouraged (!) from coming up with ideas at all.</p>\n<p>This thinking makes sense&#xA0;in a capitalist society, but is unfounded in a community that is trying to maximize good (EA is essentially dedicated to figuring out which ideas are the best and then working only on them). Furthermore, the character traits that tend to produce&#xA0;good ideas (ie- creative dreamers) are not the traits that tend to produce results (ie- hard work and skepticism). A transparent idea directory could break this, enabling idea people to focus on developing good ideas, helping&#xA0;the best ideas to float to the top, and then connecting more good ideas with doers.&#xA0;&#xA0;</p>\n<p>Finally, the main goal of a transparent idea directory is to reduce the unavoidable transaction costs of new ideas. The investment&#xA0;needed to maximize idea management may ultimately reduce the transaction costs that are currently distributed across the community.</p></body></html>", "user": {"username": "astupple"}}, {"_id": "Em7kSysc3STrH9HhQ", "title": "EA Survey 2017 Series: Demographics II", "postedAt": "2017-09-18T15:51:47.118Z", "htmlBody": "<html><body><p><img src=\"https://i.imgur.com/lSCiAYt.png?2\"></p>\n<p><span>By: Katie Gertsch and Tee Barnett</span></p>\n<p><strong>&#xA0;</strong></p>\n<blockquote>\n<p><span>The annual EA Survey is a volunteer-led project of</span><a href=\"http://rtcharity.org/\"> <span>Rethink Charity</span></a><span> that has become a benchmark for better understanding the EA community. This is the fifth article in our multi-part EA Survey 2017 Series.</span><span> You can find supporting documents at the bottom of this post, including our previous piece on </span><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"><span>community demographics</span></a><span>, prior EA surveys, and an up-to-date list of articles in the EA Survey 2017 Series. Get notified of the latest posts in this series by signing up</span><a href=\"http://eepurl.com/c2MaW5\"> <span>here</span></a><span>. </span></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>This article brings EA demographics back by popular demand. As in, demand for the metrics not covered in the previous </span><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"><span>post</span></a><span>. We hope you enjoy this second look.</span></p>\n<h3><span>Race</span>&#xA0;</h3>\n<p><span>The survey respondents identified as white by a wide majority. Among the 1,069 who self-identified regarding race, 88.9% identified as white, 0.7% identified as black, 3.3% identified as hispanic, 7.0% identified as asian, and 621 respondents preferred not to answer the question. It was possible to identify with as many races as one wanted, but only 3.59% answered &#x2018;Yes&#x2019; to self-identify as more than one race, and only one person (0.09%) identified with three races.</span></p>\n<p><span>&#xA0;</span></p>\n<p><img src=\"https://i.imgur.com/lOjTlcM.png?2\"></p>\n<p><span>While diversity comes in many forms, especially in a definitional sense, EA is unlikely to be characterized as racially diverse according to this survey. There may be considerable margin for error in these findings, not the least because such a large proportion of respondents did not answer. But the trope of EA being a predominantly white (89%) and </span><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"><span>male (70.1%)</span></a><span> community, however, is not likely to fade anytime soon without directed effort. </span></p>\n<p>&#xA0;</p>\n<p><span>A longitudinal analysis of the community&#x2019;s racial composition cannot be conducted because no data on race was gathered in the 2015 survey. </span></p>\n<p>&#xA0;</p>\n<p><span>Want to contribute more to this discussion? We recommend joining the Diversity &amp; Inclusion in EA </span><a href=\"https://www.facebook.com/groups/diversityEA/\"><span>group</span></a><span> on Facebook. </span></p>\n<h3><span>Race and Geographic Location</span></h3>\n<p><span><span>A crosstab of declared racial identity according to location revealed a vast white majority across the </span><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"><span>top five EA hubs</span></a><span> around the world. New York City emerged as the most racially diverse EA hub in the community. This was statistically significant with p = 0.02, but it&#x2019;s not clear how much we can read into this. </span></span></p>\n<p><img src=\"https://i.imgur.com/YCDeaX9.png?1\"></p>\n<h3><span>Politics</span></h3>\n<p><span>Left-leaning EAs composed 64.8% of respondents, while &#x2018;Centre&#x2019; (8.1%), &#x2018;Centre Right&#x2019; and &#x2018;Right&#x2019; (3.3%) accounted for a considerably smaller portion of the sample. Libertarian EAs constitute a sizeable proportion of the sample (8.7%) &#xA0;a small group (6%) explicitly chose not to answer, and 9% refused to identify with any of the political spectrum. These percentages do not include the 785 people who took the survey but did not answer this question.</span></p>\n<p>&#xA0;</p>\n<p><img src=\"https://i.imgur.com/Mm0qgIm.png?1\"></p>\n<p><span><span>Data on political preference was collected but not published in the 2015 EA Survey </span><a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\"><span>report</span></a><span>, allowing us in 2017 to present longitudinal data on community-wide shifts in political orientation. </span></span></p>\n<p><img src=\"https://i.imgur.com/2x22zzh.png?1\"></p>\n<p><span>From 2015 to 2017, the survey indicates a slight shift away from the political left in the EA community. The tables above show 27.27% of the 2015 &#x2018;Left&#x2019; moved to the &#x2018;Centre Left&#x2019;, and 5.88% of the &#x2018;Centre Left&#x2019; went &#x201C;Centre&#x201D;. There was also some polarization, as 46.15% of the &#x201C;Centre&#x201D; moved &#x201C;Centre Left&#x201D;.</span></p>\n<p>&#xA0;</p>\n<p><span>Want to contribute more to this discussion? We recommend joining the Effective Altruists Discuss Politics </span><a href=\"https://www.facebook.com/groups/1521136334570384/\"><span>group</span></a><span> on Facebook.</span></p>\n<h3><span>Politics and cause area preference</span></h3>\n<p><span>When looking at the relationship between politics and other areas, we broke down political orientation into whether someone identified with the &#x201C;Left&#x201D; (i.e. they said they were &#x201C;Left&#x201D; or &#x201C;Centre Left&#x201D;) or did not identify with the left (i.e., they picked a different option like, &#x201C;Centre&#x201D;, &#x201C;Centre Right&#x201D;, &#x201C;Right&#x201D;, &#x201C;Libertarian&#x201D;). &#x201C;Other&#x201D; and &#x201C;Prefer not to answer&#x201D; were dropped from this variable. We found 682 respondents who were associated with a left-leaning position (left), 212 respondents who were not associated with a left-leaning position (non-left), and 943 people with no position. </span></p>\n<p>&#xA0;</p>\n<p><span>A crosstab of political orientation and cause area preference revealed that individuals on the left are more likely to be interested in politics (28% of people on the left rate politics as a top or &quot;near top&quot; cause, compared to 22% of people not on the left), poverty (78% of people on the left rate poverty as a top or &quot;near top&quot; cause, compared to 72% of people not on the left), animal welfare (41% of people on the left say animal welfare is top or near top compared to only 28% of the non-left), and environmentalism (42% of people on the left say environmentalism is top or &quot;near top&quot;. compared to 21% of non-left). </span></p>\n<p>&#xA0;</p>\n<p><span>Conversely, people on the left are less likely to care about AI (42% of people on the left rate AI as top or &quot;near top&quot; compared to 47% of people not on the left).</span></p>\n<h3><span>Politics and geographic location </span></h3>\n<p><span>Despite the San Francisco Bay Area being anecdotally associated with libertarians, it had the highest amount of people identifying with the left, with 82.9% of Bay Area respondents. Of the other five largest EA cities, London was 80.85% left, Oxford &#xA0;was 76.92% left and Boston was 73.53% left, and New York City was 63.64% left. However, despite these percentages of left appearing quite different, there was no statistically significant trend in left vs. non-left that we could pick up in our data.</span></p>\n<p>&#xA0;</p>\n<h3><span>Politics and dietary habits</span></h3>\n<p><span>Results show a significant difference according to political affiliation, where 48.9% on the left identified as vegetarian or vegan, while only 29% on the non-left did. </span></p>\n<p>&#xA0;</p>\n<p><span>This makes sense in the light of the above, looking at politics and cause area preference, where we see a significantly greater proportion (41%) of people on the left putting a high priority on animal welfare, compared to a smaller proportion sharing that level of priority from those on the non-left (28%).</span></p>\n<h3><span>Age and cause area preference</span></h3>\n<p><span>Using the median age of 27 as a dividing point, those below the median &#xA0;grouped as &#x2018;younger&#x2019; and those above the median as &#x2018;older&#x2019;, we compared cause area preference in these two groups. The group younger than the median age showed a preference for AI (53.1% compared to 37.9% of older people) and less of a preference for poverty (72% vs. 78% of older people). </span></p>\n<h3><span>Employment status</span></h3>\n<p><span><span><span>Employment status responses were lead by for-profit work (43.7%) and non-profit organizations (17.0%). There were similar numbers for self-employed (9.5%) and academics work (9.6%). Unemployed respondents made up 7.7%, while 6.8% reported working for a government entity, and 1.2% were homemakers. Those who are financially independent, through savings, passive income or a providing partner accounted for 4.6%. </span></span></span></p>\n<p><img src=\"https://i.imgur.com/8zc2Txs.png?1\"></p>\n<h3><span>Field of study</span></h3>\n<p><span>Respondents were allowed to select more than one field of study. Most popular fields among EA&#x2019;s, by a significant margin, proved to be computer science (18.9%) and maths (16.1%). Following that, philosophy (9.9%), other sciences (9.2%), social sciences (8.6%) and economics (8.4%). Less often chosen were the fields of humanities (7.1%), engineering (6.9%), physics (6.7%) and finally medicine (2.8%). </span></p>\n<p><img src=\"https://i.imgur.com/TLYPQTT.png?1\"></p>\n<h3><span>Year joined EA</span></h3>\n<p><span>Pardoning 2017 for being the current year, the last few years appear to have been strong for EA recruitment, though there may also be a survivorship bias with EAs who joined in previous years no longer identifying with EA or take the EA survey. Post-2013, we see double-digit percentage growth in the number self-identified EAs joining the community. </span></p>\n<p>&#xA0;</p>\n<p><span>Some additional metrics on &#xA0;EA movement growth from Peter Hurford and Joey Savoie is available in </span><a href=\"/ea/1ef/is_ea_growing_some_ea_growth_metrics_for_2017/\"><span>&#x201C;Is EA Growing? Some EA Growth Metrics for 2017&#x201D;</span></a><span>.</span></p>\n<p><img src=\"https://i.imgur.com/kbxiNgE.png?1\"></p>\n<h3><span>Credits</span></h3>\n<p><span>Post written by Katie Gertsch and Tee Barnett, with edits and analysis from Peter Hurford.</span></p>\n<p>&#xA0;</p>\n<p><span>A special thanks to Ellen McGeoch, Peter Hurford, and Tom Ash for leading and coordinating the 2017 EA Survey. Additional acknowledgements include: Michael Sadowsky and Gina Stuessy for their contribution to the construction and distribution of the survey, Peter Hurford and Michael Sadowsky for conducting the data analysis, and our volunteers who assisted with beta testing and reporting: Heather Adams, Mario Beraha, Jackie Burhans, and Nick Yeretsian.</span></p>\n<p>&#xA0;</p>\n<p><span>Thanks once again to Ellen McGeoch for her presentation of the 2017 EA Survey results at EA Global San Francisco.</span></p>\n<p>&#xA0;</p>\n<p><span>We would also like to express our appreciation to the Centre for Effective Altruism, Scott Alexander via SlateStarCodex, 80,000 Hours, EA London, and Animal Charity Evaluators for their assistance in distributing the survey. Thanks also to everyone who took and shared the survey.</span></p>\n<h3><span>Supporting Documents</span></h3>\n<h3><span>EA Survey 2017 Series Articles</span></h3>\n<p><span>I -</span><a href=\"/ea/1e0/effective_altruism_survey_2017_distribution_and/\"><span> Distribution and Analysis Methodology</span></a></p>\n<p><span>II -</span><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"><span> Community Demographics &amp; Beliefs</span></a></p>\n<p><span>III -</span><a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\"> <span>Cause Area Preferences</span></a></p>\n<p><span>IV -</span><a href=\"/ea/1el/ea_survey_2017_series_donation_data/\"> <span>Donation Data</span></a></p>\n<p><span>V - <a href=\"/ea/1ex/demographics_ii/\">Demographics II</a></span></p>\n<p><span>VI - </span><a href=\"/ea/1f5/ea_survey_2017_series_qualitative_comments_summary/\">Qualitative Comments Summary</a></p>\n<p><span>VII - </span><a href=\"/ea/1fi/have_ea_priorities_changed_over_time/\">Have EA Priorities Changed Over Time?</a></p>\n<p><span>VIII - </span><a href=\"/ea/1h5/ea_survey_2017_series_how_do_people_get_into_ea/\">How do People Get Into EA?</a></p>\n<p><span>Please note: this section will be continually updated as new posts are published. </span><span>All 2017 EA Survey posts will be compiled into a single report at the end of this publishing cycle</span></p>\n<h4><span>Prior EA Surveys conducted by Rethink Charity (formerly .impact)</span></h4>\n<p><a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\"><span>The 2015 Survey of Effective Altruists: Results and Analysis</span></a></p>\n<p><a href=\"/ea/gb/the_2014_survey_of_effective_altruists_results/\"><span>The 2014 Survey of Effective Altruists: Results and Analysis</span></a></p></body></html>", "user": {"username": "Tee"}}, {"_id": "MCfa6PaGoe6AaLPHR", "title": "S-risk FAQ", "postedAt": "2017-09-18T08:05:39.850Z", "htmlBody": "<html><body><p><em>The idea that the future might contain astronomical amounts of suffering, and that we <a href=\"https://foundational-research.org/s-risks-talk-eag-boston-2017/\">should work to prevent</a>&#xA0;such worst-case outcomes<strong>,</strong> has lately <a href=\"http://lesswrong.com/lw/p5v/srisks_why_they_are_the_worst_existential_risks/\">attracted some attention</a>. I&apos;ve written this FAQ to help clarify the concept&#xA0;and to clear up potential misconceptions.</em></p>\n<p><em>[Crossposted from <a href=\"http://s-risks.org/faq/\">my website on s-risks</a>.]</em></p>\n<h2>General questions</h2>\n<h3><strong>What are s-risks?</strong></h3>\n<p>In the essay <a href=\"https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/\">Reducing Risks of Astronomical Suffering: A Neglected Priority</a>, s-risks (also called suffering risks or risks of astronomical suffering) are defined as &#x201C;events that would bring about suffering on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far&#x201D;.</p>\n<p>If you&#x2019;re not yet familiar with the idea, you can find out more by watching <a href=\"https://foundational-research.org/s-risks-talk-eag-boston-2017/\">Max Daniel&apos;s EAG Boston talk</a> or by reading the <a href=\"http://prioritizationresearch.com/s-risks-introduction/\">introduction to s-risks</a>.</p>\n<h3><strong>Can you give an example of what s-risks could look like?</strong></h3>\n<p>In the future, it may become possible to run such complex simulations that the (artificial) individuals inside these simulations are sentient. Nick Bostrom coined the term <strong><a href=\"https://arbital.com/p/mindcrime/\"><em>mindcrime</em></a></strong> for the idea that the thought processes of a superintelligent AI might cause intrinsic moral harm if they contain (suffering) simulated persons. Since there are <a href=\"https://wiki.lesswrong.com/wiki/Basic_AI_drives\">instrumental reasons</a> to run many such simulations, this could lead to vast amounts of suffering. For example, an AI might use simulations to improve its knowledge of human psychology or to predict what humans would do in a conflict situation.</p>\n<p>Other common examples include <a href=\"https://foundational-research.org/a-dialogue-on-suffering-subroutines/\">suffering subroutines</a> and <a href=\"http://reducing-suffering.org/will-space-colonization-multiply-wild-animal-suffering/\">spreading wild animal suffering to other planets</a>.</p>\n<h3><strong>Isn&#x2019;t all that rather far-fetched?</strong></h3>\n<p>At first glance, one could get the impression that s-risks are just unfounded speculation. But to dismiss s-risks as unimportant (in expectation), one would have to be highly confident that their probability is negligible, which is hard to justify upon reflection. The <a href=\"http://prioritizationresearch.com/s-risks-introduction/#S-risks_are_not_extremely_unlikely\">introduction to s-risks</a> gives several arguments why the probability is not negligible after all:</p>\n<blockquote>\n<p>First, s-risks are disjunctive. They can materialize in any number of unrelated ways. Generally speaking, it&#x2019;s hard to predict the future and the range of scenarios that we can imagine is limited. It is therefore plausible that unforeseen scenarios &#x2013; known as <a href=\"https://en.wikipedia.org/wiki/Black_swan_theory\">black swans</a> &#x2013; make up a significant fraction of s-risks. So even if any particular dystopian scenarios we can conceive of is highly unlikely, the probability of <em>some</em> s-risk may still be non-negligible.</p>\n<p>Second, while s-risks may seem speculative at first, all the underlying assumptions are plausible. [...]</p>\n<p>Third, historical precedents do exist. Factory farming, for instance, is structurally similar to (incidental) s-risks, albeit smaller in scale. In general, humanity has a <a href=\"https://en.wikipedia.org/wiki/Chemical_weapon#International_law_on_chemical_weapons\">mixed</a> <a href=\"https://en.wikipedia.org/wiki/Great_Leap_Forward\">track</a><a href=\"https://en.wikipedia.org/wiki/Nuclear_weapon#Governance.2C_control.2C_and_law\"> record</a> regarding responsible use of new technologies, so we can hardly be certain that future technological risks will be handled with appropriate care and consideration.</p>\n</blockquote>\n<h3><strong>Which value systems should care about reducing s-risks?</strong></h3>\n<p>Virtually everyone would agree that (involuntary) suffering should, all else equal, be avoided. In other words, ensuring that the future does not contain astronomical amounts of suffering is a common denominator of almost all (plausible) value systems.</p>\n<p>Work on reducing s-risks is, therefore, a good candidate for <a href=\"https://foundational-research.org/gains-from-trade-through-compromise/\">compromise between different value systems</a>. Instead of narrowly pursuing our own ethical views in potential conflict with others, we should work towards a future deemed favourable by many value systems.</p>\n<h2>The future</h2>\n<h3><strong>Aren&apos;t future generations in a much better position to do something about this? </strong></h3>\n<p>Future generations will probably have more information about s-risks in general, including which ones are the most serious, which does give them the upper hand in finding effective interventions. One might, therefore, argue that later work has a significantly higher marginal impact. However, there are also arguments for working on s-risks now.</p>\n<p>First, thinking about s-risks only as they start to materialize does not suffice because it might be too late to do anything about it. Without sufficient foresight and caution, society may already be &#x201C;locked in&#x201D; to a trajectory that ultimately leads to a bad outcome.</p>\n<p>Second, one main reason why future generations are in a better position is that they can draw on previous work. Earlier work &#x2013;&#xA0;especially research or conceptual progress &#x2013; can be effective in that it allows future generations to more effectively reduce s-risk.</p>\n<p>Third, even if future generations are <em>able</em> to prevent s-risks, it&#x2019;s not clear whether they will <em>care enough</em> to do so. We can work to ensure this by growing a movement of people who want to reduce s-risks. In this regard, we should expect earlier growth to be more valuable than later growth.</p>\n<p>Fourth, if there&#x2019;s a sufficient probability that smarter-than-human AI <a href=\"http://aiimpacts.org/ai-timeline-surveys/\">will be built in this century</a>, it&apos;s possible that we <em>already are</em> in a unique position to influence the future. If it&#x2019;s possible to <a href=\"https://intelligence.org/2015/07/20/why-now-matters/\">work productively on AI safety now</a>, then it should also be possible to reduce s-risks now.</p>\n<p>Toby Ord&#x2019;s essay <a href=\"http://www.fhi.ox.ac.uk/the-timing-of-labour-aimed-at-reducing-existential-risk/\">The timing of labour aimed at reducing existential risk</a> addresses the same question for efforts to reduce x-risks. He gives two additional reasons in favor of earlier work: namely, the possibility of changing course (which is more valuable if done early on) and the potential for self-improvement.</p>\n<h3><strong>Seeing as humans are (at least somewhat) benevolent and will have advanced technological solutions at their disposal, isn&#x2019;t it likely that the future will be good anyway?</strong></h3>\n<p>If you are (very) optimistic about the future, you might think that s-risks are unlikely for this reason (which is different from the objection that s-risks seem far-fetched). A <a href=\"/ea/1cl/an_argument_for_why_the_future_may_be_good/\">common argument</a> is that avoiding suffering will become easier with more advanced technology; since humans care at least a little bit about reducing suffering, there will be less suffering in the future.</p>\n<p>While this argument has some merit, it&#x2019;s not airtight. By default, when we humans encounter a problem in need of solving, we tend to implement the most economically efficient solution, often irrespective of whether it involves large amounts of suffering. Factory farming provides a good example of such a mismatch; faced with the problem of producing meat for millions of people as efficiently as possible, a solution was implemented which happened to involve an immense amount of nonhuman suffering.</p>\n<p>Also, the future will likely contain vastly larger populations, especially if humans colonize space at some point. All else being equal, such an increase in population may also imply (vastly) more suffering. Even if the <em>fraction</em> of suffering decreases, it&apos;s not clear whether the <em>absolute</em> amount will be higher or lower.</p>\n<p>If your <a href=\"https://foundational-research.org/the-case-for-suffering-focused-ethics/\">primary goal is to reduce suffering</a>, then your actions matter less if the future will &apos;automatically&apos; be good (because the future contains little or no suffering anyway). Given sufficient uncertainty, this is reason to focus on the possibility of bad outcomes anyway for precautionary reasons. In a world where s-risks are likely, we can have more impact.</p>\n<h3><strong>Does it only make sense to work on s-risks if one is very pessimistic about the future?</strong></h3>\n<p>Although the degree to which we are optimistic or pessimistic about the future is clearly relevant to how concerned we are about s-risks, one would need to be unusually optimistic about the future to rule out s-risks entirely.</p>\n<p>From the <a href=\"http://prioritizationresearch.com/s-risks-introduction/#S-risks_are_not_extremely_unlikely\">introduction to s-risks</a>:</p>\n<blockquote>\n<p>Working on s-risks does <em>not </em>require a particularly pessimistic view of technological progress and the future trajectory of humanity. To be concerned about s-risks, it is sufficient to believe that the probability of a bad outcome is <em>not negligible</em>, which is consistent with believing that a utopian future free of suffering is <em>also</em> quite possible.</p>\n</blockquote>\n<p>In other words, being concerned about s-risks does not require unusual beliefs about the future.</p>\n<h2>S-risks and x-risks</h2>\n<h3><strong>How do s-risks relate to existential risks (x-risks)? Are s-risks a subclass of x-risks?</strong></h3>\n<p>First, recall <a href=\"https://nickbostrom.com/existential/risks.html\">Nick Bostrom&#x2019;s definition of x-risks</a>:</p>\n<blockquote>\n<p>Existential risk &#x2013; One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.</p>\n</blockquote>\n<p>S-risks are <a href=\"https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/#I_Introduction\">defined as follows</a>:</p>\n<blockquote>\n<p>S-risks are events that would bring about suffering on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.</p>\n</blockquote>\n<p>According to these definitions, both x-risks and s-risks relate to <a href=\"http://www.academia.edu/5005462/On_the_Overwhelming_Importance_of_Shaping_the_Far_Future_PhD_Thesis_\">shaping the long-term future</a>, but reducing x-risks is about actualizing humanity&#x2019;s potential, while reducing s-risks is about preventing bad outcomes.</p>\n<p>There are two possible views on the question of whether s-risks are a subclass of x-risks.</p>\n<p>According to one possible view, it&#x2019;s conceivable to have astronomical amounts of suffering that do not lead to extinction or curtail humanity&#x2019;s potential. We could even imagine that some forms of suffering (such as <a href=\"https://foundational-research.org/a-dialogue-on-suffering-subroutines/\">suffering subroutines</a>) are instrumentally useful to human civilization. Hence, not all s-risks are also x-risks. In other words, some possible futures are both an x-risk and an s-risk (e.g. uncontrolled AI), some would be an x-risk but not an s-risk (e.g. an empty universe), some would be an s-risk but not an x-risk (e.g. suffering subroutines), and some are neither.</p>\n<table>\n<tbody>\n<tr>\n<td>&#xA0;</td>\n<td>&#xA0;</td>\n<td colspan=\"2\"><strong>S-risk?</strong></td>\n</tr>\n<tr>\n<td>&#xA0;</td>\n<td>&#xA0;</td>\n<td><strong>Yes</strong></td>\n<td><strong>No</strong></td>\n</tr>\n<tr>\n<td rowspan=\"2\"><strong>X-risk?</strong></td>\n<td><strong>Yes</strong></td>\n<td><em>Uncontrolled AI</em></td>\n<td><em>Empty universe</em></td>\n</tr>\n<tr>\n<td><strong>No</strong></td>\n<td><em>Suffering subroutines</em></td>\n<td><em>Utopian future</em></td>\n</tr>\n</tbody>\n</table>\n<p>The second view is that the meaning of &#x201C;potential&#x201D; depends on your values. For example, you might think that a cosmic future is only valuable if it does not contain (severe) suffering. If &#x201C;potential&#x201D; refers to the potential of a utopian future without suffering, then every s-risk is (by definition) an x-risk, too.</p>\n<h3><strong>How do I decide whether reducing extinction risks or reducing s-risks is more important? </strong></h3>\n<p>This depends on each of us making difficult ethical judgment calls. The answer depends on how much you care about reducing suffering versus increasing happiness, and how you would make tradeoffs between the two. (This also raises <a href=\"https://foundational-research.org/measuring-happiness-and-suffering/\">fundamental questions</a> about how happiness and suffering can be measured and compared.)</p>\n<p>Proponents of <a href=\"https://foundational-research.org/the-case-for-suffering-focused-ethics/\">suffering-focused ethics</a> argue that the reduction of suffering is of primary moral importance, and that additional happiness cannot easily counterbalance (severe) suffering. According to this perspective, preventing s-risks is morally most urgent.</p>\n<p>Other value systems, such as <a href=\"https://en.wikipedia.org/wiki/Utilitarianism#Classical_utilitarianism\">classical utilitarianism</a> or <a href=\"http://lesswrong.com/lw/xy/the_fun_theory_sequence/\">fun theory</a>, emphasize the creation of happiness or other forms of positive value, and assert that the <a href=\"https://nickbostrom.com/astronomical/waste.html\">vast possibilities of a utopian future</a> can outweigh s-risks. Although preventing s-risks is still valuable in this view, it is nevertheless considered even more important to ensure that humanity has a cosmic future at all by reducing extinction risks.</p>\n<p>In addition to normative issues, the answer also depends on the empirical question of how much happiness and suffering the future will contain. David Althaus suggests that we consider both the <em>normative suffering-to-happiness</em> <em>trade ratio</em> <em>(NSR),</em> which measures how we would trade off suffering and happiness in theory, and the <em>expected suffering-to-happiness ratio</em> (<em>ESR</em>), which measures the (relative) amounts of suffering and happiness we expect in the future.</p>\n<p>In this framework, those who emphasize happiness (low NSR) or are optimistic about the future (low ESR) will tend to focus on extinction risk reduction. If the product of NSR and ESR is high &#x2013; either because of a normative emphasis on suffering (high NSR) or pessimistic views about the future (high ESR) &#x2013; it&#x2019;s more plausible to focus on s-risk-reduction instead.</p>\n<h2>Miscellaneous</h2>\n<h3><strong>Is the concept of s-risks tied to the possibility of AGI and artificial sentience? </strong></h3>\n<p>Many s-risks, such as <a href=\"https://foundational-research.org/a-dialogue-on-suffering-subroutines/\">suffering subroutines</a> or <a href=\"https://arbital.com/p/mindcrime/\">mindcrime</a>, have to do with artificial minds or smarter-than-human AI. But the concept of s-risks is not conceptually dependent on the possibility of AI scenarios. For example, <a href=\"http://reducing-suffering.org/will-space-colonization-multiply-wild-animal-suffering/\">spreading wild animal suffering to other planets</a> does not require artificial sentience or AI.</p>\n<p>Examples often involve artificial sentience, however, due to the vast number of artificial beings that could be created if artificial sentience becomes feasible at any time in the future. Combined with humanity&#x2019;s track record of insufficient moral concern for &#x201C;voiceless&#x201D; beings at our command, this might pose a particularly serious s-risk. (More details <a href=\"http://prioritizationresearch.com/s-risks-introduction/#How_s-risks_could_come_about\">here</a>.)</p>\n<h3><strong>Why would we think that artificial sentience is possible in the first place? </strong></h3>\n<p>This question has been discussed extensively in the <a href=\"https://en.wikipedia.org/wiki/Philosophy_of_mind\">philosophy of mind</a>. Many popular theories of consciousness, such as <a href=\"https://en.wikipedia.org/wiki/Global_workspace_theory_(GWT)\">Global workspace theory</a>, <a href=\"https://en.wikipedia.org/wiki/Higher-order_theories_of_consciousness\">higher-order theories</a><a href=\"https://en.wikipedia.org/wiki/Global_workspace_theory_(GWT)\">,</a> or <a href=\"https://en.wikipedia.org/wiki/Integrated_information_theory\">Integrated information theory</a>, agree that artificial sentience is possible in principle. Philosopher Daniel Dennett <a href=\"https://www.ft.com/content/96187a7a-fce5-11e6-96f8-3700c5664d30\">puts it like this</a>:</p>\n<blockquote>\n<p>I&#x2019;ve been arguing for years that, yes, in principle it&#x2019;s possible for human consciousness to be realised in a machine. After all, that&#x2019;s what we are. We&#x2019;re robots made of robots made of robots. We&#x2019;re incredibly complex, trillions of moving parts. But they&#x2019;re all non-miraculous robotic parts.</p>\n</blockquote>\n<p>As an example of the sort of reasoning involved, consider this intuitive thought experiment: if you were to take a sentient biological brain, and replace one neuron after another with a functionally equivalent computer chip, would it somehow make the brain less sentient? Would the brain still be sentient once all of its biological neurons have been replaced? If not, at what point would it cease to be sentient?</p>\n<p>The debate is not settled yet, but it seems at least plausible that artificial sentience is possible in principle. Also, we don&#x2019;t need to be certain to justify moral concern. It&#x2019;s sufficient that we <a href=\"http://reducing-suffering.org/why-maximize-expected-value/\">can&apos;t rule it out</a>.</p>\n<h3><strong>Ok, I&#x2019;m sold. What can I personally do to help reduce s-risks?</strong></h3>\n<p>A simple first step is to join the discussion, e.g. in <a href=\"https://www.facebook.com/groups/reducing.srisks/\">this Facebook group</a>. If more people think and write about the topic (either independently or at EA organizations), we&#x2019;ll make progress on the crucial question of <a href=\"http://prioritizationresearch.com/s-risks-introduction/#How_can_we_avert_s-risks\">how to best reduce s-risks</a>. At the same time, it helps build a community that, in turn, can get even more people involved.</p>\n<p>If you&#x2019;re interested in doing serious research on s-risks right away, you could have a look at <a href=\"https://foundational-research.org/open-research-questions/\">this list of open questions</a> to find a suitable research topic. Work in <a href=\"https://80000hours.org/articles/ai-policy-guide/\">AI policy and strategy</a> is another interesting option, as progress in this area allows us to shape AI in a more fine-grained way, making it easier to identify and implement safety measures against s-risks.</p>\n<p>Another possibility is to donate to organizations working on s-risks reduction. Currently, the <a href=\"https://foundational-research.org/our-mission/\">Foundational Research Institute</a> is the only group with an explicit focus on s-risks, but other groups also contribute to solving issues that are relevant for s-risk reduction. For example, the <a href=\"https://intelligence.org/\">Machine Intelligence Research Institute</a> aims to ensure that smarter-than-human artificial intelligence is aligned with human values, which <a href=\"http://reducing-suffering.org/donation-recommendations/#Machine_Intelligence_Research_Institute_MIRI\">probably also reduces s-risks</a>. Charities that promote broad societal improvements such as better <a href=\"https://foundational-research.org/international-cooperation-vs-ai-arms-race/\">international cooperation</a> or <a href=\"http://prioritizationresearch.com/arguments-for-and-against-moral-advocacy/\">beneficial values</a> may also contribute to s-risk reduction, albeit in a less targeted way.</p>\n<p>[Disclaimer: I&apos;m in close contact with the Foundational Research Institute, but I am not employed there and don&apos;t receive any financial compensation.]</p></body></html>", "user": {"username": "Tobias_Baumann"}}, {"_id": "Ecq43jCkuTApwG6uA", "title": "2017 LEAN Impact Assessment", "postedAt": "2017-09-17T00:48:59.235Z", "htmlBody": "<html><body><p><a href=\"http://www.rtcharity.org\"><span>Rethink Charity</span></a><span> is happy to announce that </span><a href=\"https://rtcharity.org/lean/\"><span>the Local Effective Altruism Network</span></a><span> (LEAN) is conducting an impact assessment due for completion in November 2017. LEAN is now in it&#x2019;s third year of operation. Although we have previously assessed specific services, this will be the most comprehensive investigation to date. We welcome any input from the EA community during the initial planning stages of the assessment. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>The assessment will utilise mixed methods by incorporating quantitative data from the Effective Altruism Survey, the 2017 Local Group Survey, and qualitative data from interviews with group leaders. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>At a basic level, we expect to gain a clearer sense of the counterfactual value added by LEAN to the EA movement. We would define this value as tangible increases &#xA0;in the amount of engagement with EA, including but not limited to, the increase in total EA groups and individuals recruited to EA. The assessment will also aim to add to the community&#x2019;s knowledge regarding the most effective techniques and strategies for group management and impact.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Our research team is led by </span><a href=\"https://eahub.org/user/richenda-herzig\"><span>Richenda Herzig</span></a><span> and supported by </span><a href=\"https://eahub.org/user/peter-hurford\"><span>Peter Hurford</span></a><span>, </span><a href=\"https://eahub.org/user/tee-barnett\"><span>Tee Barnett,</span></a><span> and </span><a href=\"https://eahub.org/user/david-vatousios\"><span>David Vatousios</span></a><a href=\"https://eahub.org/user/tee-barnett\"><span>.</span></a> <a href=\"https://eahub.org/user/david-moss\"><span>David Moss</span></a><span> is our internal advisor, &#xA0;and </span><a href=\"https://eahub.org/user/gregory-lewis\"><span>Greg Lewis</span></a><span> is serving as an external advisor.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>If you would like to participate in our assessment, we are looking for group organisers willing to participate in interviews via Skype/Google Hangout. If you haven&#x2019;t already completed the </span><a href=\"https://www.surveymonkey.com/r/LGS2017\"><span>Local Group Survey</span></a><span>, this would be enormously helpful to us as well [1]. We are very grateful for the fantastic response we&#x2019;ve had so far. </span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Please feel free to reply to this post or contact Richenda at </span><a href=\"mailto:richenda@eahub.org\"><span>richenda@eahub.org</span></a><span> to provide any input or additional comments..</span></p>\n<p>&#xA0;</p>\n<p><span> [1] </span><span>The survey is open both to group organisers and members, and takes up to 30 mins for organisers and 10 mins for members. The survey will close on the 25th of September.</span></p></body></html>", "user": {"username": "Richenda"}}, {"_id": "744omz9Q72hLqNCKj", "title": "Capitalism and Selfishness", "postedAt": "2017-09-15T08:30:54.508Z", "htmlBody": "<html><body><p>[From my <a href=\"http://johnhalstead.org/index.php/2017/09/15/capitalism-and-selfishness/\">blog</a>]. As effective altruists make increasing forays into politics, I thought it would be good to share what I have found to be one of the most useful conceptual distinctions in recent political philosophy. Many people think if you&#x2019;re in favour of capitalism you have to be in favour of ruthless selfishness. But this isn&#x2019;t so. As the philosopher Jason Brennan has <a href=\"http://bleedingheartlibertarians.com/2014/06/socialism-%E2%89%A0-love-and-kindness-capitalism-%E2%89%A0-greed-and-fear/\">argued</a>,<a title=\"\" href=\"file:///C:/Users/Admin/Documents/My%20docs/Documents/EA%20blogs/Capitalism%20and%20an%20ethos%20of%20benevolence.docx#_ftn1\"><span><!-- [if !supportFootnotes]--><span><span>[1]</span></span><!--[endif]--></span></a> we ought to distinguish capitalism &#x2013; a <em>system of ownership</em> from selfishness &#x2013; a <em>social ethos</em>.</p>\n<p><em>Capitalism</em> = The private ownership of the means of production.</p>\n<p><em>Socialism</em> = The collective ownership of the means of production.</p>\n<p>People have an <em>ethos of selfishness</em> insofar as they pursue their own self-interest.</p>\n<p>People have an <em>ethos of benevolence</em> insofar as they pursue the interests of others.</p>\n<p>Why accept these definitions? Firstly, they align with the commonsense and dictionary definitions of &#x2018;capitalism&#x2019; and &#x2018;socialism&#x2019;. The elision between capitalism and an ethos of selfishness tends only to happen in an informal or unstated way. People unfairly compare capitalism + selfishness with socialism + universal benevolence and conclude that socialism is the superior system, when in fact universal benevolence is doing a lot of the work. Secondly, if we conceptually tie capitalism and an ethos of selfishness, then we will be left with no term for a system in which the means of production are privately owned and everyone is perfectly benevolent. On the other side of the coin, if we conceptually tie socialism and benevolence, then we will be left with no term for a system in which the means of production are collectively owned, but people are extensively motivated by selfishness.</p>\n<p>With these definitions in tow, we can infer the following important point:</p>\n<p><span>&#xA0; &#xA0; &#xA0; &#xA0;&#xA0;</span>The stance one takes on the correct social ethos implies no obvious stance on the justifiability of capitalism or socialism.</p>\n<p>Many effective altruists are strongly critical of the ethos of selfishness: Peter Singer believes that you should give up on all luxury spending in order to help others. However, this does not <em>mean</em> that capitalism is bad because capitalism is not <em>conceptually</em> tied to selfishness.</p>\n<p>The question of which system of economic ownership we ought to have is entirely separate to the question of which ethos we ought to follow. Effective altruists and others have made a strong case for an ethos of benevolence, but finding out whether capitalism or socialism is better involves completely different empirical questions.</p>\n<p>&#xA0;</p>\n<p>Update: To pre-empt a criticism that I don&apos;t think it hits the mark, note that I am saying that capitalism is not, as a conceptual matter, defined as a system in which people are selfish. I am not arguing for or against the proposition that capitalism creates incentives for people to be selfish, or makes people more selfish than the socialist alternative. This is a distinct empirical question.&#xA0;</p>\n<p>&#xA0;</p>\n<p>Thanks to Stefan Schubert for advice.</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<div><!-- [if !supportFootnotes]--><br><hr><!--[endif]-->\n<div>\n<p><a title=\"\" href=\"file:///C:/Users/Admin/Documents/My%20docs/Documents/EA%20blogs/Capitalism%20and%20an%20ethos%20of%20benevolence.docx#_ftnref1\"><span><span><!-- [if !supportFootnotes]--><span><span>[1]</span></span><!--[endif]--></span></span></a><span> He attributes the original point to Sharon Krause.&#xA0;</span></p>\n</div>\n</div></body></html>", "user": null}, {"_id": "S2ypk8fsHFrQopvyo", "title": "EA Survey 2017 Series: Donation Data", "postedAt": "2017-09-12T01:29:56.716Z", "htmlBody": "<html><body><p><span>By Huw Thomas</span></p>\n<p><strong>&#xA0;</strong></p>\n<blockquote>\n<p><span>The annual EA Survey is a volunteer-led project of</span><a href=\"http://rtcharity.org/\"> <span>Rethink Charity</span></a><span> that has become a benchmark for better understanding the EA community. This post is the fourth in a multi-part series intended to provide the survey results in a more digestible and engaging format. </span><span>You can find key supporting documents, including prior EA surveys and an up-to-date list of articles in the EA Survey 2017 Series, at the bottom of this post. G</span><span>et notified of the latest posts in this series by signing up </span><a href=\"http://eepurl.com/c2MaW5\">here</a><span>. </span></p>\n</blockquote>\n<p><span>Our earlier post presented </span><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"><span>declared preferences</span></a><span> among respondents, and donation reporting allows us to further contextualize behavioral trends within the EA community. The most recent survey of 1019 individuals collected donation data on both 2015 and 2016 donations. The survey was not distributed in 2016. </span></p>\n<p>&#xA0;</p>\n<p><span>This post aims to compare donation data of the EA community, and within a couple specific subpopulations. You can find donation data according to cause area and organization preference in our &#x201C;Cause Area Preferences&#x201D; </span><a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\"><span>post</span></a><span>. </span></p>\n<h3><span>Points of Interest</span></h3>\n<ul>\n<li>\n<p><span>Self&#xAD;-described EAs in our survey reported more than $6.6M in total donations to effective charities for 2015, and more than $9.8M in 2016.</span></p>\n</li>\n<li>\n<p><span>Average donation amounts between 2015 and 2016 were heavily skewed upward by major donors, but the median donation amount rose $118.68.</span></p>\n</li>\n<li>\n<p><span>Longitudinal survey data revealed consistent year-on-year donation growth. </span></p>\n</li>\n<li>\n<p><span>Donors parting with $655.17 or more fall within the top 50% of EA donors. Gifts totalling $12,500 or more are among the top 10%. </span></p>\n</li>\n<li>\n<p><span>405 people who identify their career plan as &#x201C;Earning to give&#x201D; (ETG). In 2015, these people accounted for 63.0% of total reported donations. In 2016, ETG donations constituted 57.3% of total reported donations. &#xA0;</span></p>\n</li>\n</ul>\n<h3><span><br>How Much are EAs Donating? </span></h3>\n<p><span>Relatively high average donation rates seem to be commonly associated with effective altruists. So how much are EAs donating? <br></span><strong>&#xA0;</strong></p>\n<p><span>Self&#xAD;-described EAs in our survey reported more than $6.6m in total donations to effective charities for 2015, and more than $9.8m in 2016. We standardized all the donations into US dollars and found that the average 2015 donation was $6,498 among respondents, while the average donation in 2016 was $9,510. These seemingly impressive are seriously skewed upward by a few major donors.<br></span><strong>&#xA0;</strong></p>\n<p><span>The more informative metric, the median donation, was $250 in 2015, and $655 in 2016. This increase was probably due, in part, to the fact that the survey was released in 2017, and so respondents were probably more involved with the movement in 2016 than in 2015 on average. We see evidence of this when comparing donation activity between years. The survey reveals that 150 respondents donated in 2016, but not in 2015. Only 29 donated in 2015, but not 2016. A total of 999 people provided data for both 2015 and 2016 donations.<br><br></span></p>\n<p><span>Although personal donation amounts fluctuated between 2015 and 2016, the mean donation amount per person increased by $3,663.68. This obviously includes a huge variance, however, the median donation amount also increased by $118.68<strong>[1]</strong>.<br></span>&#xA0;</p>\n<p><span>To help visualize the distribution of donation amounts, let&#x2019;s look at it in terms of deciles. In other words, how much you would have to donate to be in the top X% of donors based on the reports that we have from the 2016 data.</span>&#xA0;</p>\n<p>&#xA0;</p>\n<p><img src=\"https://i.imgur.com/asL6aRi.png?1\"></p>\n<p><span>In order to top the highest donation in our registry, you would have to donate over $1,934,550.<br><br></span></p>\n<p><span>According to the survey, EA donations are highly skewed toward a handful of major donors. Many individuals could make it into the top 50% of EA donors by donating a small percentage of their income, but only a distinct minority are capable of making it into the top 1%.<br></span><strong>&#xA0;</strong></p>\n<p><span>Donations are clearly affected by student status. In 2016, the median donation of non-&#xAD;students was $1,538, compared to the median donation of students at $154. The 258 students who donated gave $252,339.60 in total, while the 482 non-students who donated gave $7,242,580.64.<br></span><strong>&#xA0;</strong></p>\n<p><span>These donations may be over&#xAD;reported, given that who donate less might be less inclined to share that information. We found, however, a relatively more forthcoming sample than expected. Among those who reported on donations, 29% in 2015 and 16.4% in 2016 reported donating $0. <br></span><strong>&#xA0;</strong></p>\n<p><span>If you made donations not reported in the survey, please </span><a href=\"https://eahub.org/donations/add\"><span>report them via the EA Donation Registry</span></a><span>, which allows you to anonymously contribute to the public total for the EA community - you can &#xAD;also share </span><a href=\"/ea/7q/to_inspire_people_to_give_be_public_about_your/\"><span>your own donations</span></a><span> to inspire others.</span></p>\n<h3><span><br>Percentage of Income Donated</span>&#xA0;</h3>\n<p><span>The mean percentage of income donated was 7.98% of in 2016<strong>[2]</strong>, but again this is skewed. The median is 4.28%. While this may seem low when benchmarked against the 10% commitment of the Giving What We Can pledge, it is higher than the United States national average of around 2% of GDP<strong>[3]</strong>. To better illustrate the point, let&#x2019;s look at how many people donate at or above a certain amount of income. Since many neglected to reveal their income, or made less than $10,000, this is based on a sample of 597 EAs.</span></p>\n<p><img src=\"https://i.imgur.com/yTa1MzA.png?1\"></p>\n<p><span>It is also possible that people compensate for 2016 donation deficits by donating more at different times. Note also that this finding also doesn&#x2019;t capture the EAs that are saving now while waiting for better causes to donate to later. </span></p>\n<h3><span><br>Donations Among Earning to Give </span></h3>\n<p><span>Perhaps one of the more prescient questions in the community is how much ETG individuals are donating. This question includes all individuals who plan to pursue, or are already involved in ETG careers. In 2015, donations among the 405 ETG individuals in our survey totaled &#xA0;$4,210,633.29. In 2016, donations totaled $5,672,334.74. </span></p>\n<p><span>The median donation amount in 2015 for 255 ETG</span> <span>non-students is $237.65. For 2016, the median amount is $798.57, which is actually less than the median donation for non-students generally. This suggests that many ETG individuals &#xA0;are aiming to give later, and perhaps building </span><a href=\"https://80000hours.org/career-guide/career-capital/\"><span>career capital</span></a><span> in the meantime.<br><br></span><span>We can break this down further by analyzing how EAs responded to &#xA0;&#x201C;Do you believe that - for you at the moment - it is better to act now or invest to act better later?&#x201D;. Among the 148</span> <span>ETG non-students who answered &#x201C;Act now&#x201D;, the median donation was $4,510. Among the 51</span> <span>non-students who answered &#x201C;Act later&#x201D;, the median donation was $712.08. This suggests that the low median donation for earning to give is due to people investing to give later.</span></p>\n<h3><span><br>Longitudinal Analysis</span>&#xA0;</h3>\n<p><span>To look at how donation behavior changes between a subset of individuals, rather than among EA as a whole, we were able to follow a specific group of EAs who took both the 2015 and 2017 EA Surveys<strong>[4]</strong>. </span></p>\n<p><img src=\"https://i.imgur.com/CdT1XMX.png?1\"></p>\n<p><span>The table above reflects consistent year-on-year growth in donations among 184 individuals we tracked across the last three EA surveys. It&apos;s worth noting, however, there is survivorship bias in this group, as EAs who cease donating might also be less likely to take the 2017 EA Survey.</span></p>\n<p><strong>&#xA0;</strong></p>\n<h3><span>Endnotes</span></h3>\n<p><span><strong>[1]:</strong> The median increase is smaller than the difference between the medians for each year, because it only includes people who donated in both years.</span></p>\n<p><span><br><strong>[2]:</strong> Percent income percentages were performed only for people with income greater than $10K, as donations as a percentage of income became quite absurd with low incomes, including many people donating without any income at all. This was chosen prior to any analysis. Income here refers to self-reported individual income, as opposed to household income.</span></p>\n<p><span><br><strong>[3]:</strong> https://www.philanthropy.com/article/The-Stubborn-2-Giving-Rate/154691</span></p>\n<p><span><br><strong>[4]:</strong> The 2014 and 2015 EA surveys covered donation data of the prior year, while the 2017 EA survey covered 2015 and 2016 donation data. For everyone in the 2015 EA Survey and 2017 EA Survey who provided an email address, we hashed their email address using the MD5 hashing function and matched up email addresses between survey data while still ensuring anonymity. This variable is available as `ea_id` in all the public datasets. 180 people could be matched up between 2015 and 2017 surveys and 18 people could be matched up between all three surveys (2014, 2015, and 2017).</span></p>\n<h3>&#xA0;</h3>\n<h3><span>Credits</span></h3>\n<p><span>Post written by Huw Thomas, with edits from Tee Barnett and analysis from Peter Hurford.<br><br></span></p>\n<p><span>A special thanks to Ellen McGeoch, Peter Hurford, and Tom Ash for leading and coordinating the 2017 EA Survey. Additional acknowledgements include: Michael Sadowsky and Gina Stuessy for their contribution to the construction and distribution of the survey, Peter Hurford and Michael Sadowsky for conducting the data analysis, and our volunteers who assisted with beta testing and reporting: Heather Adams, Mario Beraha, Jackie Burhans, and Nick Yeretsian.<br><br></span></p>\n<p><span>Thanks once again to Ellen McGeoch for her presentation of the 2017 EA Survey results at EA Global San Francisco.<br><br></span></p>\n<p><span>We would also like to express our appreciation to the Centre for Effective Altruism, Scott Alexander via SlateStarCodex, 80,000 Hours, EA London, and Animal Charity Evaluators for their assistance in distributing the survey. Thanks also to everyone who took and shared the survey.</span></p>\n<h3><span><br>Supporting Documents</span></h3>\n<h3><span>EA Survey 2017 Series Articles</span></h3>\n<p><span>I -</span><a href=\"/ea/1e0/effective_altruism_survey_2017_distribution_and/\"> <span>Distribution and Analysis Methodology</span></a></p>\n<p><span>II -</span><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"> <span>Community Demographics &amp; Beliefs</span></a></p>\n<p><span>III -</span><a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\"> <span>Cause Area Preferences</span></a></p>\n<p><span>IV - <a href=\"/ea/1el/ea_survey_2017_series_donation_data/\">Donation Data</a></span></p>\n<p><span><span>V - </span><a href=\"/ea/1ex/demographics_ii/\">Demographics II</a></span></p>\n<p><span>VI - </span><a href=\"/ea/1f5/ea_survey_2017_series_qualitative_comments_summary/\">Qualitative Comments Summary</a></p>\n<p><span>VII - </span><a href=\"/ea/1fi/have_ea_priorities_changed_over_time/\">Have EA Priorities Changed Over Time?</a></p>\n<p><span>VIII - </span><a href=\"/ea/1h5/ea_survey_2017_series_how_do_people_get_into_ea/\">How do People Get Into EA?</a></p>\n<p>&#xA0;</p>\n<p><span>Please note: this section will be continually updated as new posts are published. </span><span>All 2017 EA Survey posts will be compiled into a single report at the end of this publishing cycle</span></p>\n<p>&#xA0;</p>\n<h4><span>Prior EA Surveys conducted by Rethink Charity (formerly .impact)</span></h4>\n<p><a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\"><span>The 2015 Survey of Effective Altruists: Results and Analysis</span></a></p>\n<p><span><a href=\"/ea/gb/the_2014_survey_of_effective_altruists_results/\">The 2014 Survey of Effective Altruists: Results and Analysis</a></span></p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Tee"}}, {"_id": "gKADk68wvi53zj6G6", "title": "The Turing Test", "postedAt": "2017-09-11T01:20:38.543Z", "htmlBody": "<html><body><p>I&apos;m happy to announce that the first episodes of Harvard Effective Altruism&apos;s podcast, <a href=\"https://itunes.apple.com/us/podcast/the-turing-test/id1253609712\">the Turing Test</a>, are already online.</p>\n<p>The first four episodes feature</p>\n<ul>\n<li>Larry Summers on his career, economics and EA</li>\n<li>Irene Pepperberg on animal cognition and ethics</li>\n<li>Josh Greene on moral cognition and EA</li>\n<li>Adam Marblestone on incentives in science, differential technological development etc.</li>\n</ul>\n<p>My co-host Holly Elmore and I recorded a couple more, including Lant Pritchett, Bryan Caplan, Scott Weathers, Spencer Greenberg and Brian Tomasik. Among other things, the guest has to pass an &quot;ideological Turing Test&quot; -&#xA0;i.e.&#xA0;<span>state opposing views as clearly and persuasively as their proponents.</span></p>\n<p>You should be able to subscribe on your favorite platform. We&apos;re looking forward to hearing your feedback!</p></body></html>", "user": {"username": "Ales_Flidr"}}, {"_id": "J7XLAwiB47L3zjTcR", "title": "Ten new 80,000 Hours articles made for the effective altruist community", "postedAt": "2017-09-07T02:25:36.080Z", "htmlBody": "<html><body><p>We&apos;ve produced a lot of content&#xA0;tailored for&#xA0;people heavily involved in the effective altruist community as it&apos;s&#xA0;our&#xA0;key target audience. Here&apos;s a recent list:</p>\n<ul>\n<li><a href=\"https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/\">Why the long-term future of humanity matters more than anything else, and what we should do about it, according to Oxford philosopher Dr Toby Ord</a></li>\n<li><a href=\"https://80000hours.org/2017/08/the-life-of-a-quant-trader-how-to-earn-and-donate-millions-within-a-few-years/\">The life of a quant trader: how to earn millions to donate within a few years, like Alex Gordon-Brown</a></li>\n<li><a href=\"https://80000hours.org/career-reviews/machine-learning-phd/\">Thinking of learning Machine Learning? Read this first.</a></li>\n<li><a href=\"https://80000hours.org/2017/08/podcast-we-are-not-worried-enough-about-the-next-pandemic/\">Podcast: We aren&#x2019;t that worried about the next pandemic. Here&#x2019;s why we should be &#x2013; and specifically what we can do to stop it.</a></li>\n<li><a href=\"https://80000hours.org/articles/effective-social-program/\">Is it fair to say that most social programmes don&#x2019;t work?</a></li>\n<li><a href=\"https://80000hours.org/2017/06/which-jobs-do-economists-say-create-the-largest-spillover-benefits-for-society/\">Which professions are paid too much given their actual value to society?</a></li>\n<li><a href=\"https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/\">Podcast: How to train for a job developing AI at OpenAI or DeepMind. Interview with Dr Dario Amodei.</a></li>\n<li><a href=\"https://80000hours.org/articles/harmful-career/\">Is it ever okay to take a harmful job in order to do more good? An in-depth analysis.</a></li>\n<li><a href=\"https://80000hours.org/articles/skills-most-employable/\">These skills make you most employable. Coding isn&#x2019;t one &#x2013; can that be right?</a></li>\n<li><a href=\"https://80000hours.org/2017/06/podcast-prof-david-spiegelhalter-on-risk-statistics-and-improving-the-public-understanding-of-science/\">Podcast: Prof David Spiegelhalter on risk, statistics and improving the public understanding of science</a></li>\n</ul>\n<p>More generally a large outlet for effective altruist facing content going forward is going to be our long-form interviews in <a href=\"https://soundcloud.com/80000-hours\">the 80,000 Hours podcast</a>.&#xA0;Our podcast is focussed on <em>&quot;the world&apos;s most pressing problems and how you can use your career to solve them.&quot;</em>&#xA0;You can make sure not to miss any episodes, hear them on your phone whenever is convenient, and listen to them sped up, by searching for &apos;80,000 Hours&apos;&#xA0;in whatever app&#xA0;you use to get podcasts.</p>\n<p>You can see a <a href=\"https://soundcloud.com/80000-hours\">list of the episodes so far</a> on SoundCloud - they should be weekly for the next few months.</p>\n<p>Here&apos;s <a href=\"/ea/1bn/80000_hours_articles_aimed_at_the_ea_community/\">our post from 3 months ago</a>&#xA0;with the last batch of EA-facing content.</p>\n<p>Hope you like it -&#xA0;let us know what you think in the comments.</p>\n<p>All the best,</p>\n<p>The 80,000 Hours team.</p></body></html>", "user": {"username": "80000_Hours"}}, {"_id": "f6WnAe3qDQqmmz38x", "title": "Is EA Growing? Some EA Growth Metrics for 2017", "postedAt": "2017-09-05T23:36:39.591Z", "htmlBody": "<html><body><p><em>This post was co-authored by Peter Hurford and Joey Savoie.</em></p>\n<p>&#xA0;</p>\n<p>The EA Survey team at Rethink Charity (including myself) recently released initial data from <a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"> the 2017 EA Survey</a>&#xA0;and will have more to follow it up. KBog <a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"> made a comment on the EA Forum</a>&#xA0;noticing that <a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\"> the 2015 EA Survey</a>&#xA0;had 2532 participants, whereas the 2017 EA Survey only had 1837 participants. Does this mean that EA is declining in growth? <br> <br> It&#x2019;s hard to say, and the EA Survey team will have more analysis on this, looking deeper at what the data in the EA Survey tells us about EA membership growth and churn, if anything. However, Joey Savoie (unrelated to the EA Survey team) and I (Peter Hurford) were curious to look a bit more at other metrics of potential growth to get a clearer picture. Prior analysis on this <a href=\"/ea/15o/effective_altruism_forum_web_traffic_from_google/\"> has been done by Vipul Naik at the end of 2016</a>&#xA0;and by <a href=\"/ea/vx/effects_of_major_events_on_ea_activity/\"> Eric Yu at the beginning of 2016</a>&#xA0;(in a post that I sponsored the creation of).</p>\n<p>Joey and I thought it would be a good time to try to re-check the data and see if things have changed. We compiled the following data from various sources to look at EA growth rates.<br><br></p>\n<div>\n<table><colgroup> <col> <col> <col> <col> </colgroup>\n<tbody>\n<tr>\n<td>\n<p><strong>Type of data</strong></p>\n</td>\n<td>\n<p><strong>Sep 2014 to Aug 2015</strong></p>\n</td>\n<td>\n<p><strong>Sep 2015 to Aug 2016</strong></p>\n</td>\n<td>\n<p><strong>Sep 2016 to Aug 2017</strong></p>\n</td>\n</tr>\n<tr>\n<td>\n<p>The EA Facebook group (via <a href=\"https://sociograph.io/\"> sociograph.io</a>)<strong>[1]</strong></p>\n</td>\n<td>\n<p>1547 posts</p>\n<p>471 authors</p>\n<p>920 commenters</p>\n<p>1924 reactors<strong>[2]</strong></p>\n</td>\n<td>\n<p>1374 posts</p>\n<p>521 authors</p>\n<p>1055 commenters</p>\n<p>2710 reactors</p>\n</td>\n<td>\n<p>696 posts<strong>[3]</strong></p>\n<p>400 authors</p>\n<p>1089 commenters</p>\n<p>2872 reactors</p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"https://www.givingwhatwecan.org/dashboard\">New Giving What We Can pledgers </a></p>\n</td>\n<td>\n<p>618</p>\n</td>\n<td>\n<p>770</p>\n</td>\n<td>\n<p>1150</p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"https://docs.google.com/spreadsheets/d/1HyELsX9n85D7M1GKxZ1BndxU9nVFLEPH0eh61g2PI4U/edit#gid=0\"> Number of 80,000 Hours impact-adjusted significant career changes</a></p>\n</td>\n<td>\n<p>184.8</p>\n</td>\n<td>\n<p>631.3</p>\n</td>\n<td>\n<p>1202</p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"https://trends.google.com/trends/explore?q=Effective%20Altruism\"> Google interest in &#x201C;effective altruism&#x201D;</a>&#xA0;(relative scoring)<strong>[4]</strong></p>\n</td>\n<td>\n<p>16.94</p>\n</td>\n<td>\n<p>28.4</p>\n</td>\n<td>\n<p>38.2</p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"https://wikipediaviews.org/displayviewsformultiplemonths.php?page=Effective%20altruism&amp;allmonths=allmonths-present&amp;language=en&amp;drilldowns[0]=desktop&amp;drilldowns[1]=mobile-web&amp;drilldowns[2]=mobile-app&amp;drilldowns[3]=desktop-spider&amp;drilldowns[4]=mobile-web-spider&amp;drilldowns[5]=cumulative-facebook-shares\"> EA wikipedia</a>&#xA0;data (desktop pageviews)<strong>[5]</strong></p>\n</td>\n<td>\n<p>51,100</p>\n</td>\n<td>\n<p>70,500</p>\n</td>\n<td>\n<p>77,800</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>New EA Newsletter sign-ups</p>\n</td>\n<td>\n<p><em>Didn&#x2019;t really exist</em><strong>[6]</strong></p>\n</td>\n<td>\n<p>11,167</p>\n</td>\n<td>\n<p>35,554</p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"/ea/15o/effective_altruism_forum_web_traffic_from_google/\"> EA Forum</a>&#xA0;pageviews</p>\n</td>\n<td>\n<p>309,961</p>\n</td>\n<td>\n<p>345,202</p>\n</td>\n<td>\n<p>390,463</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>New&#xA0;<a href=\"http://www.reddit.com/r/effectivealtruism\">EA Reddit</a>&#xA0;subscribers<strong>[7,8]</strong></p>\n</td>\n<td>\n<p>308</p>\n</td>\n<td>\n<p>869</p>\n</td>\n<td>\n<p>1484</p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>&#xA0;</p>\n<div>\n<table><colgroup> <col> <col> <col> <col> </colgroup>\n<tbody>\n<tr>\n<td>\n<p><strong>Type of data</strong></p>\n</td>\n<td>\n<p><strong>January 2014 to Dec 2014</strong></p>\n</td>\n<td>\n<p><strong>January 2015 to December 2015</strong></p>\n</td>\n<td>\n<p><strong>January 2016 to December 2016</strong></p>\n</td>\n</tr>\n<tr>\n<td>\n<p>Non-OpenPhil GiveWell donations</p>\n</td>\n<td>\n<p><a href=\"https://drive.google.com/file/d/0B8ompSd8S_anakRyR0p4bkNsRWM/view\"> $13.3M </a></p>\n</td>\n<td>\n<p><a href=\"https://drive.google.com/file/d/0B8ompSd8S_anakRyR0p4bkNsRWM/view\"> $40M </a></p>\n</td>\n<td>\n<p>$30-40M (math)</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>OpenPhil GiveWell donations<strong>[9]</strong></p>\n</td>\n<td>\n<p><a href=\"http://blog.givewell.org/2014/12/01/our-updated-top-charities/\"> $8M </a></p>\n</td>\n<td>\n<p>$70M</p>\n</td>\n<td>\n<p><a href=\"http://blog.givewell.org/2016/11/28/updated-top-charities-giving-season-2016/#Sec3a\"> $50M </a></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"http://www.openphilanthropy.org/giving/grants\">Total OpenPhil donations</a></p>\n</td>\n<td>$30.2M\n<p>&#xA0;</p>\n</td>\n<td>$34.2M</td>\n<td>\n<p>$126.4M</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>OpenPhil + non-OpenPhil GiveWell donations</p>\n</td>\n<td>\n<p>$21.3M</p>\n</td>\n<td>\n<p><a href=\"http://blog.givewell.org/2017/03/30/givewell-as-an-organization-progress-in-2016-and-plans-for-2017/?fref=gc&amp;dti=163441374010685\"> $110M </a></p>\n</td>\n<td>\n<p><a href=\"http://blog.givewell.org/2017/03/30/givewell-as-an-organization-progress-in-2016-and-plans-for-2017/?fref=gc&amp;dti=163441374010685\"> ~$80-90M </a></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"https://drive.google.com/file/d/0B8ompSd8S_anakRyR0p4bkNsRWM/view\"> Total non-OpenPhil donors </a></p>\n</td>\n<td>\n<p>9044</p>\n</td>\n<td>\n<p>14,287</p>\n</td>\n<td>\n<p><em>No data available</em><strong>[10]</strong></p>\n</td>\n</tr>\n<tr>\n<td>\n<p>Total recorded money actually donated (not pledges) from Giving What We Can members</p>\n</td>\n<td>\n<p><a href=\"https://www.givingwhatwecan.org/impact\">$7.1M</a></p>\n</td>\n<td>\n<p><a href=\"https://www.centreforeffectivealtruism.org/fundraising/#giving-what-we-can-pledge\">$7.0M</a><strong>[11]</strong></p>\n</td>\n<td>\n<p><em>No data available</em></p>\n</td>\n</tr>\n<tr>\n<td>\n<p>80,000 Hours Newsletter Subscribers</p>\n</td>\n<td>\n<p>262</p>\n</td>\n<td>\n<p>23,000</p>\n</td>\n<td>\n<p>76,000</p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>&#xA0;</p>\n<div>\n<table><colgroup> <col> <col> <col> <col> </colgroup>\n<tbody>\n<tr>\n<td>\n<p><strong>Type of data</strong></p>\n</td>\n<td>\n<p><strong>EA Survey 2014</strong></p>\n</td>\n<td>\n<p><strong>EA Survey 2015</strong></p>\n</td>\n<td>\n<p><strong>EA Survey 2017</strong></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"> EA Survey respondents </a></p>\n</td>\n<td>\n<p>813</p>\n</td>\n<td>\n<p>2532</p>\n</td>\n<td>\n<p>1837</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>Total amount of recorded donations in EA Survey</p>\n</td>\n<td>\n<p>$5.2M</p>\n</td>\n<td>\n<p>$6.8M</p>\n</td>\n<td>\n<p>$9.9M<strong>[12]</strong></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>&#xA0;</p>\n<p><strong>Additional Facebook Data</strong></p>\n<p>&#xA0;As an admin of the EA Facebook group, I was also able to access the built-in statistics, but they only go back 60 days. Right now, as of 30 August 2017, the EA FB group has about 8629 &#x201C;active users&#x201D; in the last 60 days as defined by Facebook (user has viewed, posted, commented, or reacted in the group). The last 60 days also saw 99 posts, 2073 comments, and 7042 reactions. 339 members were added in the past 60 days, to a total of 13,407 members right now. Comparing the past sixty days to the past 28 days shows roughly linear growth, but I would not expect to be able to see a trend in only two months of data, even if EA was growing very quickly.</p>\n<p>This data is not very useful right now since it only goes back 60 days, but it might be valuable as a time capsule, since a year from now we could re-do this post and compare the numbers for the group as of 2018 to these archived numbers.</p>\n<p>&#xA0;</p>\n<p><strong>Commentary</strong>&#xA0;</p>\n<p>Overall, it&#x2019;s hard to get a good sense of a trend from only two to three data points for each trend. It looks like EA is growing somewhat and that the growth rate is somewhat linear, but it varies depending on the source and I would definitely want to see more data to be sure. A lot of the growth rate can vary depending on how much marginal resources organizations like the Center of Effective Altruism put into intentionally growing a particular source (e.g., the EA Newsletter) versus letting a source grow organically (e.g., the EA Forum).&#xA0;</p>\n<p>I don&#x2019;t really know what kind of growth rate I was expecting prior to seeing this data, so I can&#x2019;t say if this does or doesn&#x2019;t support my hypotheses about movement growth. Joey and I are trying to hold off from having firm additional opinions until we can look at the data more. I think it would be ideal to create concrete predictions around where these numbers will be within one year or so (e.g., see&#xA0;<a href=\"https://predictionbook.com/predictions/185286\">here</a>,&#xA0;<a href=\"https://predictionbook.com/predictions/185287\">here</a>, and <a href=\"https://predictionbook.com/predictions/185288\">here</a>). We certainly invite discussion and predictions on this.</p>\n<p>&#xA0;</p>\n<p><strong>Acknowledgements</strong>&#xA0;</p>\n<p>This post was co-authored by Peter Hurford and Joey Savoie. Thanks to Vipul Naik and Issa Rice for putting together some of the source data used in this report and referring us to the correct places for getting EA Forum and EA Wikipedia pageview data, and thanks Pascal Zimmer for supplying data on the EA Newsletter. Thanks to Kerry Vaughan for providing data on pageviews for the EA Forum in 2017, thanks to Rob Wiblin for some 80,000 Hours statistics, and thanks to Zeke Sherman for pointing out Reddit statistics.</p>\n<p>&#xA0;</p>\n<p><strong>Endnotes</strong>&#xA0;</p>\n<p><strong>[1]:</strong> Sociograph does offer to show member growth over time as a paid feature, which I got via a 14-day free trial, but the data looked incorrect and unusable.&#xA0;</p>\n<p><strong>[2]:</strong> I believe &#x201C;reactors&#x201D; refer to the number of people who like, haha, love, etc. I am not sure if this includes shares, but I do not think that it does.</p>\n<p><strong>[3]:</strong> It&#x2019;s worth noting that the EA Facebook group engages in reasonably heavy moderation of Facebook posts and probably (my intuition as a Facebook mod) rejects about five posts for every one post that reaches the page. Given that moderation may have changed over time, it&#x2019;s not clear how much to read into this decline in the post count.</p>\n<p><strong>[4]:</strong> These numbers are not search volumes -- they&#x2019;re the mean relative &#x201C;score&#x201D; for that year, relative to the search volume for the highest day between January 2004 and the end of August 2017.</p>\n<p><strong>[5]:</strong> See some more wiki data <a href=\"https://www.wikipediatrends.com/Effective_altruism.html\">here</a> and <a href=\"https://wikipediaviews.org/displayviewsformultipleyears.php?tag=Effective%20altruism&amp;language=en&amp;drilldown=desktop&amp;allyears=allyears\"> here</a>.</p>\n<p><strong>[6]:</strong> The EA Newsletter was relaunched and first had regular content and marketing in October 2015.</p>\n<p><strong>[7]:</strong>&#xA0;Both r/EffectiveAltruism and r/smartgiving have been simultaneous EA subreddits since September 2012. r/smartgiving was the default EA subreddit until an intentional migration on 28 Feb 2016. From Sep 2016 - Aug 2017, r/effectivealtruism had +1484 subscribers (1068-&gt;2552) and r/smartgiving had&#xA0;+93 (1466-&gt;1559); from Sep 2015 - Aug 2016, +869 (199-&gt;1068) for r/effectivealtruism and&#xA0;+230 (1236-&gt;1466) for r/smartgiving; and from Sep 2014 - Aug 2015, +91 (108-&gt;199) for r/effectivealtruism and&#xA0;+308 (928-&gt;1236) for r/smartgiving. I will use r/smartgiving numbers for the 2014-2015 period and r/effectivealtruism numbers for all periods after that, to reflect the transition. Note that this growth will therefore involve some inherent double-counting as people who were subscribed on r/smartgiving re-subscribe on r/effectivealtruism. Pageviews for reddit were calculated via <a href=\"http://redditmetrics.com/\">http://redditmetrics.com/</a>.</p>\n<p><strong>[8]:&#xA0;</strong>Reddit admin stats go back a year for pageviews. Looking at this,&#xA0;there were&#xA0;~65,350 pageviews between Sep 2016 and August 2017. We can keep this number in this comment as a time capsule and then if/when we do a Growth Metrics 2018 we can compare the pageviews to this number.</p>\n<p><strong>[9]:&#xA0;</strong>Money from the Open Philanthropy Project is counted for the year in which the grant is announced, which may be different from the year the grant is decided or the year the grant money is actually dispersed.</p>\n<p><strong>[10]:</strong> GiveWell has not yet published their Metrics Report for 2016 data.</p>\n<p><strong>[11]:</strong> These numbers come from different sources, so I don&#x2019;t know if they are directly comparable.&#xA0;</p>\n<p><strong>[12]:</strong> The EA Survey 2017 recorded donations from both 2015 and 2016. This is 2016 data only. There was $6.7M in donations recorded for 2015 in the 2017 EA Survey.</p></body></html>", "user": {"username": "Peter_Hurford"}}, {"_id": "qMGFf8u32no9HtEhc", "title": "Which five books would you recommend to an 18 year old?", "postedAt": "2017-09-05T21:10:19.667Z", "htmlBody": "<p>During EA Global San Francisco 2017, there was a panel discussion called \"Celebrating Failed Projects.\" At one point, Nathan Labenz, the moderator,&nbsp;<a href=\"https://www.youtube.com/watch?v=Y4YrmltF2I0&amp;t=45m25s\">asks</a>, \"What are some projects that you guys are harboring in the backs of your respective minds that you'd love to see people undertake even if, and maybe especially where, the chance of ultimate success might be pretty low?\" In response, Anna Salamon says,&nbsp;\"There's a set of books that pretty often change people's lives, especially 18 year old type people's lives, hopefully in good directions. I think it would be lovely to make a list of five of those books and make a list of all the smart kids and mail the books to the smart kids. This has been on the list of obvious things to do for the last ten years but somehow nobody has ever done it. I didn't do it. I don't know. I really wish someone would do it. I think it would be really high impact.\"</p><p>It seems that the following five books are popular in the EA community:</p><p>1. <a href=\"https://smile.amazon.com/dp/B00OYXWL4W/\">Doing Good Better</a> by William MacAskill</p><p>2. <a href=\"https://smile.amazon.com/dp/B01M70QISP/\">80,000 Hours</a>&nbsp;by Benjamin Todd and the 80,000 Hours Team</p><p>3. <a href=\"https://smile.amazon.com/dp/B001S59CP0/\">The Life You Can Save</a> by Peter Singer</p><p>4. <a href=\"https://smile.amazon.com/dp/B00TZE2Q0O/\">Animal Liberation</a> by Peter Singer</p><p>5. <a href=\"https://smile.amazon.com/dp/B00LOOCGB2/\">Superintelligence</a>&nbsp;by Nick Bostrom</p><p>However, I doubt that Salamon meant to limit the selection to books related to effective altruism. If you could choose five books on any topic, which five would you choose?&nbsp;</p>", "user": {"username": "RandomEA"}}, {"_id": "oXzRPoCmYkEySevj8", "title": "Ideological engineering and social control: A neglected topic in AI safety research?", "postedAt": "2017-09-01T18:52:47.012Z", "htmlBody": "<html><body><div>\n<div><span>Will enhanced government control of populations&apos; behaviors and ideologies become one of AI&apos;s biggest medium-term safety risks?</span></div>\n</div>\n<div>\n<div><span>&#xA0;</span>For example, China seems determined to gain a decisive lead in AI research research by 2030, according to the new plan released this summer by its State Council:</div>\n</div>\n<div>\n<div><span>https://www.newamerica.org/documents/1959/translation-fulltext-8.1.17.pdf</span></div>\n</div>\n<div>\n<div>One of China&apos;s key proposed applications is promoting &apos;social stability&apos; and automated &apos;social governance&apos; through comprehensive monitoring of public spaces (through large-scale networks of sensors for face recognition, voice recognition, movement patterns, etc) and social media spaces (through large-scale monitoring of online activity). This would allow improved &apos;anti-terrorism&apos; protection, but also much easier automated monitoring and suppression of dissident people and ideas. Over the longer term, inverse reinforcement learning could allow AI systems to learn to model the current preferences and likely media reactions of populations, allowing new AI propaganda systems to pre-test ideological messaging with much more accuracy, shaping gov&apos;t &apos;talking points&apos;, policy rationales, and ads to be much more persuasive. Likewise, the big US, UK, EU media conglomerates could weaponize AI ideological engineering systems to shape more effective messaging in their TV, movies, news, books, magazines, music, and web sites -- insofar as they have any ideologies to promote. (I think it&apos;s become pretty clear that they do.) As people spend more time with augmented reality systems, AI systems might automatically attach visual labels to certain ideas as &apos;hate speech&apos; or certain people as &apos;hate groups&apos;, allowing mass automated social ostracism of dissident opinions. As people spend more time in virtual reality environments during education, work and leisure, AI ideological control might become even more intensive, resulting in most citizens spending most of their time in an almost total disconnect from reality. Applications of AI ideological control in mass children&apos;s education seem especially horrifying.</div>\n</div>\n<div>\n<div>Compared to other AI applications, suppressing &apos;wrong-think&apos; and promoting &apos;right-think&apos; seems relatively easy. It requires nowhere near AGI. Data mining companies such as Youtube, Facebook, and Twitter are already using semi-automatic methods to suppress, censor, and demonetize dissident political opinions. And governments have strong incentives to implement such programs quickly and secretly, without any public oversight (which would undermine their utility by empowering dissidents to develop counter-strategies). Near-term AI ideological control systems don&apos;t even have to be as safe as autonomous vehicles, since their accidents, false positives, and value misalignments would be invisible to the public, hidden deep within the national security state.</div>\n</div>\n<div>\n<div>AI-enhanced ideological control of civilians by governments and by near-monopoly corporations might turn into &apos;1984&apos; on steroids. We might find ourselves in a &apos;thought bubble&apos; that&apos;s very difficult to escape -- long before AGI becomes an issue.</div>\n</div>\n<div>\n<div><span>This probably isn&apos;t an existential risk, but it could be serious threat to human and animal welfare whenever governments and near-monopolies realize that their interests diverge from those of their citizens and non-human subjects. And it could increase other global catastrophic risks wherever citizen oversight could decrease risks from bioweapons, pandemics, nuclear weapons, other more capable AI systems, etc.</span></div>\n</div>\n<div>\n<div>Has anyone written anything good on this problem of AI ideological engineering systems? I&apos;d appreciate any refs, links, or comments.</div>\n</div>\n<div>\n<div><span>(I posted a shorter version of this query on the &apos;AI Safety Discussion&apos; group in Facebook.)</span></div>\n</div></body></html>", "user": {"username": "geoffreymiller"}}, {"_id": "xeduPnHfCQ9m9f3go", "title": "EA Survey 2017 Series: Cause Area Preferences", "postedAt": "2017-09-01T14:55:27.596Z", "htmlBody": "<html><body><p><img src=\"https://i.imgur.com/lSCiAYt.png?2\"></p>\n<p><span>By Eve McCormick</span></p>\n<p><strong>&#xA0;</strong></p>\n<blockquote><span>The annual EA Survey is a volunteer-led project of </span><a href=\"http://rtcharity.org/\"><span>Rethink Charity</span></a><span> that has become a benchmark for better understanding the EA community. </span><span>This post is the third in a multi-part series intended to provide the survey results in a more digestible and engaging format.</span><span> You can find key supporting documents, including prior EA surveys and an up-to-date list of articles in the EA Survey 2017 Series, at the bottom of this post. Get notified of the latest posts in this series by signing up <a href=\"http://eepurl.com/c2MaW5\">here</a>. </span></blockquote>\n<p>&#xA0;</p>\n<p><span><span>Significant plurality within the community means EAs have different ideas as to which causes will have the most impact. As in previous years, we asked which causes people think are important, first presenting a series of causes, and then letting people answer whether they feel the cause is &#xA0;&quot;The top priority&quot;, &quot;Near the top priority&quot;, through to &quot;I do not think any EA resources should be devoted to this cause&quot;.</span></span></p>\n<p><img src=\"https://i.imgur.com/yKt6XfS.png?1\"></p>\n<p>&#xA0;</p>\n<p><span><span>As in previous years (2014 and 2015), poverty was overwhelmingly identified as the top priority by respondents. As can be seen in the chart above, 601 EAs (or nearly 41%) identified poverty as the top priority, followed by cause prioritization (~19%) and AI (~16%). Poverty was also the most common choice of near-top priority (~14%), followed closely by cause prioritization (~13%) and non-AI far future existential risk (~12%).</span></span></p>\n<p><img src=\"https://i.imgur.com/1OkKox9.png?1\"></p>\n<p><span><span>Causes that many EAs thought no resources should go toward included politics, animal welfare, environmentalism, and AI. There were very few people who did not want to put any EA resources into cause prioritization, poverty, and meta causes.</span></span></p>\n<p><img src=\"https://i.imgur.com/pzkbpfj.png\"></p>\n<p><span><span>Overall, cause prioritisation among EAs reflects very similar trends to the results from 2014 and 2015. However, the proportion of EAs who thought that no resources should go towards AI has dropped significantly since the 2014 and 2015 survey, down from ~16% to ~6%. We find this supports the common assumption that EA has become increasingly accepting of AI as an important cause area to support. Global poverty continues to be overwhelmingly identified as top-priority despite this noticeable softening toward AI. </span></span></p>\n<h3>How are Cause Area Priorities Correlated with Demographics?</h3>\n<p><span>The degree to which individuals prioritised the far future varied considerably according to gender identity. Only 1.6% of donating women said that they donated to far future, compared to 10.9% of men (p = 0.00015). Donations to organisations focusing on poverty were less varied according to gender, with 46% of women donating to poverty, compared to 50.6% of men (not statistically significant).</span></p>\n<p><span><br><span>The identification of animal welfare as the top priority was highly correlated with the amount of meat that EAs were eating. The chart below shows the proportion of EAs who identified animal welfare as a top priority according to gender. Considerably more EAs who identified as female ranked animal welfare as a top or near top priority (~47%), as opposed to ~35% males. The second chart shows the dietary choices of those who identified animal welfare as the top priority. Those who identified animal welfare as top or near top priority were overwhelmingly vegetarian or vegan (~57%), much more than the EA rate of ~20%, which looks promising when compared to the estimated </span><a href=\"https://faunalytics.org/a-summary-of-faunalytics-study-of-current-and-former-vegetarians-and-vegans/\"><span>proportion</span></a><span> of US citizens aged 17+ who are vegetarian or vegan (2%). </span></span></p>\n<p><img src=\"https://i.imgur.com/Qw6yUKr.png\"></p>\n<p><span>The survey also indicated a clustering of cause prioritisation according to geography. Most notably, 62.7% of respondents in the San Francisco Bay area thought that AI was a top or near top priority, compared to 44.6% of respondents outside the Bay (p = 0.01). In all other locations in which more than 10 EAs reported living, cause prioritisation or poverty (and more often the latter) were the two most popular cause areas. For years, the San Francisco Bay area has been known anecdotally as a hotbed of interest in artificial intelligence. Interesting to note would be the concentration of EA-aligned organizations located in an area that heavily favors AI as a cause area [1].</span></p>\n<p><span>&#xA0;</span></p>\n<p><span>Furthermore, environmentalism was one of the lowest ranking cause areas in the Bay Area, New York, Seattle and Berlin. However, it was more favored elsewhere, including in Oxford and Cambridge (UK), where it was ranked second highest. Also, with the exception of Cambridge (UK) and New York, politics was consistently ranked either lowest or second lowest.</span></p>\n<p>&#xA0;</p>\n<p><span><em>[1] This paragraph was revised on September 9, 2017 to reflect the Bay Area as an outlier in terms of the amount of support for AI, rather than declaring AI an outlier as a cause area. </em></span></p>\n<p>&#xA0;</p>\n<h3>Donations by Cause Area</h3>\n<p><span><span>Donation reporting provides valuable data on behavioral trends within EA. In this instance, we were interested to see what tangible efforts EAs were making toward supporting specific cause areas. We presented a list and asked to which organization EAs donated. We will write a post about general donation habits of EAs in the next survey.</span> <img src=\"https://i.imgur.com/jEazzNU.png?1&gt;&lt;/span\"></span></p>\n<p><span><span>As in 2014, the most popular organisations included some of GiveWell&#x2019;s top-rated charities, all of which were focused on global poverty. Once again, AMF received by far the most in total donations in both 2015 and 2016. GiveWell, despite only attracting the fourth highest number of individual donors in both 2015 and 2016, was second in terms of amount per donation received each year.</span></span></p>\n<p><img src=\"https://i.imgur.com/CbOEscL.png?1\"></p>\n<p><span>Meta organisations were the third most popular cause area, in which CEA was by far the most favoured in terms of number of donors and combined size of donations in both years. Mercy for Animals was the most popular out of the animal welfare organisations in both years in number of donors, though the Good Food Institute received more in donations than MFA in 2016. MIRI was the most popular organisation focusing on the far future, which was the least popular cause area overall by donation amount (though the fact that only two far future organisations were listed may explain this, at least in part). However, the least popular organisations among EAs were spread across cause areas: Sightsavers and The END Fund were the two least popular, followed by Faunalytics, the Foundational Research Institute and the Malaria Consortium. The relative unpopularity of Sightsavers, The END Fund and the Malaria Consortium, despite their focus on global poverty, may relate to the fact that they were only confirmed on GiveWell&#x2019;s list of </span><a href=\"http://www.givewell.org/charities/top-charities\"><span>top-recommended charities</span></a><span> quite recently and are not in GiveWell&#x2019;s default recommendation for individual donors.</span></p>\n<p><span>&#xA0;</span></p>\n<p><span>The results solely for the 476 GWWC members in the sample were similar to the above. Global poverty was the most popular cause area, with ~41% respondents reporting to having donated to organisations within this category. This was followed by cause-prioritization organisations, to which ~13% donated.</span></p>\n<h3>Top Donation Destinations</h3>\n<p><span>For both 2015 and 2016, the survey results suggest that GiveWell had the largest mean donation size ($5,179.72 in 2015 and $6,093.822 in 2016). Therefore, despite receiving far fewer individual donations than AMF, the total of GiveWell&#x2019;s combined donations in both years was almost as large. Nevertheless, AMF had the second largest mean donation size ($2,675.39 in 2015 and $3,007.63 in 2016) followed by CEA ($2,796.66 in 2015 and $1,607.32 in 2016). Although GiveWell and CEA were not among the top three most popular organisations for individual donors, they were, like AMF, the most popular within their respective cause areas.</span></p>\n<p><span><span>The top twenty donors by donation size in 2016 donated similarly to the population as a whole. The top twenty donors donated the most to poverty charities, and specifically AMF within that cause area. However, the third most popular organisation among these twenty individuals was CEA, which was not one of the top five highest-ranked organisations in aggregate donations for either 2015 or 2016.</span></span></p>\n<p>&#xA0;</p>\n<h3>Credits</h3>\n<p><span>Post written by Eve McCormick, with edits from Tee Barnett and analysis from Peter Hurford.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>A special thanks to Ellen McGeoch, Peter Hurford, and Tom Ash for leading and coordinating the 2017 EA Survey. Additional acknowledgements include: Michael Sadowsky and Gina Stuessy for their contribution to the construction and distribution of the survey, Peter Hurford and Michael Sadowsky for conducting the data analysis, and our volunteers who assisted with beta testing and reporting: Heather Adams, Mario Beraha, Jackie Burhans, and Nick Yeretsian.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>Thanks once again to Ellen McGeoch for her presentation of the 2017 EA Survey results at EA Global San Francisco.</span></p>\n<p><strong>&#xA0;</strong></p>\n<p><span>We would also like to express our appreciation to the Centre for Effective Altruism, Scott Alexander via SlateStarCodex, 80,000 Hours, EA London, and Animal Charity Evaluators for their assistance in distributing the survey. Thanks also to everyone who took and shared the survey.</span></p>\n<p><strong>&#xA0;</strong></p>\n<blockquote>\n<h3><span>Supporting Documents</span></h3>\n<h3><span>EA Survey 2017 Series Articles</span></h3>\n<p><span>I - </span><a href=\"/ea/1e0/effective_altruism_survey_2017_distribution_and/\"><span>Distribution and Analysis Methodology</span></a></p>\n<p><span>II - </span><a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\"><span>Community Demographics &amp; Beliefs</span></a></p>\n<p><span>III - <a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\">Cause Area Preferences</a></span></p>\n<p><span><span>IV - </span><a href=\"/ea/1el/ea_survey_2017_series_donation_data/\">Donation Data</a></span></p>\n<p><span><span>V - </span><a href=\"/ea/1ex/demographics_ii/\">Demographics II</a></span></p>\n<p><span>VI - </span><a href=\"/ea/1f5/ea_survey_2017_series_qualitative_comments_summary/\">Qualitative Comments Summary</a></p>\n<p><span>VII - </span><a href=\"/ea/1fi/have_ea_priorities_changed_over_time/\">Have EA Priorities Changed Over Time?</a></p>\n<p><span>VIII - </span><a href=\"/ea/1h5/ea_survey_2017_series_how_do_people_get_into_ea/\">How do People Get Into EA?</a></p>\n<p>&#xA0;</p>\n<p><span>Please note: this section will be continually updated as new posts are published. </span><span>All 2017 EA Survey posts will be compiled into a single report at the end of this publishing cycle. </span><span>Get notified of the latest posts in this series by signing up </span><a href=\"http://eepurl.com/c2MaW5\">here</a><span>. </span></p>\n<p>&#xA0;</p>\n<h4><span>Prior EA Surveys conducted by Rethink Charity (formerly .impact)</span></h4>\n<p>&#xA0;</p>\n<p><a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\"><span>The 2015 Survey of Effective Altruists: Results and Analysis</span></a></p>\n<p><span><a href=\"/ea/gb/the_2014_survey_of_effective_altruists_results/\">The 2014 Survey of Effective Altruists: Results and Analysis</a></span></p>\n</blockquote>\n<p>&#xA0;</p></body></html>", "user": {"username": "Tee"}}, {"_id": "YnXwXKbvmCDnHWx8s", "title": "Should EAs think twice before donating to GFI?", "postedAt": "2017-08-31T13:50:30.073Z", "htmlBody": "<html><body><p><!-- [if gte mso 9]><xml>\n<o:OfficeDocumentSettings>\n<o:AllowPNG/>\n</o:OfficeDocumentSettings>\n</xml><![endif]--></p>\n<p><!-- [if gte mso 9]><xml>\n<w:WordDocument>\n<w:View>Normal</w:View>\n<w:Zoom>0</w:Zoom>\n<w:TrackMoves/>\n<w:TrackFormatting/>\n<w:PunctuationKerning/>\n<w:ValidateAgainstSchemas/>\n<w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n<w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n<w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n<w:DoNotPromoteQF/>\n<w:LidThemeOther>EN-GB</w:LidThemeOther>\n<w:LidThemeAsian>X-NONE</w:LidThemeAsian>\n<w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript>\n<w:Compatibility>\n<w:BreakWrappedTables/>\n<w:SnapToGridInCell/>\n<w:WrapTextWithPunct/>\n<w:UseAsianBreakRules/>\n<w:DontGrowAutofit/>\n<w:SplitPgBreakAndParaMark/>\n<w:EnableOpenTypeKerning/>\n<w:DontFlipMirrorIndents/>\n<w:OverrideTableStyleHps/>\n</w:Compatibility>\n<m:mathPr>\n<m:mathFont m:val=\"Cambria Math\"/>\n<m:brkBin m:val=\"before\"/>\n<m:brkBinSub m:val=\"&#45;-\"/>\n<m:smallFrac m:val=\"off\"/>\n<m:dispDef/>\n<m:lMargin m:val=\"0\"/>\n<m:rMargin m:val=\"0\"/>\n<m:defJc m:val=\"centerGroup\"/>\n<m:wrapIndent m:val=\"1440\"/>\n<m:intLim m:val=\"subSup\"/>\n<m:naryLim m:val=\"undOvr\"/>\n</m:mathPr></w:WordDocument>\n</xml><![endif]--><!-- [if gte mso 9]><xml>\n<w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"false\"\nDefSemiHidden=\"false\" DefQFormat=\"false\" DefPriority=\"99\"\nLatentStyleCount=\"375\">\n<w:LsdException Locked=\"false\" Priority=\"0\" QFormat=\"true\" Name=\"Normal\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 9\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 7\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 8\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 9\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 9\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Normal Indent\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"footnote text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"annotation text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"header\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"footer\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index heading\"/>\n<w:LsdException Locked=\"false\" Priority=\"35\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"caption\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"table of figures\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"envelope address\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"envelope return\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"footnote reference\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"annotation reference\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"line number\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"page number\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"endnote reference\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"endnote text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"table of authorities\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"macro\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"toa heading\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"10\" QFormat=\"true\" Name=\"Title\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Closing\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Signature\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"Default Paragraph Font\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text Indent\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Message Header\"/>\n<w:LsdException Locked=\"false\" Priority=\"11\" QFormat=\"true\" Name=\"Subtitle\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Salutation\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Date\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text First Indent\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text First Indent 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Heading\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text Indent 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text Indent 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Block Text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Hyperlink\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"FollowedHyperlink\"/>\n<w:LsdException Locked=\"false\" Priority=\"22\" QFormat=\"true\" Name=\"Strong\"/>\n<w:LsdException Locked=\"false\" Priority=\"20\" QFormat=\"true\" Name=\"Emphasis\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Document Map\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Plain Text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"E-mail Signature\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Top of Form\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Bottom of Form\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Normal (Web)\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Acronym\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Address\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Cite\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Code\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Definition\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Keyboard\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Preformatted\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Sample\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Typewriter\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Variable\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Normal Table\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"annotation subject\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"No List\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Outline List 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Outline List 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Outline List 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Simple 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Simple 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Simple 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Colorful 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Colorful 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Colorful 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 7\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 8\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 7\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 8\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table 3D effects 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table 3D effects 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table 3D effects 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Contemporary\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Elegant\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Professional\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Subtle 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Subtle 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Web 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Web 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Web 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Balloon Text\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"Table Grid\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Theme\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" Name=\"Placeholder Text\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" QFormat=\"true\" Name=\"No Spacing\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" Name=\"Revision\"/>\n<w:LsdException Locked=\"false\" Priority=\"34\" QFormat=\"true\"\nName=\"List Paragraph\"/>\n<w:LsdException Locked=\"false\" Priority=\"29\" QFormat=\"true\" Name=\"Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"30\" QFormat=\"true\"\nName=\"Intense Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"19\" QFormat=\"true\"\nName=\"Subtle Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"21\" QFormat=\"true\"\nName=\"Intense Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"31\" QFormat=\"true\"\nName=\"Subtle Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"32\" QFormat=\"true\"\nName=\"Intense Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"33\" QFormat=\"true\" Name=\"Book Title\"/>\n<w:LsdException Locked=\"false\" Priority=\"37\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"Bibliography\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"TOC Heading\"/>\n<w:LsdException Locked=\"false\" Priority=\"41\" Name=\"Plain Table 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"42\" Name=\"Plain Table 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"43\" Name=\"Plain Table 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"44\" Name=\"Plain Table 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"45\" Name=\"Plain Table 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"40\" Name=\"Grid Table Light\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\" Name=\"Grid Table 1 Light\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\" Name=\"Grid Table 6 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\" Name=\"Grid Table 7 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\" Name=\"List Table 1 Light\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\" Name=\"List Table 6 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\" Name=\"List Table 7 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Mention\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Smart Hyperlink\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Hashtag\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Unresolved Mention\"/>\n</w:LatentStyles>\n</xml><![endif]--><!-- [if gte mso 10]>\n<style>\n/* Style Definitions */\ntable.MsoNormalTable\n{mso-style-name:\"Table Normal\";\nmso-tstyle-rowband-size:0;\nmso-tstyle-colband-size:0;\nmso-style-noshow:yes;\nmso-style-priority:99;\nmso-style-parent:\"\";\nmso-padding-alt:0cm 5.4pt 0cm 5.4pt;\nmso-para-margin-top:0cm;\nmso-para-margin-right:0cm;\nmso-para-margin-bottom:8.0pt;\nmso-para-margin-left:0cm;\nline-height:107%;\nmso-pagination:widow-orphan;\nfont-size:11.0pt;\nfont-family:\"Calibri\",sans-serif;\nmso-ascii-font-family:Calibri;\nmso-ascii-theme-font:minor-latin;\nmso-hansi-font-family:Calibri;\nmso-hansi-theme-font:minor-latin;\nmso-bidi-font-family:\"Times New Roman\";\nmso-bidi-theme-font:minor-bidi;\nmso-fareast-language:EN-US;}\n</style>\n<![endif]--></p>\n<p><!-- [if gte mso 9]><xml>\n<o:OfficeDocumentSettings>\n<o:AllowPNG/>\n</o:OfficeDocumentSettings>\n</xml><![endif]--><!-- [if gte mso 9]><xml>\n<w:WordDocument>\n<w:View>Normal</w:View>\n<w:Zoom>0</w:Zoom>\n<w:TrackMoves/>\n<w:TrackFormatting/>\n<w:PunctuationKerning/>\n<w:ValidateAgainstSchemas/>\n<w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n<w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n<w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n<w:DoNotPromoteQF/>\n<w:LidThemeOther>EN-GB</w:LidThemeOther>\n<w:LidThemeAsian>X-NONE</w:LidThemeAsian>\n<w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript>\n<w:Compatibility>\n<w:BreakWrappedTables/>\n<w:SnapToGridInCell/>\n<w:WrapTextWithPunct/>\n<w:UseAsianBreakRules/>\n<w:DontGrowAutofit/>\n<w:SplitPgBreakAndParaMark/>\n<w:EnableOpenTypeKerning/>\n<w:DontFlipMirrorIndents/>\n<w:OverrideTableStyleHps/>\n</w:Compatibility>\n<m:mathPr>\n<m:mathFont m:val=\"Cambria Math\"/>\n<m:brkBin m:val=\"before\"/>\n<m:brkBinSub m:val=\"&#45;-\"/>\n<m:smallFrac m:val=\"off\"/>\n<m:dispDef/>\n<m:lMargin m:val=\"0\"/>\n<m:rMargin m:val=\"0\"/>\n<m:defJc m:val=\"centerGroup\"/>\n<m:wrapIndent m:val=\"1440\"/>\n<m:intLim m:val=\"subSup\"/>\n<m:naryLim m:val=\"undOvr\"/>\n</m:mathPr></w:WordDocument>\n</xml><![endif]--><!-- [if gte mso 9]><xml>\n<w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"false\"\nDefSemiHidden=\"false\" DefQFormat=\"false\" DefPriority=\"99\"\nLatentStyleCount=\"375\">\n<w:LsdException Locked=\"false\" Priority=\"0\" QFormat=\"true\" Name=\"Normal\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"heading 9\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 7\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 8\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index 9\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 7\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 8\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"toc 9\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Normal Indent\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"footnote text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"annotation text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"header\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"footer\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"index heading\"/>\n<w:LsdException Locked=\"false\" Priority=\"35\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"caption\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"table of figures\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"envelope address\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"envelope return\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"footnote reference\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"annotation reference\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"line number\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"page number\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"endnote reference\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"endnote text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"table of authorities\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"macro\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"toa heading\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Bullet 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Number 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"10\" QFormat=\"true\" Name=\"Title\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Closing\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Signature\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"Default Paragraph Font\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text Indent\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"List Continue 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Message Header\"/>\n<w:LsdException Locked=\"false\" Priority=\"11\" QFormat=\"true\" Name=\"Subtitle\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Salutation\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Date\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text First Indent\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text First Indent 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Note Heading\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text Indent 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Body Text Indent 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Block Text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Hyperlink\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"FollowedHyperlink\"/>\n<w:LsdException Locked=\"false\" Priority=\"22\" QFormat=\"true\" Name=\"Strong\"/>\n<w:LsdException Locked=\"false\" Priority=\"20\" QFormat=\"true\" Name=\"Emphasis\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Document Map\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Plain Text\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"E-mail Signature\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Top of Form\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Bottom of Form\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Normal (Web)\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Acronym\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Address\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Cite\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Code\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Definition\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Keyboard\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Preformatted\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Sample\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Typewriter\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"HTML Variable\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Normal Table\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"annotation subject\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"No List\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Outline List 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Outline List 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Outline List 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Simple 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Simple 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Simple 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Classic 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Colorful 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Colorful 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Colorful 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Columns 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 7\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Grid 8\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 4\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 5\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 7\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table List 8\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table 3D effects 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table 3D effects 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table 3D effects 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Contemporary\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Elegant\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Professional\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Subtle 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Subtle 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Web 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Web 2\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Web 3\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Balloon Text\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" Name=\"Table Grid\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Table Theme\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" Name=\"Placeholder Text\"/>\n<w:LsdException Locked=\"false\" Priority=\"1\" QFormat=\"true\" Name=\"No Spacing\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" Name=\"Revision\"/>\n<w:LsdException Locked=\"false\" Priority=\"34\" QFormat=\"true\"\nName=\"List Paragraph\"/>\n<w:LsdException Locked=\"false\" Priority=\"29\" QFormat=\"true\" Name=\"Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"30\" QFormat=\"true\"\nName=\"Intense Quote\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"60\" Name=\"Light Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"61\" Name=\"Light List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"62\" Name=\"Light Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"63\" Name=\"Medium Shading 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"64\" Name=\"Medium Shading 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"65\" Name=\"Medium List 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"66\" Name=\"Medium List 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"67\" Name=\"Medium Grid 1 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"68\" Name=\"Medium Grid 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"69\" Name=\"Medium Grid 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"70\" Name=\"Dark List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"71\" Name=\"Colorful Shading Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"72\" Name=\"Colorful List Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"73\" Name=\"Colorful Grid Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"19\" QFormat=\"true\"\nName=\"Subtle Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"21\" QFormat=\"true\"\nName=\"Intense Emphasis\"/>\n<w:LsdException Locked=\"false\" Priority=\"31\" QFormat=\"true\"\nName=\"Subtle Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"32\" QFormat=\"true\"\nName=\"Intense Reference\"/>\n<w:LsdException Locked=\"false\" Priority=\"33\" QFormat=\"true\" Name=\"Book Title\"/>\n<w:LsdException Locked=\"false\" Priority=\"37\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" Name=\"Bibliography\"/>\n<w:LsdException Locked=\"false\" Priority=\"39\" SemiHidden=\"true\"\nUnhideWhenUsed=\"true\" QFormat=\"true\" Name=\"TOC Heading\"/>\n<w:LsdException Locked=\"false\" Priority=\"41\" Name=\"Plain Table 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"42\" Name=\"Plain Table 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"43\" Name=\"Plain Table 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"44\" Name=\"Plain Table 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"45\" Name=\"Plain Table 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"40\" Name=\"Grid Table Light\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\" Name=\"Grid Table 1 Light\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\" Name=\"Grid Table 6 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\" Name=\"Grid Table 7 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"Grid Table 1 Light Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"Grid Table 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"Grid Table 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"Grid Table 4 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"Grid Table 5 Dark Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"Grid Table 6 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"Grid Table 7 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\" Name=\"List Table 1 Light\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\" Name=\"List Table 6 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\" Name=\"List Table 7 Colorful\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 1\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 2\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 3\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 4\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 5\"/>\n<w:LsdException Locked=\"false\" Priority=\"46\"\nName=\"List Table 1 Light Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"47\" Name=\"List Table 2 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"48\" Name=\"List Table 3 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"49\" Name=\"List Table 4 Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"50\" Name=\"List Table 5 Dark Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"51\"\nName=\"List Table 6 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" Priority=\"52\"\nName=\"List Table 7 Colorful Accent 6\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Mention\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Smart Hyperlink\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Hashtag\"/>\n<w:LsdException Locked=\"false\" SemiHidden=\"true\" UnhideWhenUsed=\"true\"\nName=\"Unresolved Mention\"/>\n</w:LatentStyles>\n</xml><![endif]--><!-- [if gte mso 10]>\n<style>\n/* Style Definitions */\ntable.MsoNormalTable\n{mso-style-name:\"Table Normal\";\nmso-tstyle-rowband-size:0;\nmso-tstyle-colband-size:0;\nmso-style-noshow:yes;\nmso-style-priority:99;\nmso-style-parent:\"\";\nmso-padding-alt:0cm 5.4pt 0cm 5.4pt;\nmso-para-margin-top:0cm;\nmso-para-margin-right:0cm;\nmso-para-margin-bottom:8.0pt;\nmso-para-margin-left:0cm;\nline-height:107%;\nmso-pagination:widow-orphan;\nfont-size:11.0pt;\nfont-family:\"Calibri\",sans-serif;\nmso-ascii-font-family:Calibri;\nmso-ascii-theme-font:minor-latin;\nmso-hansi-font-family:Calibri;\nmso-hansi-theme-font:minor-latin;\nmso-bidi-font-family:\"Times New Roman\";\nmso-bidi-theme-font:minor-bidi;\nmso-fareast-language:EN-US;}\n</style>\n<![endif]-->Like <a href=\"https://medium.com/@harrisonnathan/the-actual-number-is-almost-surely-higher-92c908f36517\">some</a> others I was a little surprised the <a href=\"http://www.gfi.org/what\">Good Food Institute</a> (GFI) became a top <a href=\"https://animalcharityevaluators.org/donation-advice/recommended-charities/\">recommendation</a> for Animal Charity Evaluators (ACE) this year.<span>&#xA0; </span>The idea that a group with no discernible track record would ascend to top charity status seemed an unlikely proposition.&#xA0; <span>However, t</span>he decision itself seemed to have some basis in GFI arising out of Mercy for Animals*, a group which is a regular beneficiary of top ACE status.<span>&#xA0; </span>This seemed to help set the scene for the association of GFI as an EA organisation, one which links in with Nick Cooney and Bruce Friedrich&#x2019;s venture capital fund <a href=\"http://www.newcropcapital.com/\">New Crop Capital</a>. <span>&#xA0;</span>As it stands GFI has been organised as a non-profit promotional group for clean meat and plant based alternatives, and this could be identified as an attractive donation opportunity in terms of impact and effectiveness.<span>&#xA0; </span>However, if it is that good a prospect then it follows that would also be the case for various other philanthropically intentioned groups.</p>\n<p>Some of the main considerations for making a funding decision about GFI would probably include factoring in such issues as <a href=\"https://concepts.effectivealtruism.org/concepts/diminishing-returns/\">diminishing returns</a>, the funding gap (presently likely negligible), and the scenario of the <a href=\"http://www.openphilanthropy.org/\">Open Philanthropy Project</a> (Open Phil) as the donor of last resort (unlikely to allow GFI to fall short when GFI likely advocate on behalf of investments for philanthropists who also support Open Phil).<span>&#xA0; </span>If we were to accept these points then it follows that we could start to make a case that it isn&#x2019;t particularly worthwhile for EAs to donate to GFI, because this opportunity will arguably be filled anyway.<span>&#xA0; </span>However, Open Phil may prefer if other people do so first, and can then put funds into other areas, or we could argue that EAs may have more faith in Open Phil / <a href=\"https://www.effectivealtruism.org/effective-altruism-funds-options/\">EA Funds</a> (both Lewis Bollard in relation to animal welfare) at finding different <a href=\"https://app.effectivealtruism.org/funds/animal-welfare\">opportunities</a> in the animal movement. Particularly if we believe the value offered by the animal movement in terms of harm reduction would remain greater than elsewhere, or that we prefer to donate across different core areas.&#xA0;</p>\n<p>If we choose to work outside EA Funds and Open Phil, then it is reasonably the case that we need to find alternatives to GFI, so we could start to look at other groups that might fit our criteria.<span>&#xA0; </span>As part of this process, if we accept the claims made by GFI, then I would suggest there is little value to be found elsewhere in the &#x2018;mainstream&#x2019; (ideologically reducetarian) animal movement.&#xA0; So if we broadly accept the transformative potential of GFI, then the alternative products could cause significant reductions in farmed animal suffering, and as Bruce Friedrich mentions <a href=\"https://animalcharityevaluators.org/charity-review/the-good-food-institute/#c3\">here</a>, it could be the efforts of &#x2018;mainstream&#x2019; oriented groups might have less value than is generally perceived.<span>&#xA0; </span></p>\n<p>Yet we still reasonably need to hedge this issue (particularly in relation to how the Animal Industrial Complex will contest the plant based / clean meat space), and in my view it isn&#x2019;t clear that Open Phil have thoroughly considered this issue. <span>&#xA0;</span>For example, welfare would have low comparative value in the face of GFI claims, seeing as reduced harm is driven largely by increasing demand for plant based products rather than adjusting the system of exploitation.<span>&#xA0; </span>Another issue would relate to how welfarism can act as a carnistic defence, and potentially run counter to reduction efforts through the construct of the &#x2018;humane myth&#x2019;.<span>&#xA0; </span>So if we choose to look for groups that appear to navigate this issue, we could examine organisations engaged in considering wild animal suffering; perhaps <a href=\"http://www.animal-ethics.org/\">Animal Ethics</a>, which is a standout charity at ACE and could be a good donation prospect, or maybe the <a href=\"https://www.nonhumanrights.org/\">Nonhuman Rights Project</a>, another standout charity.</p>\n<p>In exploring different opportunities, I think we would need to identify groups that appreciate the <a href=\"/ea/181/introducing_ceas_guiding_principles/\">guiding principles</a> of EA.&#xA0; Where they meet basic ACE <a href=\"https://animalcharityevaluators.org/charity-reviews/evaluating-charities/evaluation-criteria/#full-criteria-template\">requirements</a> (though given GFI I think there is some flexibility here), and are also interested in empowerment and <a href=\"https://animalcharityevaluators.org/blog/how-can-we-integrate-diversity-equity-and-inclusion-into-the-animal-advocacy-movement/\">inclusion</a>. In a sense groups traditionally neglected by EAA, partly because they tend to fall outside the welfare / abolition paradigm favoured by EAA, <a href=\"https://animalcharityevaluators.org/blog/welfarists-or-abolitionists-division-hurts-animal-advocacy/\">ACE</a> and Open Phil.<span>&#xA0; </span>For a starting point, I would be most interested in the <a href=\"http://www.foodispower.org/\">Food Empowerment Project</a>, perhaps <a href=\"http://encompassmovement.org/\">Encompass</a> (new), and <a href=\"http://www.bettereating.com/\">Better Eating International</a> (also new).<span>&#xA0; </span>These groups wouldn&#x2019;t represent a large funding opportunity, though a degree of funding will be required to help some of them develop further.<span>&#xA0; </span></p>\n<p><span>There is also a further option, that w</span>e consider whether EAs could prioritise meta-evaluation projects for ACE and other EA related groups.<span>&#xA0; </span>If we desire to optimise evidence based (rather than more ideologically weighted) opportunities for donors, it could be argued that we ought to limit donations until these criteria are met, or more importantly, explore ways to allocate donations that would seek to address some of the related <a href=\"/ea/19e/effective_altruism_is_selfrecommending/\">issues</a>.</p>\n<p>To me it would seem reasonable that EAs might choose not to fund GFI or the other top ACE charities, primarily because these are not neglected groups.&#xA0; <span>Instead,</span> we could consider developing a broader framework for intervention that incorporates wide ranging <a href=\"https://concepts.effectivealtruism.org/concepts/frameworks-for-selecting-interventions/\">consultation</a>, and subsequent work around <a href=\"https://concepts.effectivealtruism.org/concepts/counterfactual-considerations/\">counterfactual</a> considerations that often appear to be neglected.<span>&#xA0; </span>Overlooking this form of work can create disruption and contestation in areas that ought to be reasonably covered within an animal movement model.<span>&#xA0; </span>Consequently, it may well be the case that EAs ought to invest in developing more inclusive frameworks for intervention, and concentrate more resources on movement theorising.<span>&#xA0; </span>It is my belief that undertaking work to further explore these issues through a system of meta-evaluation could in turn create a stronger foundation for improved outcomes.</p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p>*Mercy for Animals has the appearance of a one stop shop for interventions.<span>&#xA0; </span>Where various interventions are constructed without a corresponding assessment of how they fit (or don&#x2019;t fit) together. <span>&#xA0;</span></p>\n<p>&#xA0;</p>\n<p><span>Notes.</span></p>\n<p><span>Two groups working in a similar area to GFI.</span></p>\n<p><span>In relation to <a href=\"http://www.new-harvest.org/\">New Harvest</a> from <a href=\"https://animalcharityevaluators.org/charity-review/new-harvest/\">ACE</a>:</span><span>&quot;Furthermore, recently they have been having great success in fundraising on their own, so we want to give them time to determine whether those efforts will fully fund their activities.&quot;&#xA0; <br></span></p>\n<p><span>The <a href=\"http://www.prweb.com/releases/2017/06/prweb14423343.htm\">Plant Based Foods Association</a> requires evaluation.</span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "KevinWatkinson"}}, {"_id": "YFoddZtXdeJE5KEM7", "title": "Looking at how Superforecasting might improve some EA projects response to Superintelligence", "postedAt": "2017-08-29T22:22:26.206Z", "htmlBody": "<html><body><p>*Cross-posted from my <a href=\"https://improvingautonomy.wordpress.com/2017/08/24/improving-open-philanthropy-projects-response-to-superintelligence/\">blog</a>*</p>\n<p>Even if we don&apos;t <a href=\"https://improvingautonomy.wordpress.com/2017/08/24/initial-views-on-intelligence-and-takeoff/\">takeoff very quickly</a> we still have to deal with potential <a href=\"https://www.amazon.co.uk/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111\">Superintelligence</a>.</p>\n<p>There are lots of important uncertainties around intelligence that impact our response, &#xA0;these are called by the community <a href=\"https://concepts.effectivealtruism.org/concepts/the-importance-of-crucial-considerations/\">crucial considerations</a>. The interlocking (non-exhaustive) list of crucial considerations list I tend to think about are things like:</p>\n<ul>\n<li>Will man-made Intelligence be sufficiently like current machine learning system so that you can expect safety solutions for ML to be at all transferable to it?</li>\n<li>Will man-made intelligence be neat or messy (and how does that impact the takeoff speed)</li>\n<li>Will a developing intelligence (whether a pure AI or human/machine hybrid) be able to get a <a href=\"http://lesswrong.com/lw/l4i/superintelligence_7_decisive_strategic_advantage/\">decisive strategic advantage</a>? Can they do so without war?</li>\n<li>Can we make a good world with intelligence augmentation or is co-ordination too hard?</li>\n<li>Should we expect a singleton to be stable?&#xA0;</li>\n</ul>\n<p>These cannot be known ahead of time, until we have developed intelligence or got pretty far down that road. We can estimate the current probabilities with our current knowledge but new information is coming in all the time.</p>\n<p>Answering all these questions is an exercise in forecasting, making educated guesses. The better we can reduce the uncertainty around these questions, the better we can allocate resources to making sure the future of intelligence is beneficial to all.&#xA0;</p>\n<p>It is worthwhile to look at how forecasting is best done (with the caveat that even the best forecasting isn&apos;t very good at looking more than 5 years out). The state of the art, as lots of people are probably familiar, is <a href=\"https://www.amazon.co.uk/Superforecasting-Science-Prediction-Philip-Tetlock/dp/1511358491\">Superforecasting</a>, a methodology to follow developed by the <a href=\"https://www.gjopen.com/\">Good Judgement Project</a>. It is worth having a brief overview of what the Good Judgement Project was, so we can get an idea of why it might be worthwhile adopting it&apos;s lessons and why they might not apply.</p>\n<h2>Good Judgement Project</h2>\n<h3>What they did&#xA0;</h3>\n<p>From <a href=\"https://en.wikipedia.org/wiki/The_Good_Judgment_Project\">wikipedia</a>:</p>\n<blockquote>\n<p>The study employed several thousand people as volunteer forecasters.<sup><a href=\"https://en.wikipedia.org/wiki/The_Good_Judgment_Project#cite_note-Meller2014-13\">[13]</a></sup> Using personality-trait tests, training methods and strategies the researchers at GJP were able to select forecasting participants with less cognitive bias than the average person; as the forecasting contest continued the researchers were able to further down select these individuals in groups of so-called <em>superforecasters</em>. The last season of the GJP enlisted a total of 260 superforecasters.</p>\n</blockquote>\n<h3>What they found to work</h3>\n<p>What they found to work was groups of people with diverse viewpoints of the world but a certain way of thinking. That way of thinking (take from the end of the book) was:</p>\n<ul>\n<li>Looking at both statistical probabilities of events and the specifics of one event to create an updated scenario.</li>\n<li>Looking at a problem from multiple different view points and synthesising them?</li>\n<li>Updating well after getting new information</li>\n<li>Breaking the problem down into sub-problems (fermi) that can be estimated numerically and combined</li>\n<li>Striving to distinguish as many degrees of doubt as possible - be as precise in your estimates as you can</li>\n<li>Learning to forecast better by doing it</li>\n<li>Work well in teams - understanding others view points</li>\n</ul>\n<h3>How relevant is it?</h3>\n<p>It may not be relevant at all. It was focused on questions that were explicitly just about tractable. At a sweet spot between too hard and too easy. We are dealing with questions that are too hard and too far out. But we don&apos;t really have anything better. Even prediction markets can not help us as we can&apos;t resolve these questions before hand and judge betters accuracy on problems.&#xA0;</p>\n<h3>What lessons should we learn?</h3>\n<p>So how well is the community doing at following the lessons of Superforecasting? I shall focus on the Open Philanthropy Project and 80,000 hours, because they seem indicative of the Effective Altruism&apos;s response in general.</p>\n<p>In part, I can&apos;t tell, I am led to believe that discussion has moved off-line and into private forums. If outsiders can&apos;t tell the thought processes behind the decisions then they cannot suggests improvements or alternative views to synthesize. What can we tell from their actions and public statements? I shall use three sources.</p>\n<ul>\n<li><a href=\"http://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity\">Potential risks advanced artificial intelligence: The philanthropic opportunity</a>&#xA0;- as it states the reasons for donating</li>\n<li><a href=\"http://www.openphilanthropy.org/giving/grants?field_focus_area_target_id_selective=532\">List of the donations</a> - to get an idea of the relative weightings it gives to different problem areas</li>\n<li>Advice to people who want to work on <a href=\"https://80000hours.org/ai-safety-syllabus/\">AI safety technology</a> and <a href=\"https://80000hours.org/articles/ai-policy-guide/\">policy</a></li>\n</ul>\n<p>So what can we see from these documents</p>\n<ul>\n<li>A focus on the <a href=\"https://arxiv.org/abs/1606.06565\">concrete problems</a> and a lack of investment in the novel</li>\n<li>A focus on AI rather than IA</li>\n<li>A narrow range of relevant subjects suggested for study, especially on the technical side</li>\n<li>A lack of a break down of the open questions</li>\n</ul>\n<p>Let us take each in turn.</p>\n<h3>Focus on concrete problems</h3>\n<p>It seems that work is heavily weighted to concrete technical problems now. 38 million of 43 million dollars has been donated to teams looking at current AI techniques. &#xA0;The rest has been donated to policy (FHI) or reasoning theory (MIRI). There is no acknowledgement anywhere that current AI techniques might not lead directly to future AI. Also this quote goes against them integrating multiple view points. From the strategy document.&#xA0;</p>\n<blockquote>\n<p>We believe that AI and machine learning researchers are the people best positioned to make many assessments that will be important to us, such as which technical problems seem tractable and high-potential and which researchers have impressive accomplishments.</p>\n</blockquote>\n<p>This strategy is not getting a diverse view point to help forecaster what is likely to be useful. So this is quite worrying for how good we think their predictions will be about their subjects (considering that experts did worse predicting their events in their own fields compared to fields outside their own&#xA0;in the Good Judgement Project). It might be that the Open Philanthropy Project thinks that only AI projects based on current ML are currently technically tractable. But if that was the case you would expect more weight on AI policy/research as they might be able to gather more perspectives on what intelligence is (and how it might be made), I would also expect more calls for novel technical approaches.</p>\n<p><strong>Counterpoint</strong></p>\n<p>Reading the grant for OpenAI:</p>\n<blockquote>\n<p>In fact, much of our other work in this cause aims primarily to help build a general field and culture that can react to a wide variety of potential future situations, and prioritizes this goal above supporting any particular line of research.</p>\n</blockquote>\n<p>So the grant to OpenAI seems to be mainly about culture building. So the question is if current ML work does not lead to AI directly, e.g. if there is something like my <a href=\"https://improvingautonomy.wordpress.com/2017/07/25/why-study-resource-allocation/\">resource allocation paradigm</a>&#xA0;or something not yet thought of needed for AGI as the base-layer, will OpenAI &#xA0;and the team their adopt it and lead the field? &#xA0;Does OpenAI provide the necessary openness to multiple view points and exploration of other possibilities missing from the grants themselves? From what I can tell as an outsider it is only interested in deep learning. I would be pleased to learn otherwise though!</p>\n<h3>A focus on AI with no IA</h3>\n<p>While Superintelligence is bearish on Intelligence augmentation as a solution to the AI problem, that doesn&apos;t mean that it should not be considered at all. We may gain more knowledge that makes it more promising. But if you only search in one field, you will never get knowledge about other fields so you can update your probabilities. One of the super forecasters explicitly chose to get his news from randomised sources, in order to not be too biased. Funding lots of AI people and then asking them how their projects are going is the equivalent of getting information from one news source. There are IA projects like <a href=\"https://www.neuralink.com/\">neuralink</a>&#xA0;and <a href=\"https://kernel.co/\">kernel</a>&#xA0;, so it is not as if there are no IA projects. I don&apos;t think either of them are looking at the software required on the computer to make a useful brain prosthesis (or whether we can make one that is not physically connected) at the moment, so this seems like a neglected question.</p>\n<h3>A narrow range of relevant subjects suggested for study</h3>\n<p>Currently 80,000 hours does not suggest learning any psychology or neuroscience for technical subjects. You can quite happily do machine learning without these things, but if there is a probability that current machine learning is not sufficient then the technical AI safety community needs something to steer itself with ourside of knowledge of machine learning. <a href=\"/ea/1b3/cognitive_sciencepsychology_as_a_neglected/\">Kaj sotala has suggested psychology</a>&#xA0;is neglected so there has been some discussion. But the <a href=\"/ea/1b3/cognitive_sciencepsychology_as_a_neglected/b7e\">reply</a> was that ML is more useful for technical work right now for AI safety, which is true. This might be a tragedy of the commons problem of career choice, which I think was high-lighted in William Macaskill&apos;s speech, which I&apos;ve not watched yet. There is a need for a diverse people set of people working on AI safety in the long term, but the best way of getting ahead is to optimize for the short term, which could lead to everyone knowing the same things and group think. On this point, to reduce this potential problem I think we should suggest technical people take a minor interest as well (be it cognitive science/psychology/sociology/neuroscience/philosophy/computer hardware) and try and integrate their knowledge of that with their normal ML or IA work. Encourage this by trying to make sure teams have people with diverse backgrounds and sub-interests so that they can make better predictions about where their work will go.</p>\n<h3>A lack of breakdown of the open questions and a current lack of updating</h3>\n<p>The only mention of probability in the strategy document is</p>\n<blockquote>\n<p>I believe there&#x2019;s a nontrivial probability that <a href=\"http://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Transformative\">transformative AI</a> will be developed within the next 20 years, with enormous global consequences.</p>\n</blockquote>\n<p>And it is not quantified or broken down. We don&apos;t have fermi style breakdown (as recommended by super forecasting) of what he thinks the questions we need answering to increase or decrease our confidence. &#xA0;He mentions at least two other points where there might be more details forthcoming.</p>\n<blockquote>\n<p>I&#x2019;ve long worried that it&#x2019;s simply too difficult to make meaningful statements (even probabilistic ones) about the future course of technology and its implications. However, I&#x2019;ve gradually changed my view on this topic, partly due to reading I&#x2019;ve done on personal time. It will be challenging to assemble and present the key data points, but I hope to do so at some point this year.</p>\n</blockquote>\n<p>Looking through the blogs I can&apos;t see the breakdown I also cannot see the update mentioned below</p>\n<blockquote>\n<p>In about a year, we&#x2019;ll formally review our progress and reconsider how senior staff time is allocated.</p>\n</blockquote>\n<p>These two things that they had suggested that would happen would be very interesting to see and might enable us to see more of the internal workings and give better criticisms.</p>\n<h3>Suggestions</h3>\n<p>In order to have more accurate forecasts, I would like to see Open Philanthropy Project consult not just AI and ML viewpoints when trying to forecast what will be important work or the important organisations.&#xA0;</p>\n<p>They should look at allocating resources, finding advisers and perhaps building networks within the psychology and neuroscience communities and also possibly the nascent IA community to get their viewpoints on what intelligence is. This will enable them to update on what they think is the more probable approach to solving the super intelligence problem.</p>\n<p>80,000 hours should encourage people studying technical AI subjects to have a broad background in other intelligence related fields as well as ML so that they can have more information on how the field should shift, if it is going to.</p>\n<p>I&apos;d also encourage Open Philanthropy Project to send out the data points and updates that they said they were going to do.</p>\n<p>Whatever we end up forecasting about what we should do about the future of intelligence, I want to try and navigate a way through that improves human&apos;s freedom and autonomy, whilst not neglecting safety and existential risk reduction.</p></body></html>", "user": {"username": "WillPearson"}}, {"_id": "nJ2JpZZquqFrSakK5", "title": "EA Survey 2017 Series: Community Demographics & Beliefs", "postedAt": "2017-08-29T18:36:16.146Z", "htmlBody": "<html><body><p>&#xA0;<img src=\"https://i.imgur.com/lSCiAYt.png?2\"></p>\n<p><span>By: Katie Gertsch </span></p>\n<p>&#xA0;</p>\n<blockquote>\n<p><span><span>The annual EA Survey is a volunteer-led project of </span><a href=\"http://rtcharity.org\"><span>Rethink Charity</span></a><span> that has become a benchmark for better understanding the EA community.</span> This post is the second in a multi-part series intended to provide the survey results in a more digestible and engaging format. Important to bear in mind is the potential for sampling bias and other considerations outlined in the methodology post published <a href=\"/ea/1e0/effective_altruism_survey_2017_distribution_and/\">here</a></span><span>.</span><em><span> You can find key supporting documents, including prior EA surveys and an up-to-date list of articles in the EA Survey 2017 Series, at the bottom of this post. </span><span>Get notified of the latest posts in this series by signing up </span><a href=\"http://eepurl.com/c2MaW5\">here</a></em><span>. </span></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>Summary<br><br></span></p>\n<ul>\n<li>\n<p><span>EAs remain predominantly young and male, though there has been a small increase in female representation since the 2015 survey.</span></p>\n</li>\n<li>\n<p><span>The top five cities with the highest concentration of EAs include the San Francisco Bay Area, London, New York, Boston/Cambridge, and Oxford. </span></p>\n</li>\n<li>\n<p><span>The proportion of EA&#x2019;s that identify as atheist, agnostic, or non-religious came down from 87% in the 2014 and 2015 surveys to 80% in the 2017 survey.</span></p>\n</li>\n<li>\n<p><span>The number who saw EA as a moral duty or opportunity increased, and the number who saw it as an only an obligation decreased.</span></p>\n</li>\n</ul>\n<h3>&#xA0;</h3>\n<h3><span>Age</span></h3>\n<h3><span><span><br><span>The EA community is still predominantly represented by a young adult demographic, with 81% of those giving their age in the EA survey falling between 20 and 35 years of age[1]</span><span>. This year, ages ranged between 15 to 77, with a mean age of 29 and a median age of 27 (and a standard deviation of 10 years). The histogram below shows a visual representation of the distribution of ages.</span></span></span></h3>\n<p><img src=\"https://i.imgur.com/YTIk4bb.png?2\"></p>\n<p><span>[1] Ages were calculated by subtracting the self-reported birth year from 2017.</span></p>\n<h3>&#xA0;</h3>\n<h3><span>Gender<br><br></span></h3>\n<p><span>The survey respondents were male by a wide majority. Of the 1,080 who answered the question asking how they self-identified regarding gender, 757 (70.1%) identified as male, 281 (26.01%) identified as female, 21 (1.9%) respondents identified as &#x201C;other&#x201D;, and another 21 respondents preferred not to answer. This is similar to the 2015 survey, which had a 73% proportion of males.<br><br><br><br></span></p>\n<p><img src=\"https://i.imgur.com/ze55jTw.png?1\"></p>\n<p><span><span>Consistent with the results of the previous survey, the US and UK are main hubs for EA, home to the majority (63.4%) of this year&#x2019;s surveyed EAs. Additionally, the top five countries by population (US, UK, Germany, Canada, and Australia) from the 2015 survey remain the top five countries again in 2017. Australia and New Zealand both dropped ranking slightly, and we saw a small increase of EAs living in Northern European countries, such as Germany, Denmark, Sweden, the Netherlands, and the Czech Republic. Representation from Continental Europe overall rose from 14% to 18%.</span></span></p>\n<p><img src=\"https://i.imgur.com/LFEb3gL.png?1\"></p>\n<p><span>The San Francisco Bay Area (which includes Berkeley, San Francisco, Oakland, Mountain View, Menlo Park, and other areas) remains the most populous area for EAs in our survey for this question, but only outnumbers respondents from London by a very small margin. This gap between London and the Bay Area has shrunk substantially from 2015.</span></p>\n<p>&#xA0;</p>\n<p><span>Oxford, Boston/Cambridge (US) and Cambridge (UK) all show consistently high populations of EAs. Washington D.C. dropped from the fifth most densely populated EA city to eleventh. Newly reported additions include Berlin, Sydney, Madison, Oslo, Toronto, Z&#xFC;rich, Munich, Philadelphia, and Bristol.<br><br></span></p>\n<p><img src=\"https://i.imgur.com/mjRDI9H.png?1\"></p>\n<p><span>The proportion of atheist, agnostic or non-religious people is less than the 2015 survey. Last year that number was 87% compared to 80.6% this year. That metric hadn&#x2019;t changed over the last two surveys, so this could be an indicator that inclusion of people of faith in the EA community is increasing. </span></p>\n<p><span><br><span>As noted in 2015, it has been </span><a href=\"/ea/nt/effective_altruism_and_religious_faiths_mutually/\"><span>suggested</span></a><span> that greater efforts should be made on the part of EA to be more inclusive of religious groups. The numbers definitely still show room for growth in religious communities.</span></span></p>\n<p><br><img src=\"https://i.imgur.com/JNhXNg9.png?2\"></p>\n<p><span>The distribution of responses regarding a stance on moral philosophy is extremely similar to the last survey. In 2015, 56% selected Consequentialism (Utilitarian), 22% No opinion or not familiar with these terms, 13% Non-utilitarian consequentialism, 5% Virtue Ethics and 3% Deontology. Among respondents, the distribution of philosophical stances has not noticeably changed. </span></p>\n<p>&#xA0;<br><br></p>\n<h3><span>Do they see EA as an opportunity or an obligation?</span></h3>\n<h3>&#xA0;</h3>\n<p><span>This question was inspired by Peter Singer&#x2019;s classic </span><a href=\"http://www.utilitarian.net/singer/by/199704--.htm\"><span>essay</span></a><span> on whether doing a tremendous amount of good is an obligation or an opportunity, which inspired commentary by Luke Muehlhauser (see this </span><a href=\"http://lukemuehlhauser.com/effective-altruism-as-opportunity-or-obligation/\"><span>post</span></a><span>) and Holden Karnofsky (see this </span><a href=\"http://blog.givewell.org/2013/08/20/excited-altruism/\"><span>post</span></a><span>), among others. Perhaps even more than a preferred moral philosophical stance, this helps us get a view to the participants&#x2019; motivation to be effective altruists.</span></p>\n<p>&#xA0;</p>\n<p><img src=\"https://i.imgur.com/OiYJNMb.png?1\"></p>\n<p><span>The 2015 survey posed this question a little differently, presenting the choices as &#x2018;Opportunity,&#x2019; &#x2018;Obligation,&#x2019; or &#x2018;Both&#x2019; instead of &#x2018;Moral Duty&#x2019;. Both surveys included &#x2018;Other&#x2019; as a choice as well. About the same proportion chose &#x2018;Both&#x2019; in 2015, as those who selected &#x2018;Moral Duty&#x2019; this year. We could guess that there was a richer connotation understood by &#x2018;Moral Duty&#x2019;, over the more narrow, and somewhat negatively biased &#x2018;Obligation&#x2019; option. </span></p>\n<p>&#xA0;</p>\n<p><span>From 2015 to this year, those who saw EA as only an opportunity stayed the same, while those seeing it only as an obligation decreased significantly.</span></p>\n<p>&#xA0;</p>\n<p><span>By offering &#x2018;Moral Duty&#x2019; as a response, we may have given those who see participating in EA as primarily a dutiful action, a more neutral (less negative) and/or more principled (less self-focused) match to their personal interpretation. </span></p>\n<p>&#xA0;</p>\n<p><strong><span>Credits</span></strong></p>\n<p><span>Post written by Katie Gertsch, with edits from Tee Barnett and analysis from Peter Hurford.</span></p>\n<p>&#xA0;</p>\n<p><span>A special thanks to Ellen McGeoch, Peter Hurford, and Tom Ash for leading and coordinating the 2017 EA Survey. Additional acknowledgements include: Michael Sadowsky and Gina Stuessy for their contribution to the construction and distribution of the survey, Peter Hurford and Michael Sadowsky for conducting the data analysis, and our volunteers who assisted with beta testing and reporting: Heather Adams, Mario Beraha, Jackie Burhans, and Nick Yeretsian.</span></p>\n<p>&#xA0;</p>\n<p><span>Thanks once again to Ellen McGeoch for her presentation of the 2017 EA Survey results at EA Global San Francisco.</span></p>\n<p>&#xA0;</p>\n<p><span>We would also like to express our appreciation to the </span><a href=\"https://www.centreforeffectivealtruism.org/\"><span>Centre for Effective Altruism</span></a><span>, Scott Alexander via </span><a href=\"http://slatestarcodex.com/\"><span>Slate Star Codex</span></a><span>, </span><a href=\"https://80000hours.org/\"><span>80,000 Hours</span></a><span>, </span><a href=\"https://eahub.org/groups/london-effective-altruism\"><span>EA London</span></a><span>, and </span><a href=\"https://animalcharityevaluators.org/\"><span>Animal Charity Evaluators</span></a><span> for their assistance in distributing the survey. Thanks also to everyone who took and shared the survey.</span></p>\n<p>&#xA0;</p>\n<blockquote>\n<h3>Supporting Documents</h3>\n<p><span>EA Survey 2017 Series Articles</span></p>\n<p>I -&#xA0;<a href=\"/ea/1e0/effective_altruism_survey_2017_distribution_and/\">Distribution and Analysis Methodology</a></p>\n<p>II - <a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\">Community Demographics &amp; Beliefs</a></p>\n<p><span>III - </span><a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\">Cause Area Preferences</a></p>\n<p><span>IV - </span><a href=\"/ea/1el/ea_survey_2017_series_donation_data/\">Donation Data</a></p>\n<p><span>V - </span><a href=\"/ea/1ex/demographics_ii/\">Demographics II</a></p>\n<p><span>VI - </span><a href=\"/ea/1f5/ea_survey_2017_series_qualitative_comments_summary/\">Qualitative Comments Summary</a></p>\n<p><span>VII - </span><a href=\"/ea/1fi/have_ea_priorities_changed_over_time/\">Have EA Priorities Changed Over Time?</a></p>\n<p><span>VIII - </span><a href=\"/ea/1h5/ea_survey_2017_series_how_do_people_get_into_ea/\">How do People Get Into EA?</a></p>\n<p>&#xA0;</p>\n<p><em><span>Please note: this section will be continually updated as new posts are published. </span></em><span><span><em>All 2017 EA Survey posts will be compiled into a single report at the end of this publishing cycle. </em></span></span><span>Get notified of the latest posts in this series by signing up </span><a href=\"http://eepurl.com/c2MaW5\">here</a><span>. </span></p>\n<p>&#xA0;</p>\n<p><span>Prior EA Surveys conducted by Rethink Charity (formerly .impact) </span></p>\n<p>&#xA0;</p>\n<p><span><a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\">The 2015 Survey of Effective Altruists: Results and Analysis</a></span></p>\n<p><span><a href=\"/ea/gb/the_2014_survey_of_effective_altruists_results/\">The 2014 Survey of Effective Altruists: Results and Analysis</a></span></p>\n<p>&#xA0;</p>\n<p><span>Raw Data</span></p>\n<p>&#xA0;</p>\n<p><span>Anonymized raw data for the entire EA Survey can be found </span><span><a href=\"https://github.com/peterhurford/ea-data/blob/master/data/2017/imsurvey2017-anonymized-currencied.csv\">here</a></span>.</p>\n</blockquote></body></html>", "user": {"username": "Tee"}}, {"_id": "TuzkYANf7tmQDF2Hk", "title": "EA Survey 2017 Series: Distribution and Analysis Methodology", "postedAt": "2017-08-29T18:31:51.850Z", "htmlBody": "<html><body><p><img src=\"https://i.imgur.com/lSCiAYt.png?2\"></p>\n<p><span><span>By: Ellen McGeoch and Peter Hurford</span></span></p>\n<p>&#xA0;</p>\n<blockquote>\n<p><span><span>The annual EA Survey is a volunteer-led project of </span><a href=\"http://rtcharity.org\"><span>Rethink Charity</span></a><span> that has become a benchmark for better understanding the EA community.</span> This post is the first in a multi-part series intended to provide the survey results in a more digestible and engaging format. </span><em><span><span>You can find key supporting documents, including prior EA surveys and an up-to-date list of articles in the EA Survey 2017 Series, at the bottom of this post. </span></span></em><span>Get notified of the latest posts in this series by signing up </span><a href=\"http://eepurl.com/c2MaW5\">here</a><span>. </span></p>\n</blockquote>\n<p>&#xA0;</p>\n<p><span>Platform and Collection</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Data was collected using LimeSurvey. This year, a &#x201C;Donations Only&#x201D; version of the survey was created for respondents who had filled out the survey in prior years. This version was shorter and could be linked to responses from prior years if the respondent provided the same email address each year.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Distribution Strategy</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Author Note: Any mention of distribution of &#x201C;the survey&#x201D; refers to the URL of the full effective altruism (EA) survey as well as the URL for the &#x201C;Donations Only&#x201D; version of the survey. Each URL has a unique tracking tag that referenced the organization or group sharing the URLs and the type of medium it was being shared on. For example, the URLs shared in the 80,000 Hours newsletter had the tracking tag &#x201C;80k-nl&#x201D;.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Distribution began on April 19, 2017 and continued on a rolling basis until the close of the survey on June 16, 2017. The expansive outreach plan and lag time associated with particular forms of outreach necessitated distributing the survey on a rolling basis. We reached out to over 300 individuals and groups that posted the survey on our behalf, and/or who required permission by a group administrator for a member of the survey team to post the link to a specific site.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>To minimize undersampling and oversampling of different parts of EA, and to make the survey as representative of the community as a whole, we initially followed the distribution plan from the 2014 and 2015 EA surveys, and made modifications based on team consensus. This distribution plan was implemented in 2014 by Peter Hurford, Tom Ash, and Jacy Reese to reach as many members of the EA population as possible.</span><span> Certain additions and omissions were made depending on the availability of particular channels since the initial drafting of the distribution plan. Anyone who had access to the survey was encouraged to share it.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>An appropriate amount of caution should accompany any interpretation of the EA survey results. While the distribution plan included all known digital avenues to reach the EA population, there is room for error and bias in this plan. Claims that a certain percentage of respondents to the survey have certain predispositions or harbor certain beliefs should not necessarily be taken as representative of all EAs or &#x201C;typical&#x201D; of EAs as a whole. Any additional suggestions on how to reach the EA community are welcome.</span></p>\n<p><span>&#xA0;</span></p>\n<p><span><span><span>In an attempt to maximize community engagement, we distributed the survey through email mailing lists, the EA slack group, social networks, forums, websites, emailing prior survey takers, and personal contact.</span></span></span></p>\n<p>&#xA0;</p>\n<p><span><span>The survey was shared on the following websites and forums:</span></span></p>\n<p><span><span><span><img src=\"https://i.imgur.com/P50bTNc.png\"></span></span></span></p>\n<p>&#xA0;</p>\n<p><span><span><span>The survey team reached out to the following mailing lists and listservs to share the survey, those with an asterisk confirmed that they had shared the survey:</span></span></span></p>\n<p><img src=\"https://i.imgur.com/IQ5YA4u.png\"></p>\n<p>&#xA0;</p>\n<p><span><span>The survey was posted to the following general Facebook groups: <img src=\"https://i.imgur.com/xINnqgr.png\"></span></span></p>\n<p>&#xA0;</p>\n<p><span><span><span>The survey was shared with the following local and university Facebook groups, it might not have been posted to all groups due to permissions from administrators:</span></span></span></p>\n<p><img src=\"https://i.imgur.com/o9SiNzm.png\"></p>\n<p>&#xA0;</p>\n<p><span>The survey was also emailed to those who had taken the 2014 and/or 2015 survey and had provided their email address.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Data Analysis</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Analysis began on June 16, 2017 when the dataset was exported and frozen. Any responses after this date were not included in the analysis. The analysis was done by Peter Hurford with assistance from Michael Sadowsky.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Analysis was done in R. All scripts and associated data can be found in the public GitHub repository for the project (see </span><a href=\"https://github.com/peterhurford/ea-data\"><span>the repository here</span></a><span> and </span><a href=\"https://github.com/peterhurford/ea-data/blob/master/data/2017/imsurvey2017-anonymized-currencied.csv\"><span>the anonymized raw data for the 2017 survey here</span></a><span>). Data was collected by Ellen McGeoch and then transferred to the analysis team in an anonymized format, as described in the survey&#x2019;s privacy policy. Currencies were converted into American dollars and standardized, and then processed and analyzed using the open source </span><a href=\"https://github.com/peterhurford/surveytools2\"><span>Surveytools2 R package</span></a><span> created by Peter Hurford.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Subpopulation Analysis</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>In general, people found our survey via Facebook (such as the main EA Facebook group, but not including Facebook pages for local groups), SlateStarCodex, local groups (mailing lists and Facebook groups), the EA Forum, the EA Newsletter, people personally sharing the survey with others, LessWrong, Animal Charity Evaluators (social media and newsletter), 80,000 Hours (newsletter), and an email sent to prior survey takers.</span></p>\n<p><span>&#xA0;</span></p>\n<p><span>By numbers, the referrers broke down like this:</span></p>\n<p><img src=\"https://i.imgur.com/vNZi4a5.png\"></p>\n<p>&#xA0;</p>\n<p><span>Referrer data was gathered via URL tracking. We also asked people to self-report from where they heard about the survey. Similar to the 2014 and 2015 surveys, the self-report data does not line up with the URL data perfectly (e.g., only 72.73% of those for whom URL tracking shows they took it from the EA Newsletter said they heard about the survey from the EA Newsletter). While we don&apos;t know the cause of this, one possible reason might be that some individuals first hear of the survey from one source, but don&apos;t actually take it until they see it posted via another source. Given this discrepancy, we consider URL tracking to be more reliable for determining referrers.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Since we know what populations we are drawing from, we want to know two key questions:</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<ul>\n<li>\n<p><span>Do our subpopulations successfully capture EA as a whole?</span><span> If we have 2.2% (19 LessWrong refers divided by 856 people who responded) of our population coming from LessWrong, is this close to the &#x201C;true&#x201D; number of self-identified EAs that frequent LessWrong more than other channels? Are we over- or under-sampling LessWrong or other channels? Are we systematically missing any part of EA by not identifying the correct channels in order to get people to respond?</span></p>\n</li>\n</ul>\n<p><span><strong>&#xA0;</strong></span></p>\n<ul>\n<li>\n<p><span>Do we successfully capture our subpopulations?</span><span> Are the people who take the survey from LessWrong actually representative of EAs who frequent LessWrong more than other channels? Are we systematically misrepresenting who EAs are by getting a skewed group of people who take our survey?</span></p>\n</li>\n</ul>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Do our subpopulations successfully capture EA as a whole?</span></p>\n<p><span>&#xA0;</span></p>\n<p><span>Unfortunately, we can&#x2019;t answer this question outright without knowing what the &#x201C;true&#x201D; population of EAs actually looks like. However, we can evaluate the strength of that concern by seeing how different our subpopulations are from each other. If our subpopulations vary substantially, then oversampling and undersampling can dramatically affect our representativeness. If our subpopulations don&#x2019;t vary by a large margin, then there is less risk from undersampling or oversampling individual populations that we did sample from, but there is still risk from missing populations that we did not sample.</span></p>\n<p><img src=\"https://i.imgur.com/GekrdM1.png\"></p>\n<p>&#xA0;</p>\n<p><span>Based on the above table, it seems our subpopulations do differ in demographics and affinity toward causes, but not in donation amounts or income. There is a definite risk that oversampling some groups and undersampling others could introduce bias in demographics and answers like top causes.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>As a contrived example to demonstrate what this bias could look like, imagine that SSC truly has 500 EAs on the site all of which are entirely male, and 400 of them take our survey. Whereas, the EA FB group has 1000 EAs, is entirely female, but only 100 of them take our survey. This means that the &#x201C;true&#x201D; population (in our contrived example) would be 33% male, whereas our sampled population would be 80% male.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Unfortunately, without knowing the true distribution of EAs, there&#x2019;s no real way we can know whether we oversampled, undersampled, or got things close to right. This means we should be careful when interpreting EA survey results.</span></p>\n<p><span><strong>&#xA0;</strong></span></p>\n<p><span>Do we successfully capture our subpopulations?</span></p>\n<p><span><br><span>The next question is how well we capture our subpopulations. Again, without an unbiased census of the entire subpopulation, it will be difficult to tell. However, we can compare to another survey. We did some detailed analysis on this for the 2014 EA Survey. There haven&#x2019;t been that many other surveys of EAs lately, but there was a 5500 person </span><a href=\"http://slatestarcodex.com/2017/03/17/ssc-survey-2017-results/\"><span>survey of SlateStarCodex readers</span></a><span> launched just two months before we launched our survey.</span></span></p>\n<p><img src=\"https://i.imgur.com/mdkVK0N.png\"></p>\n<p>&#xA0;</p>\n<p><span>The SSC Survey had many more SSC readers who were EAs than our EA Survey had EA Survey takers who are SSC readers. However, it seems that our EA Survey properly matched the SSC Survey on many demographics, with the exception that the EA Survey had a more consequentialist audience that donated slightly more while earning slightly less. This would indicate that there is a good chance we adequately captured at least the SSC survey-taking EA population in our EA Survey.</span></p>\n<p><span><span><strong>&#xA0;</strong></span></span></p>\n<h3><span>Credits</span></h3>\n<p><span><span><strong>&#xA0;</strong></span></span></p>\n<p><span>Post written by Ellen McGeoch and Peter Hurford, with edits from Tee Barnett and analysis from Peter Hurford.</span></p>\n<p><span><span><strong>&#xA0;</strong></span></span></p>\n<p><span>A special thanks to Ellen McGeoch, Peter Hurford, and Tom Ash for leading and coordinating the 2017 EA Survey. Additional acknowledgements include: Michael Sadowsky and Gina Stuessy for their contribution to the construction and distribution of the survey, Peter Hurford and Michael Sadowsky for conducting the data analysis, and our volunteers who assisted with beta testing and reporting: Heather Adams, Mario Beraha, Jackie Burhans, and Nick Yeretsian.</span></p>\n<p><span><span><strong>&#xA0;</strong></span></span></p>\n<p><span>Thanks once again to Ellen McGeoch for her presentation of the 2017 EA Survey results at </span><a href=\"https://www.eaglobal.org/events/ea-global-2017-san-francisco/\"><span>EA Global San Francisco</span></a><span>.</span></p>\n<p><span><span>&#xA0;</span></span></p>\n<p><span>We would also like to express our appreciation to the </span><a href=\"https://www.centreforeffectivealtruism.org/\"><span>Centre for Effective Altruism</span></a><span>, Scott Alexander of </span><a href=\"http://slatestarcodex.com/\"><span>Slate Star Codex</span></a><span>, </span><a href=\"https://80000hours.org/\"><span>80,000 Hours</span></a><span>, </span><a href=\"https://eahub.org/groups/london-effective-altruism\"><span>EA London</span></a><span>, and </span><a href=\"https://animalcharityevaluators.org/\"><span>Animal Charity Evaluators</span></a><span> for their assistance in distributing the survey. Thanks also to everyone who took and shared the survey.</span></p>\n<p>&#xA0;</p>\n<blockquote>\n<h3>Supporting Documents</h3>\n<h3><span>EA Survey 2017 Series Articles</span></h3>\n<p>I -&#xA0;<a href=\"/ea/1e0/effective_altruism_survey_2017_distribution_and/\">Distribution and Analysis Methodology</a></p>\n<p>II -&#xA0;<a href=\"/ea/1e1/ea_survey_2017_series_community_demographics/\">Community Demographics &amp; Beliefs</a></p>\n<p><span>III - </span><a href=\"/ea/1e5/ea_survey_2017_series_cause_area_preferences/\">Cause Area Preferences</a></p>\n<p><span>IV - </span><a href=\"/ea/1el/ea_survey_2017_series_donation_data/\">Donation Data</a></p>\n<p><span>V - </span><a href=\"/ea/1ex/demographics_ii/\">Demographics II</a></p>\n<p><span>VI - </span><a href=\"/ea/1f5/ea_survey_2017_series_qualitative_comments_summary/\">Qualitative Comments Summary</a></p>\n<p><span>VII - </span><a href=\"/ea/1fi/have_ea_priorities_changed_over_time/\">Have EA Priorities Changed Over Time?</a></p>\n<p><span>VIII - </span><a href=\"/ea/1h5/ea_survey_2017_series_how_do_people_get_into_ea/\">How do People Get Into EA?</a></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p><em><span>Please note: this section will be continually updated as new posts are published. </span></em><span><span><em>All 2017 EA Survey posts will be compiled into a single report at the end of this publishing cycle. </em></span></span><span>Get notified of the latest posts in this series by signing up </span><a href=\"http://eepurl.com/c2MaW5\">here</a><span>. </span></p>\n<p>&#xA0;</p>\n<h4><span>Prior EA Surveys conducted by Rethink Charity (formerly .impact) </span></h4>\n<p>&#xA0;</p>\n<p><span><a href=\"/ea/zw/the_2015_survey_of_effective_altruists_results/\">The 2015 Survey of Effective Altruists: Results and Analysis</a></span></p>\n<p><span><a href=\"/ea/gb/the_2014_survey_of_effective_altruists_results/\">The 2014 Survey of Effective Altruists: Results and Analysis</a></span></p>\n<p>&#xA0;</p>\n<h4><span>Raw Data</span></h4>\n<p><span>Anonymized raw data for the entire EA Survey can be found </span><span><a href=\"https://github.com/peterhurford/ea-data/blob/master/data/2017/imsurvey2017-anonymized-currencied.csv\">here</a></span>.</p>\n</blockquote></body></html>", "user": {"username": "Tee"}}, {"_id": "rFSy5ir5wxFhHNTHe", "title": "Nothing Wrong With AI Weapons", "postedAt": "2017-08-28T02:52:29.953Z", "htmlBody": "<html><body><p>By Kyle Bogosian</p>\n<p>With all the recent worries over AI risks, a lot of people have raised fears about lethal autonomous weapons (LAWs) which take the place of soldiers on the battlefield. Specifically, in the news recently: Elon Musk and over 100 experts requested that the UN implement a ban. https://www.theguardian.com/technology/2017/aug/20/elon-musk-killer-robots-experts-outright-ban-lethal-autonomous-weapons-war</p>\n<p>However, we should not dedicate efforts towards this goal. I don&apos;t know if anyone in the Effective Altruist community has, but I have seen many people talk about it, and I have seen FLI dedicate nontrivial effort towards aggregating and publishing views against the use of LAWs. I don&apos;t think we should be engaged in any of these activities to try and stop the implementation of LAWs, so first I will answer worries about the dangers of LAWs, and then I will point out a benefit.</p>\n<p>The first class of worries is that it is morally wrong to kill someone with an LAW - specifically, that it is more morally wrong than killing someone in a different way. These nonconsequentialist arguments hold that the badness of death has something to do with factors other than the actual suffering and deprivation caused to the victim, the the victim&apos;s family, or society at large. There is a lot of philosophical literature on this issue, generally relating to the idea that machines don&apos;t have the same agency, moral responsibility, or moral judgement that humans do, or something of the sort. I&apos;m going to mostly assume that people here aren&apos;t persuaded by these philosophical arguments in the first place, because this is a lazy forum post, it would take a lot of time to read and answer all the arguments on this topic, and most people here are consequentialists.</p>\n<p>I will say one thing though, which hasn&apos;t been emphasized before and undercuts many of the arguments alleging that death by AI is intrinsically immoral: in contrast to the typical philosopher&apos;s abstract understanding of killing in war, soldiers do not kill after some kind of pure process of ethical deliberation which demonstrates that they are acting morally. Soldiers learn to fight as a mechanical procedure, with the motivation of protection and success on the battlefield, and their ethical standard is to follow orders as long as those orders are lawful. Infantry soldiers often don&apos;t target individual enemies; rather, they lay down suppressive fire upon enemy positions and use weapons with a large area of effect, such as machine-guns and grenades. They don&apos;t think about each kill in ethical terms, they just memorize their Rules Of Engagement, which is an algorithm that determines when you can or can&apos;t use deadly force upon another human. Furthermore, military operations involve the use of large systems where there it is difficult to determine a single person who has the responsibility for a kinetic effect. In artillery bombardments for instance, an officer in the field will order his artillery observer to make a request for support or request it himself based on an observation of enemy positions which may be informed by prior intelligence analysis done by others. The requested coordinates are checked by a fire direction center for avoidance of collateral damage and fratricide, and if approved then the angle for firing is relayed to the gun line. The gun crews carry out the request. Permissions and procedures for this process are laid out beforehand. At no point does one person sit down and carry out philosophical deliberation on whether the killing is moral - it is just a series of people doing their individual jobs making sure that a bunch of things are being done correctly. The system as a whole looks just as grand and impersonal as automatic weaponry does. (I speak from experience, having served on a field artillery unit.)</p>\n<p>When someone in the military screws up and gets innocents killed, the blame often falls upon the commander who had improper procedures in place, not some individual who lost his moral compass. This implies that there is no problem with the attribution of responsibility for an LAW screwing up: it will likewise go to the engineer/programmer who had improper procedures in place. So if killing by AI is immoral because of the lack of individual moral responsibility or the lack of moral deliberation, then killing by soldiers is not really any better and we shouldn&apos;t care about replacing one with the other.</p>\n<p>So, on we go to the consequential harms of LAWs.</p>\n<p>First, there is the worry that it will make war more frequent, since nations don&apos;t have to worry about losing soldiers, thereby increasing civilian deaths. This worry is attributed to unnamed experts in the Guardian article linked above. The logic here is a little bit gross, since it&apos;s saying that we should make sure that ordinary soldiers like me die for the sake of the greater good of manipulating the political system and it also implies that things like body armor and medics should be banned from the battlefield, but I won&apos;t worry about that here because this is a forum full of consequentialists and I honestly think that consequentialist arguments are valid anyway.</p>\n<p>But the argument assumes that the loss of machines is not an equal cost to governments. If nations are indifferent to whether their militaries have soldiers or equally competent machines, then the machines have the same cost as soldiers, so there will be no difference in the expected utility of warfare. If machine armies are better than human soldiers, but also more expensive overall, and nations just care about security and economic costs, then it seems that nations will go to war less frequently, in order to preserve their expensive and better-than-human machines. However, you might believe (with good reason) that nations respond disproportionately to the loss of life on the battlefield, will go to great lengths to avoid it, and will end up with a system that enables them to go to war for less overall cost.</p>\n<p>Well, in undergrad I wrote a paper on the expected utility of war (https://docs.google.com/document/d/1eGzG4la4a96ueQl-uJD03voXVhsXLrbUw0UDDWbSzJA/edit?usp=sharing). Assuming Eckhardt (1989)&apos;s figure of the civilian casualty ratio (https://en.wikipedia.org/wiki/Civilian_casualty_ratio) being 50%, I found that proliferation of robots on the battlefield would only increase total casualties if nations considered the difference between the loss of human armies in wartime and the loss of comparable machines in wartime to be more than 1/3 of the total costs of war. Otherwise, robots on the battlefield would decrease total casualties. It seems to me like it could go either way, particularly with robot weapons having a more positive impact in wars of national security and a more negative impact in wars of foreign intervention and peacekeeping. While I can&apos;t demonstrate that robotic weapons will reduce the total amount of death and destruction caused by war, there is not a clear case that robot weapons would increase total casualties, which is what you need to provide a reason for us to work against them.</p>\n<p>There is also a flaw in the logic of this argument, which is the fact that it applies equally well to some other methods of waging war. In particular, having a human remotely control a military vehicle would have the same impact here as having a fully autonomous military vehicle. So if LAWs were banned, but robot technology turned out to be pretty good and governments wanted to protect soldiers&apos; lives, we would have a similar result.</p>\n<p>Second, there is the worry that autonomous weapons will make tense military situations between non-belligerent nations less stable and more escalatory, prompting new outbreaks of war. I don&apos;t know what reason there is to expect a loss in stability in tense situations; if militaries decide that machines are competent enough to replace humans in battlefield decision making, then they will probably be at least as good at avoiding errors. They do have faster response times - cutting humans out of the loop causes actions to happen faster, enabling a quicker outbreak of violence and escalation of tactical situations. However, the flip side of this is that having humans not be present in these kinds of situations implies that outbreaks of violence will have less political sting and therefore more chance of ending with a peaceful solution. A country can always be compensated for lost machinery through diplomatic negotiations and financial concessions; the same cannot be said for lost soldiers.</p>\n<p>Third, you might say that LAWs will prompt an arms race in AI, reducing safety. But faster AI development will help us avoid other kinds of risks unrelated to AI, and it will expedite humanity&apos;s progress and expansion towards a future with exponentially growing value. Moreover, there is already substantial AI development in civilian sectors as well as non-battlefield military use, and all of these things have competitive dynamics. AGI would have such broad applications that restricting its use in one or two domains is unlikely to make a large difference; after all, economic power is the source of all military power, and international public opinion has nontrivial importance in international relations, and AI can help nations beat their competitors at both.</p>\n<p>Moreover, no military is currently at the cutting edge of AI or machine learning (as far as we can tell). The top research is done in academia and the tech industry; militaries all over the world are just trying to adopt existing techniques for their own use, and don&apos;t have the best talent to do so. Finally, if there is in fact a security dilemma regarding AI weaponry, then activism to stop it is unlikely to be fruitful. The literature on the utility of arms control in international relations is mixed to say the least; it seems to work only as long as the weapons are not actually vital for national security.</p>\n<p>Finally, one could argue that the existence of LAWs makes it possible for hackers such as an unfriendly advanced AI agent to take charge of them and use them for bad ends. However, in the long run a very advanced AI system would have many tools at its disposal for capturing global resources, such as social manipulation, hacking, nanotechnology, biotechnology, building its own robots, and things which are beyond current human knowledge. A superintelligent agent would probably not be limited by human precautions; making the world as a whole less vulnerable to ASI is not a commonly suggested strategy for AI safety since we assume that once it gets onto the internet then there&apos;s not really anything that can be done to stop it. Plus, it&apos;s foolish to assume that an AI system with battlefield capabilities, which is just as good at general reasoning as the humans it replaced, would be vulnerable to a simple hack or takeover in a way that humans aren&apos;t. If a machine can perform complex computations and inference regarding military rules, its duties on the battlefield, and the actions it can take, then it&apos;s likely to have the same intrinsic resistance and skepticism about strange and apparently unlawful orders that human soldiers do. Our mental model of the LAWs of the far future should not be something like a calculator with easy-to-access buttons or a computer with a predictable response to adversarial inputs.</p>\n<p>And in the near run, more autonomy would not necessarily make things any less secure than they are with many other technologies which we currently rely on. A fighter jet has electronics, as does a power plant. Lots of things can theoretically be hacked, and hacking an LAW to cause some damage isn&apos;t necessarily any worse than hacking infrastructure or a manned vehicle. Just replace the GPS coordinates in a JDAM bomb package and you&apos;ve already figured out how to use our existing equipment to deliberately cause many civilian casualties. Things like this don&apos;t happen often, however, because military equipment is generally well hardened and difficult to access in comparison to civilian equipment.</p>\n<p>And this brings me to a counterpoint in favor of LAWs. Military equipment is generally more robust than civilian equipment, and putting AI systems in tense situations where many ethics panels and international watchdogs are present is a great place to test their safety and reliability. Nowhere will the requirements of safety, reliability, and ethics be more stringent than in machines whose job it is to take human life. The more development and testing is conducted by militaries in this regard, the room there is for collaboration, testing and lobbying for safety and beneficial standards of ethics that can be applied to many types of AI systems elsewhere in society. We should be involved in this latter process, not a foolhardy dream of banning valuable weaponry.</p>\n<p>edit: I forgot that disclosures are popular around here. I just started to work on a computer science research proposal for the Army Research Office. But that doesn&apos;t affect my opinions here, which have been the same for a while.</p></body></html>", "user": {"username": "kbog"}}, {"_id": "zXLcsEbzurd39Lq5u", "title": "Setting our salary based on the world\u2019s average GDP per capita", "postedAt": "2017-08-26T19:57:30.492Z", "htmlBody": "<html><body><p><span>A lot of people in the EA movement have a large say over their salary, whether it be earning to give where you can donate down to a certain amount or working for a nonprofit where you take a lower salary. EAs are a unique group in that many of them are taking a salary they feel is ethical instead of the average amount the market would pay for someone of their skill set. So what amount is ethical?<br><br></span></p>\n<p><span>One model I really like the idea of, and Katherine and I have decided to use for now, is taking a look at the world average GDP per capita(</span><a href=\"http://data.worldbank.org/indicator/NY.GDP.PCAP.CD?end=2015&amp;start=1960&amp;view=chart\"><span>1</span></a><span>,</span><a href=\"http://statisticstimes.com/economy/world-gdp-capita-ranking.php\"><span>2</span></a><span>). This comes out to about 10k USD per person or about 20k USD for a couple, although estimates vary and there are other plausible models (e.g. this number does not take into account PPP adjustments). This approximate world average has a very strong intuitive appeal to us, because it&#x2019;s what somebody would get paid if there was complete equality. It fits well with utilitarianism and the veil of ignorance arguments. It also nicely goes up over time (as world poverty is going down and inflation happens) and is currently achievable for a couple with no children in many first world cities (I personally live in </span><a href=\"https://www.numbeo.com/cost-of-living/in/Vancouver\"><span>Vancouver</span></a><span> but have also lived off similar/less wages in </span><a href=\"https://www.numbeo.com/cost-of-living/in/Oxford\"><span>Oxford</span></a><span>). I personally do not feel this model impairs my work productivity (I pay for many time saving luxuries such as having a dishwasher, premade vegan meals and getting my groceries delivered) nor is it is a strong self sacrifice (I live in a safe part of town at a decent level). &#xA0;<br><br></span></p>\n<p><span>For people interested my monthly budget breaks down roughly like this (per person USD)</span></p>\n<p><span>Rent $220, Utilities $37, Phone bill $19, Internet $25, Food $170, Transportation $50, Other spending $150, Saving $100, Taxes $35. <br></span><br><span>There are some things that are specific to my life that is not replicable. For example having no healthcare costs due to living in Canada, sharing a room with my wife, and having no student debt. There are some sacrifices for sure. I do not own a car (although I do have a car-coop membership); I do not eat out often (maybe once a month); I don&#x2019;t do expensive activities (like rock climbing), the basement suite we are renting is old and things occasionally break down; I live with a roommate as well as my wife; and I do not travel often.</span><span><br></span><span><br></span><span>But I really feel far from deprived, especially after seeing poverty first hand in India. I never feel hunger or live without heat. I never live paycheck to paycheck and always have thousands of hours of entertainment at my fingertips. I end up living like a lot of people lived in college. I&#x2019;m posting this because I think a lot of other people can do this too if they try and want to show that it&#x2019;s possible.</span></p></body></html>", "user": {"username": "Joey"}}, {"_id": "uAjQjvNnzNknyQ5uS", "title": "Open Thread #38", "postedAt": "2017-08-22T10:01:15.557Z", "htmlBody": "<html><body><p>Use this thread to post things that are awesome, but not awesome enough to be full posts. This is also a great place to post if you don&apos;t have enough karma to post on the main forum.</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "WillPearson"}}, {"_id": "xfetacPEqzwnpPEcT", "title": "How should we assess very uncertain and non-testable stuff?", "postedAt": "2017-08-17T13:24:44.537Z", "htmlBody": "<html><body><p>There is a good and widely accepted approach to assessing testable projects - roughly what GiveWell does. &#xA0;It is much less clear how EA research organisations should assess projects, interventions and organisations with very uncertain non-testable impact, such as policy work or academic research. There are some disparate materials on this question on blogs, Open Phil&apos;s website, on 80k&apos;s website, in the academic/grey literature etc. However, this information is not centralised; it&apos;s not clear what the points of agreement and disagreement are; lots of the organisations who will have thought about this question will have insights that have not been shared with the community (e.g. maybe CSER, FHI?); and the mechanisms for sharing relevant information in the future are unclear.&#xA0;</p>\n<p>Ultimately, it would be good to collate and curate all the best material on this, so that EA researchers at separate EA orgs would have easy access to it and would not have to approach this question on their own. As a first step, we invite people who have thought about this question to discuss their insights in the comments to this post. Topics could include:</p>\n<ul>\n<li>How far should we use quantified models?</li>\n<ul>\n<li>e.g. The Oxford Prioritisation Project used quantified models to assess really uncertain things like 80k and MIRI.&#xA0;</li>\n<li>Open Phil doesn&apos;t appear to do this (they don&apos;t mention that often in their public facing docs.)</li>\n</ul>\n<li>What role should the Importance/Neglected/Tractable framework play?</li>\n<ul>\n<li>Should it be used to choose between interventions and/or causes?</li>\n<li>Should quantitative models be instead of ITN?</li>\n<li>How quantified should the ITN framework be? As quantified as 80k&apos;s? More intuitive?</li>\n</ul>\n<li>What are the key takeaways from the history of philanthropy, and the history of scientific research?</li>\n<li>What&apos;s the best way to assess historical impact?</li>\n<ul>\n<li>Process tracing or something like it?</li>\n<li>What are the main biases at play in assessing historical impact?</li>\n<li>Who do you ask ?</li>\n</ul>\n<li>Is hits-based giving the right approach and what follows from it?</li>\n<ul>\n<li>How relevant is track record, on this approach? Sometimes Open Phil takes account of track record, other times not.&#xA0;</li>\n<li>Should we favour choosing a cause area and then making lots of bets, or should we be more discerning?</li>\n</ul>\n<li>What are the most important considerations for assessing charities doing uncertain-return stuff?</li>\n<ul>\n<li>Strength of team</li>\n<li>Current strategy</li>\n<li>Potential to crowd in funding.&#xA0;</li>\n</ul>\n<li>What are the best theories of how to bring about political change?</li>\n<li>How much weight should we put on short to medium-term tractability?</li>\n<ul>\n<li>Given the nonlinear nature of e.g. political change, current tractability may not be the best guide.&#xA0;</li>\n</ul>\n<li>Are there any disciplines we could learn from?</li>\n<ul>\n<li>Intelligence analysis.</li>\n<li>Insurance (especially catastrophe insurance).&#xA0;</li>\n</ul>\n</ul>\n<p>&#xA0;</p>\n<p>Thanks, John and Marinella @ Founders Pledge.&#xA0;</p></body></html>", "user": null}, {"_id": "KZz5ajM5gLfY29SCH", "title": "High Time For Drug Policy Reform. Part 4/4: Estimating Cost-Effectiveness vs Other Causes; What EA Should Do Next", "postedAt": "2017-08-12T18:03:34.835Z", "htmlBody": "<html><body><p>This is the fourth of four posts on DPR. In this part I provide some simplistic but illustrative cost-effectiveness estimates comparing an imaginary campaign for DPR against current interventions for poverty, physical health and mental health; I also consider what EAs should do next.</p>\n<p>Links to the&#xA0;articles in&#xA0;this series:</p>\n<p><a href=\"/ea/1d8/dpr/\">Part 1&#xA0;</a>(1,800 words): Introduction and Summary.</p>\n<p><a href=\"/ea/1df/high_time_for_drug_policy_reform_introduction_and/\">Part 2</a>&#xA0;(8,000 words): Six Ways DPR Could Do Good And Anticipating The Objections</p>\n<p><a href=\"/ea/1de/high_time_for_drug_policy_reform_policy/\">Part 3</a>&#xA0;(3,000 words): Policy Suggestions, Tractability and Neglectedess.</p>\n<p><a href=\"/ea/1dj/high_time_for_drug_policy_reform_part_44/\">Part 4</a>&#xA0;(3,500 words): Estimating Cost-Effectiveness vs Other Causes; What EA Should Do Next.</p>\n<p><strong>6. Speculative cost-effectiveness calculations</strong></p>\n<p>For the sake of argument, assume there is an effective campaigning organisation we could fund, or set up, if we wanted to bring about drug policy reform. How cost-effective would it need to be to be more cost-effective than other things effective altruists currently fund, such as Give Directly (unconditional cash transfers to those in poverty) or the Against Malaria Foundation (&#x2018;AMF&#x2019;; bednets to stop children dying from malaria)?</p>\n<p>In addition to Give Directly and AMF, I&#x2019;m also going to add Basic Needs, a mental health charity that operates in the developing world, to the list, even though it isn&apos;t a GiveWell top charity. This&#xA0;is because I(&#x2019;m an outrageous heretic and) think that, if you want to make people happier, it&apos;s probably easier to target misery directly by treating mental illnesses such as depression. By contrast, targeting disease and poverty seem a nuch less direct way to achieving the same goal.&#xA0;I&#x2019;ve made this point&#xA0;<a href=\"/ea/yv/is_effective_altruism_overlooking_human_happiness/\">elsewhere</a> and won&#x2019;t restate the case.</p>\n<p>Technical paragraph non-philosophers may want to skip:</p>\n<p>I&#x2019;m also assuming we&#x2019;re interested in happiness, where happiness is understood in a roughly Benthamite way as positive conscious states &#x2013; those that feel good to you, that you enjoy &#x2013; and unhappiness the opposite, as negative conscious states.<!-- [if !supportFootnotes]-->[1]<!--[endif]--> All plausible moral theories think this matters, even if they don&#x2019;t think it&#x2019;s the only thing that matters, so let&#x2019;s start with a concept of valuable mental states everyone at least shares (even if you think well-being consists in having your desires realised, one desire everyone presumably shares is feeling good). I&#x2019;ll assume we can do interpersonal cardinal comparisons of happiness, i.e. we can compare units of happiness, where one unit to you feels as good as it does to me. This is just the QALY-approach, but using happiness instead of health. Let&#x2019;s also say 1 is maximum sustainable happiness, 0 is neutral and -1 is minimum sustainable happiness.[2]</p>\n<p>If we&#x2019;re going to do the comparison, what we need to know is:</p>\n<ol>\n<li>The cost of the intervention</li>\n<li>The number of people it could affect</li>\n<li>How long it affects each of them for</li>\n<li>The amount it increases their happiness by (on the -1 to 1 scale)</li>\n</ol>\n<p><!-- [if !supportLists]--></p>\n<p>I&#x2019;ll explain my thinking in words, but I&#x2019;ve also put the figures in this <a href=\"https://docs.google.com/spreadsheets/d/17F5Fh8VdHhDqkKoqURmAzaZFhS_svtrETLhKG47fqhc/edit?usp=sharing\">google sheet</a> I invite people to copy and use to create their own estimates.</p>\n<p>Let&#x2019;s consider AMF&#x2019;s cost-effectiveness. Give Well estimate it costs $7,500 to save each child under 5 from dying from malaria.[3]&#xA0;Assume that child lives another 45 years (developing world life expectancy), each year at full happiness (very improbable), then that&#x2019;s one happiness-adjusted life year (&#x2018;HALY&#x2019;) for each $166.6 spent. Or, that&#x2019;s 60 HALYs/$10,000. (You need to have some quite implausible beliefs about population ethics and the badness of death to get these numbers, which <a href=\"/ea/14k/are_you_sure_you_want_to_donate_to_the_against/\">I&#x2019;ve discussed here</a>, but let&#x2019;s leave those aside for now.)</p>\n<p>I&#x2019;m less sure how to do the numbers for Give Directly. Suppose recipients are given $1/day extra and this increase their happiness by 1 (i.e. very improbably taking them from neutral to full happiness) for a year. This is $365 per HALY or 27.4 HALYs/$10,000. (I think this is extremely generous to Give Directly and I&#x2019;m sceptical it increases happiness at all, which I&#x2019;ve <a href=\"/ea/yv/is_effective_altruism_overlooking_human_happiness/\">discussed here</a>, but let&#x2019;s leave this aside too.)</p>\n<p>Basic Needs estimate it costs $14/participate/month to run their programme, which is $168/participant/year.[4]&#xA0;Assume the effect lasts one year and increases happiness by 0.3 over that year. It costs Basic Needs $560 to generate one HALY. Or, that&#x2019;s 17.8 HALYS/$10,000.</p>\n<p>Currently, AMF is well in the lead with $166.6 per HALY, so that&#x2019;s the one to beat. Now, let&#x2019;s assume we could fund a rescheduling campaign in just the UK to change the schedule on just the psychedelic drugs, LSD and psilocybin (magic mushrooms). This would make it easier to use them to research and treat depression and anxiety, which together affect around 1/6th of the UK&#x2019;s 66m population each week.[5] Let&#x2019;s round down the 11m figure down to 10m, to be conservative. Assume the research caused by the rescheduling reveals ways to increase the happiness of each of these 10m people by 0.1 for a single year (I think this is a conservative figure). [Update&#xA0;14/08/2017: following Tom Sittler&apos;s comment below, I should have said that this rescheduling would increase the happiness of those 10m by 0.1&#xA0;<em>on average</em>; it&apos;s unreasonable to assume all will get this treatment. However, we might assume that if some people get the new treatment, which I presume is cheaper, that frees up resources for more people, who wouldn&apos;t have received the old treatment, to get that in the first instance]</p>\n<p>We don&#x2019;t know how much it would cost to run a successful campaign, but we could ask the question the other way: how much could we spend on a successful rescheduling campaign &#x2013; I.e. one that worked, got the law changed and allowed research to happen &#x2013; and for that campaign to still be as cost-effective as our currently most cost-effective pick, AMF?</p>\n<p>On the assumptions made above, the rescheduling campaign would generate 1m HALYs (0.1 HALY for each of 10m people). As AMF generates 1 HALY for $166, a successful rescheduling campaign costing less than &#xA3;166m would be more cost-effective than AMF. If we spend $83m on the campaign it would be twice as cost-effective, if we spend $333m on the campaign before it succeeded that would be half as cost-effective as AMF, and so on.</p>\n<p>To be clear, and before the figure is anchored in the mind, I am not suggesting this is how much a successful rescheduling campaign would, in fact, cost. The point is that if you had &#xA3;166m to spend and thought you could pull off the rescheduling campaign for less than that, you should do that instead of giving you money to AMF as the rescheduling campaign is, in expectation, going to do more good.</p>\n<p>What I suggested is a simple, relatively conservative cost-effectiveness estimate. I&#x2019;ll now add a few more considerations.</p>\n<p>You might worry the chance of success really varies with the size of campaign: you can&#x2019;t believe a campaign for less than $10m would have any impact; thus, adding your $100 once there&apos;s a $10m pot is much more effective, in expectation, than adding your $100 to a &#xA3;1m pot. This could be true, but if it is, this is a reason for a big funder to kick-start the campaign before smaller donors add their money, not a reason to give up on the project.&#xA0;</p>\n<p>You might also worry that an extra $100 here or there can&#x2019;t make a difference between a policy change happening and not happening. This is a &#x2018;sorites&#x2019;-type problem (e.g. &quot;how many grains do you need to add before it become a heap of sand?&quot;)[6] If you think $100 wouldn&#x2019;t change people&#x2019;s minds but $1bn would, you have to accept these extra $100s will matter somewhere because if you keep adding $100s, as eventually you&#x2019;ll get to $1bn. Also, this objection applies just as easily to other interventions, such as AMF: how do you know any of your additional nets made the difference between life and death for someone? Presumably you accept the idea that $7,500 saves a life in expectation.</p>\n<p>Having noted those two worries, I&#x2019;ll now suggest my earlier estimate how much we should be prepared to pay for the rescheduling campaign was conservative.</p>\n<p>I assumed the happiness impact was 0.1. We might expect someone who is depressed or anxious to be below 0 (as 0 is the neutral point, this means there are mostly unhappy) on a -1 to +1 scale. Potentially, someone could go up 1 point if they&#x2019;re going from -0.5 to 0.5, so my 0.1 should be upgraded ten times to 1.</p>\n<p>However, counted against this is consideration the size of the impact is really counterfactual, not absolute. We should be asking: how much better will the treatment they received due to the rescheduling campaign be than what they would have received without it? This can get a bit complicated: maybe the psychedelic-based treatments would be cheaper and more effective, so the government could treat more people; maybe the drugs offer only a tiny improvement; maybe many of those people would never sought or received treatment in either case; maybe the fact some people get&#xA0;cheaper, more-effective&#xA0;psychedelic-assisted therapy frees up resources to treat those who wouldn&apos;t have been treated with current therapy; etc. Having considered the counterfactuals, let&apos;s now divide the cost-effective estimate by three. As we increase the estimate by 10 in the last paragraph and reduce it be 3 in this one, the net increase is 3.3 (i.e. 10/3).</p>\n<p>Next, we should recognise that the rescheduling policy, if it occurred, wouldn&#x2019;t last for just one year, but would last in perpetuity and continue to provide better treatment than the status quo. This also needs to be counterfactual: if some group of EAs didn&#x2019;t campaign for rescheduling, how many years would it be before it happened anyway? 5? 10? 50? Let&#x2019;s say the campaign counterfactually makes the policy happen 10 years earlier, so it has an effective duration of 10 years. An additional confusing thought is that, if we &apos;solved&apos; DPR, then current drug policy campaigners would probably move on to do something else good instead. Let&apos;s ignore this additional fact as it&apos;s unclear how this &apos;replacement&apos; feature is going to play out.</p>\n<p>If Britain changed its drug policies, this might have a knock-on effect around the world as other countries took note and copied. Alternatively, this might be isolated just to Britain. Total isolation seems unlikely, so let&#x2019;s assume a spillover effect and double the number of people affected.</p>\n<p>In considering the rescheduling of drugs for mental health, I haven&#x2019;t factored in any of the other 5 arguments I mentioned in section 2. It seems, if you&#x2019;re going to campaign for drug policy reform, once you&#x2019;re campaigning for rescheduling, the additional cost of campaign for decriminalisation and/or legalisation is presumably quite small and, if the rescheduling did happen, it might do so alongside other drug policy reforms. I haven&#x2019;t assessed the impact of the other 5 arguments but think the impact of mental health is likely to be by far the biggest anyway. Let&#x2019;s cautiously add 5% on top of the effect.</p>\n<p>For this more optimistic estimate we multiply our original &#xA3;166m figure by 3 (counterfactual impact) x 10 (counterfactual duration) x 2 (spillover) x 1.05 (other benefits of drug policy reform) which is a multiplier of 63, giving is &#xA3;10.5bn, a rather high number. While the number is large, I don&#x2019;t find it implausible. It&#x2019;s the result of bringing about a systemic change could potentially improve the lives of very many people by a sizeable amount.</p>\n<p>What the estimated figure of &#xA3;10.5bn means is that, if you believed an imaginary rescheduling campaign would succeed with less than &#xA3;10.5bn of funding, then you should think that campaign would be more cost-effective than giving your money to AMF and, therefore, you should support it over AMF. I accept this is currently hypothetical &#x2013; I haven&#x2019;t identified a place readers could send their money to (yet) &#x2013; but I would ask the reader to make their own guess about how much money it would require to run a successful rescheduling campaign. This will give them a sense of how cost-effective they think drug policy reform is compared to the other charities I mentioned (AMF, Give Directly, Basic Needs). If you believe a cool &#xA3;100m is all that&#x2019;s required to get the laws changed, the rescheduling campaign would be 105 times more cost-effective than AMF.</p>\n<p>If we want an even bigger number to chew on, consider that around 500m people have either depression or anxiety worldwide. If we could improve their happiness level by 0.3 and do so 10 years earlier, we could spend up to $250bn on that campaign and it would be as cost-effective as I&#x2019;ve assumed AMF is.</p>\n<p>I&#x2019;ll now anticipate two objections to my cost-effectiveness estimates and the conclusion I reach.</p>\n<p>First, one could accept everything I&#x2019;ve said is true, but argue another cause is still more cost-effective. I&#x2019;ve claimed DPR looks particularly good at increasing happiness for presently existing people. However, you might, for instance, be a total utilitarian whose wants to maximise the happiness of the history of the universe. If you were such a person, you&#x2019;d care not just about the happiness of those alive today, but about the happiness of all future, possible people too, and might think existential threats to humanity, such as AI safety, as more pressing.</p>\n<p>For those who think something else is more important, I would be very grateful if you could produce some (very rough) estimates of how many times more cost effective money to their preferred cause is than DPR. As far as I&#x2019;m aware, there is only one&#xA0;cost-effectiveness estimates comparing near-term causes like Give Directly and AMF to far-future ones, (Michael Dicken&apos;s, which I&apos;m not smart enough to use) so I don&#x2019;t know how much better X-risk charities are supposed to be.[7] As all plausible moral theories hold improving the happiness of existing people is good, even total utilitarian X-risk advocates should be prepared to support near-term altruism if it can be done cheaply enough (e.g. if you think MIRI, an AI safety research charity, is 5x more effective than AMF, but then conclude DPR is 10x more effective than AMF, you should switch to DPR). Certainly, even if you&#x2019;re largely uninterested in the happiness of present people, the long-run effects benefits of DPR are considerable: stopping the War on Drugs with its associated crime, corruption and instability, as well as helping potentially half a billion mentally ill or drug addicted people get back to work would be quite an economic and societal boon.</p>\n<p>Second, one might object DPR only looks attractive because I&#x2019;ve used a suspicious mechanism to generate the expected value calculations: systemic change campaigns look (delusionally) effective because they have a small chance of affecting so many people. Roughly, the complaint is that I&#x2019;ve found a new Pascal&#x2019;s Mugging. I&#x2019;m not sure how suspicious this kind of mugging is: I&#x2019;m at least talking about real, concretely existing people, rather than conjuring up a near infinity of possible people. There doesn&#x2019;t seem to be anything strange about systemic changes per se; everyone should accept the abolition of slavery, which affected millions, was a substantial systemic change that had a large positive impact. For those who think my estimates are too generous I would welcome them pointing out exactly which part they disagree with; that would be helpful and allow me to improve them.</p>\n<p>An alternate way of pressing the second objection would be to accept the type of expected value calculations I&#x2019;ve used but claim they don&#x2019;t favour DPR more than any another cause. The idea here is to claim &#x201C;fine, but all systemic change campaigns look ludicrously effective&#x201D;. The critic could then generate some additional numbers to show, for instance, it has a higher expected value for private individuals to support a campaign that lobbies governments to increase international development spending than it does for those donors to send their straight cash to Give Directly (or AMF or Basic Needs). On this sort of analysis, one could object what my argument has really done is push EA towards systemic change interventions and away from &#x2018;sticking plaster&#x2019; interventions (those which help one person at a time), such as Give Directly, in general, rather than push EAs towards drug policy reform in particular.</p>\n<p>I think it is possible that systemic change campaigns could have higher expected value than their &#x2018;sticking plaster&#x2019; alternatives. This would be a very interesting result and I would like to see people producing worked out systemic changes estimates for say, poverty and physical health. However, my sense is that DPR is uniquely well placed to be a good systemic change intervention. This is because not only would it not cost governments any more money, it could generate lots of revenue. Drug policy reform is the in the enlightened self-interested of taxpayers and governments. In contrast, raising taxes to fund greater aid spending runs counter to the self-interest of taxpayers, and taking money from one part of government spending to increase international aid will similarly meet resistance from whoever loses by this redistribution. There may be some, but I can&#x2019;t think of any other policy changes that would simultaneously reduce costs and increase happiness and so do without even an initial investment from the government.</p>\n<p><strong>7. What should EAs do now?</strong></p>\n<p>I admit I don&#x2019;t have a top charity EAs should give to, nor an ironed-out plan for DPR campaigning; I haven&#x2019;t got that far. I thought the prudent thing to do, in the first instance, would be to write this up and see if others agree it is an important, unrecognised cause. If there are some crucial considerations that render this area unpromising, it makes sense to establish that now before spending time trying to plan the next steps are in detail.</p>\n<p>Somewhat glibly, my answer to the question &#x201C;what should EAs do now?&#x201D; is &#x201C;Answer that question.&#x201D; Supposing people do agree this is important, I think what&#x2019;s needed is more research to figure out what to do. I don&#x2019;t think I fully understand what the best way to tackle this problem is. More concretely, some obvious next steps would be to talk to the charities in the area, get their thoughts and try to assess where money, time and research could be best used.</p>\n<p>There are also a whole host of questions I&#x2019;ve littered through this document I think need answers. I&#x2019;ve collected them here in case anyone wants to help. These are in the order I raised them, not necessarily of importance:</p>\n<ul>\n<li>How effective and how expensive are treatments of anxiety? Do people relapse? I only have information for depression currently.</li>\n<li>What is the worldwide scheduling system on drugs? How does it differ from place to place?</li>\n<li>How much happiness might be gained from arguments 2-6 that I didn&#x2019;t really included in my cost-effectiveness speculations? Questions that need to be answered to find this out include:</li>\n<li>How valuable is the illicit drug trade? Does the drug trade fuel other crimes including terrorism? How much crime, corruption, etc. would be removed by legalisation? How much happiness would this create?</li>\n<li>How much do governments spend on locking up drug users? How much does getting a criminal record impact one&#x2019;s life prospects and happiness?</li>\n<li>How much could governments raise from legalising and taxes drugs? How much good could this extra money do?</li>\n<li>What would the recreational benefits, if any, be from legalising (some) drugs? How big is this compared to the other benefits?</li>\n<li>What are good ways of thinking about tractability? How effective are public opinions or lobbying campaigns? What are good comparisons to make?</li>\n<li>What are the best campaigning organisations working on this? How do we assess how effective such organisations are? Should we just fund them or try and start something of our own? If so, what?</li>\n<li>What should effective altruists do next?</li>\n</ul>\n<hr>\n<p><!--[endif]--></p>\n<div>\n<p><span><span>[1]</span></span>&#xA0;<!-- [if supportFields]><span\nstyle='mso-bidi-font-family:Calibri;mso-bidi-theme-font:minor-latin'><span\nstyle='mso-element:field-begin;mso-field-lock:yes'></span>ADDIN CSL_CITATION {\n&quot;citationItems&quot; : [ { &quot;id&quot; : &quot;ITEM-1&quot;,\n&quot;itemData&quot; : { &quot;author&quot; : [ { &quot;dropping-particle&quot;\n: &quot;&quot;, &quot;family&quot; : &quot;Bentham&quot;, &quot;given&quot; :\n&quot;J&quot;, &quot;non-dropping-particle&quot; : &quot;&quot;,\n&quot;parse-names&quot; : false, &quot;suffix&quot; : &quot;&quot; } ],\n&quot;id&quot; : &quot;ITEM-1&quot;, &quot;issued&quot; : {\n&quot;date-parts&quot; : [ [ &quot;1789&quot; ] ] }, &quot;title&quot; :\n&quot;An introduction to the principles of morals and legislation&quot;,\n&quot;type&quot; : &quot;book&quot; }, &quot;uris&quot; : [\n&quot;http://www.mendeley.com/documents/?uuid=e3b7b765-53cc-3ce2-8400-44fdb1813d6a&quot;\n] } ], &quot;mendeley&quot; : { &quot;formattedCitation&quot; : &quot;J\nBentham, &lt;i&gt;An Introduction to the Principles of Morals and\nLegislation&lt;/i&gt;, 1789.&quot;, &quot;plainTextFormattedCitation&quot; :\n&quot;J Bentham, An Introduction to the Principles of Morals and Legislation,\n1789.&quot;, &quot;previouslyFormattedCitation&quot; : &quot;J Bentham,\n&lt;i&gt;An Introduction to the Principles of Morals and Legislation&lt;/i&gt;,\n1789.&quot; }, &quot;properties&quot; : { &quot;noteIndex&quot; : 0 },\n&quot;schema&quot; : &quot;https://github.com/citation-style-language/schema/raw/master/csl-citation.json&quot;\n}<span style='mso-element:field-separator'></span></span><![endif]-->J Bentham, <em>An Introduction to the Principles of Morals and Legislation</em>, 1789.<!-- [if supportFields]><span\nstyle='mso-bidi-font-family:Calibri;mso-bidi-theme-font:minor-latin'><span\nstyle='mso-element:field-end'></span></span><![endif]--></p>\n</div>\n<div>\n<p><span><span>[2]</span></span> &#x2018;Maximum &#x2018;sustainable&#x2019; happiness refers to the highest average happiness level one can keep up over a lifetime, which contrasts with maximum &#x2018;peak&#x2019; happiness, the most intensity happiness could can feel at a given point, which is presumably higher than maximum sustainable happiness.</p>\n</div>\n<div>\n<p><span><span>[3]</span></span>&#xA0;<!-- [if supportFields]><span\nstyle='mso-bidi-font-family:Calibri;mso-bidi-theme-font:minor-latin'><span\nstyle='mso-element:field-begin;mso-field-lock:yes'></span>ADDIN CSL_CITATION {\n&quot;citationItems&quot; : [ { &quot;id&quot; : &quot;ITEM-1&quot;,\n&quot;itemData&quot; : { &quot;URL&quot; : &quot;http://www.givewell.org/charities/against-malaria-foundation&quot;,\n&quot;accessed&quot; : { &quot;date-parts&quot; : [ [ &quot;2017&quot;,\n&quot;4&quot;, &quot;21&quot; ] ] }, &quot;author&quot; : [ {\n&quot;dropping-particle&quot; : &quot;&quot;, &quot;family&quot; :\n&quot;GiveWell&quot;, &quot;given&quot; : &quot;&quot;,\n&quot;non-dropping-particle&quot; : &quot;&quot;, &quot;parse-names&quot; :\nfalse, &quot;suffix&quot; : &quot;&quot; } ], &quot;id&quot; :\n&quot;ITEM-1&quot;, &quot;issued&quot; : { &quot;date-parts&quot; : [ [\n&quot;2016&quot; ] ] }, &quot;title&quot; : &quot;Against Malaria Foundation |\nGiveWell&quot;, &quot;type&quot; : &quot;webpage&quot; }, &quot;uris&quot; : [ &quot;http://www.mendeley.com/documents/?uuid=b9de3017-ad32-3f70-9ac0-aaa9cb55219b&quot;\n] } ], &quot;mendeley&quot; : { &quot;formattedCitation&quot; : &quot;GiveWell,\n\\u201cAgainst Malaria Foundation | GiveWell,\\u201d 2016,\nhttp://www.givewell.org/charities/against-malaria-foundation.&quot;,\n&quot;plainTextFormattedCitation&quot; : &quot;GiveWell, \\u201cAgainst Malaria\nFoundation | GiveWell,\\u201d 2016, http://www.givewell.org/charities/against-malaria-foundation.&quot;,\n&quot;previouslyFormattedCitation&quot; : &quot;GiveWell, \\u201cAgainst Malaria\nFoundation | GiveWell,\\u201d 2016,\nhttp://www.givewell.org/charities/against-malaria-foundation.&quot; }, &quot;properties&quot;\n: { &quot;noteIndex&quot; : 0 }, &quot;schema&quot; :\n&quot;https://github.com/citation-style-language/schema/raw/master/csl-citation.json&quot;\n}<span style='mso-element:field-separator'></span></span><![endif]-->GiveWell, &#x201C;Against Malaria Foundation | GiveWell,&#x201D; 2016, http://www.givewell.org/charities/against-malaria-foundation.<!-- [if supportFields]><span\nstyle='mso-bidi-font-family:Calibri;mso-bidi-theme-font:minor-latin'><span\nstyle='mso-element:field-end'></span></span><![endif]--></p>\n</div>\n<div>\n<p><span><span>[4]</span></span>&#xA0;<!-- [if supportFields]><span\nstyle='mso-bidi-font-family:Calibri;mso-bidi-theme-font:minor-latin'><span\nstyle='mso-element:field-begin;mso-field-lock:yes'></span>ADDIN CSL_CITATION {\n&quot;citationItems&quot; : [ { &quot;id&quot; : &quot;ITEM-1&quot;,\n&quot;itemData&quot; : { &quot;author&quot; : [ { &quot;dropping-particle&quot;\n: &quot;&quot;, &quot;family&quot; : &quot;Basic Needs&quot;, &quot;given&quot;\n: &quot;&quot;, &quot;non-dropping-particle&quot; : &quot;&quot;,\n&quot;parse-names&quot; : false, &quot;suffix&quot; : &quot;&quot; } ],\n&quot;id&quot; : &quot;ITEM-1&quot;, &quot;issued&quot; : {\n&quot;date-parts&quot; : [ [ &quot;2016&quot; ] ] }, &quot;title&quot; :\n&quot;Basic Need Annual Report&quot;, &quot;type&quot; : &quot;report&quot; },\n&quot;uris&quot; : [\n&quot;http://www.mendeley.com/documents/?uuid=a4b390ce-7848-350c-a923-708770d668e1&quot;\n] } ], &quot;mendeley&quot; : { &quot;formattedCitation&quot; : &quot;Basic\nNeeds, \\u201cBasic Need Annual Report.\\u201d&quot;, &quot;plainTextFormattedCitation&quot;\n: &quot;Basic Needs, \\u201cBasic Need Annual Report.\\u201d&quot;,\n&quot;previouslyFormattedCitation&quot; : &quot;Basic Needs, \\u201cBasic Need\nAnnual Report.\\u201d&quot; }, &quot;properties&quot; : { &quot;noteIndex&quot;\n: 0 }, &quot;schema&quot; :\n&quot;https://github.com/citation-style-language/schema/raw/master/csl-citation.json&quot;\n}<span style='mso-element:field-separator'></span></span><![endif]-->Basic Needs, &#x201C;Basic Need Annual Report.&#x201D;<!-- [if supportFields]><span\nstyle='mso-bidi-font-family:Calibri;mso-bidi-theme-font:minor-latin'><span\nstyle='mso-element:field-end'></span></span><![endif]--></p>\n</div>\n<div>\n<p><span><span>[5]</span></span>&#xA0;<!-- [if supportFields]><span\nstyle='mso-bidi-font-family:Calibri;mso-bidi-theme-font:minor-latin'><span\nstyle='mso-element:field-begin;mso-field-lock:yes'></span>ADDIN CSL_CITATION {\n&quot;citationItems&quot; : [ { &quot;id&quot; : &quot;ITEM-1&quot;,\n&quot;itemData&quot; : { &quot;author&quot; : [ { &quot;dropping-particle&quot;\n: &quot;&quot;, &quot;family&quot; : &quot;NHS&quot;, &quot;given&quot; :\n&quot;&quot;, &quot;non-dropping-particle&quot; : &quot;&quot;,\n&quot;parse-names&quot; : false, &quot;suffix&quot; : &quot;&quot; } ],\n&quot;id&quot; : &quot;ITEM-1&quot;, &quot;issued&quot; : {\n&quot;date-parts&quot; : [ [ &quot;2014&quot; ] ] }, &quot;title&quot; :\n&quot;Adult Psychiatric Morbidity Survey: Survey of Mental Health and\nWellbeing, England&quot;, &quot;type&quot; : &quot;report&quot; },\n&quot;uris&quot; : [ &quot;http://www.mendeley.com/documents/?uuid=2419a987-e0da-3cf5-b0e6-74b36e3e32f1&quot;\n] } ], &quot;mendeley&quot; : { &quot;formattedCitation&quot; : &quot;NHS,\n\\u201cAdult Psychiatric Morbidity Survey: Survey of Mental Health and\nWellbeing, England,\\u201d 2014, http://content.digital.nhs.uk/catalogue/PUB21748.&quot;,\n&quot;plainTextFormattedCitation&quot; : &quot;NHS, \\u201cAdult Psychiatric\nMorbidity Survey: Survey of Mental Health and Wellbeing, England,\\u201d 2014,\nhttp://content.digital.nhs.uk/catalogue/PUB21748.&quot;,\n&quot;previouslyFormattedCitation&quot; : &quot;NHS, \\u201cAdult Psychiatric\nMorbidity Survey: Survey of Mental Health and Wellbeing, England,\\u201d 2014,\nhttp://content.digital.nhs.uk/catalogue/PUB21748.&quot; },\n&quot;properties&quot; : { &quot;noteIndex&quot; : 0 }, &quot;schema&quot; :\n&quot;https://github.com/citation-style-language/schema/raw/master/csl-citation.json&quot;\n}<span style='mso-element:field-separator'></span></span><![endif]-->NHS, &#x201C;Adult Psychiatric Morbidity Survey: Survey of Mental Health and Wellbeing, England,&#x201D; 2014, http://content.digital.nhs.uk/catalogue/PUB21748.<!-- [if supportFields]><span\nstyle='mso-bidi-font-family:Calibri;mso-bidi-theme-font:minor-latin'><span\nstyle='mso-element:field-end'></span></span><![endif]-->nnhs</p>\n</div>\n<div>\n<p><span><span>[6]</span></span> The paradox of the heap: if you take away a grain of sand from a heap of sand, when does the heap stop being heap? No single grain seems to make a difference but if you keep taking grains away eventually they&#x2019;ll be nothing left, at which point there must be no heap.</p>\n</div>\n<div>\n<p><span><span>[7]</span></span>&#xA0;Michael Dickens&#x2019; cause prioritisation model can be found here (<a href=\"http://mdickens.me/causepri-app/\">http://mdickens.me/causepri-app/</a>). Sadly I am not mathematically competent enough to tweak so it matches my assumptions. I also understand CEA are working on a model comparing near-term to far-future causes.</p>\n</div></body></html>", "user": {"username": "MichaelPlant"}}, {"_id": "iBvmKaqb7YE32W5EK", "title": "Local Group Support Overview: CEA, EAF and LEAN", "postedAt": "2017-08-11T22:07:51.877Z", "htmlBody": "<html><body><p><img src=\"https://i.imgur.com/KkhvvnS.png\" alt=\"EA Org Logos\"></p>\n<p><span>As the Effective Altruism community has grown, efforts have emerged to promote and support the growth of EA groups in universities and cities around the world. Presently there are three organisations supporting EA groups on a large scale. This article outlines the background of each organisation, and clarifies the functional role and division of labour between them. We hope that the post will give the wider EA community a better understanding of the EA Group Support landscape Outreach space, while helping people involved in organising EA groups to identify the best ways to get assistance in different contexts.</span></p>\n<p>&#xA0;</p>\n<p><strong></strong><span>Key Facts:</span></p>\n<ul>\n<li>\n<p><a href=\"https://docs.google.com/spreadsheets/d/1ATRWGcN3GLouaWJIa6Za3xbLe5nuk0CQHhwhsBLTDvA/edit?usp=sharing\"><span>Click here</span></a><span> to see our online resources combined in one place</span></p>\n</li>\n<li>\n<p><span>All groups can approach CEA, LEAN and EAF to inquire about support</span></p>\n</li>\n<li>\n<p><span>Some of EAF&#x2019;s resources and events are only conveniently accessed by German speakers living in the region.</span></p>\n</li>\n</ul>\n<h2><span>Joint Support </span></h2>\n<p><span>The following are jointly provided by CEA, LEAN and EAF:</span></p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><span>The EA Organisers&#x2019; </span><a href=\"https://www.facebook.com/groups/956362287803174/\"><span>Facebook group</span></a></p>\n</li>\n<li>\n<p><span>The Local EA Groups newsletter, aimed at supporting organisers and sharing relevant news and opportunities. New groups are automatically added, but email </span><a href=\"mailto:groupnewsletter@eahub.org\"><span>groupnewsletter@eahub.org</span></a><span> if you think you have been left off of the mailing list</span></p>\n</li>\n<li>\n<p><span>The annual Local EA Groups Survey. This year&#x2019;s survey contains sections for both group members and organisers. </span><a href=\"https://www.surveymonkey.com/r/LGS2017\"><span>Click</span></a><span> to participate!</span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<h2><span>CEA: Local Groups Support</span></h2>\n<p><span>The </span><a href=\"https://www.centreforeffectivealtruism.org/\"><span>Centre for Effective Altruism</span></a><span> (CEA) is non-profit organisation which helps to grow and maintain the effective altruism community. Their mission is to create a global community of people who have made helping others a core part of their lives, and who use evidence and scientific reasoning to figure out how to do so as effectively as possible. </span></p>\n<p><span><br></span><span>Before the Centre for Effective Altruism </span><a href=\"/ea/zn/some_organisational_changes_at_the_centre_for/\"><span>merger</span></a><span>, </span><a href=\"https://www.givingwhatwecan.org/\"><span>Giving What We Can</span></a><span> and EA Build (as a part of EA Outreach) supported local groups independently. CEA has since centralised both EA Outreach and Giving What We Can into one organisation at CEA. This also meant they centralised their support to local EA groups, which is currently provided by CEA&#x2019;s Local Groups Coordinator - Harri Besceli. There is a small minority of EA groups which brand themselves as GWWC and 80,000 Hours groups, however these are supported by CEA in the same way as other EA-branded groups. </span></p>\n<p>&#xA0;</p>\n<p><span>CEA Local Groups Support currently includes 1-1 mentoring, funding for EA Groups and resources and materials collected in the </span><a href=\"https://drive.google.com/drive/u/0/folders/0B6GSBBEzLsorb09MbUg4dEduWkk\"><span>Effective Altruism Groups&#x2019; Google Drive Folder</span></a><span>. </span></p>\n<p>&#xA0;</p>\n<p><span>The support offered is currently being reviewed and updated. A new funding process, opportunities for receiving mentoring, suggested projects for EA groups and an EA Groups page on <a href=\"/effectivealtruism.org\">effectivealtruism.org</a> will be announced by the end of August. </span></p>\n<p>&#xA0;</p>\n<p><span>You can contact CEA&apos;s Local Groups Coordinator at </span><a href=\"mailto:harri.besceli@centreforeffectivelaltruism.org\"><span>harri.besceli@centreforeffectivealtruism.org</span></a></p>\n<p>&#xA0;</p>\n<h2><span>Effective Altruism Foundation: Outreach</span></h2>\n<p><span>The </span><a href=\"https://ea-foundation.org\"><span>Effective Altruism Foundation</span></a><span> (EAF, Stiftung f&#xFC;r Effektiven Altruismus) is an effective altruist project incubator founded in Switzerland in 2013. It supports local groups in the German-speaking area (Germany, Austria, and Switzerland) in the following ways:</span></p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><a href=\"https://effektiveraltruismus.de/lokalgruppen/ressourcen/\"><span>German resources</span></a><span>, e.g. local group guide, leaflets, presentation slides, event flyer templates, and more</span></p>\n</li>\n<li>\n<p><span>Speakers for EA introduction talks</span></p>\n</li>\n<li>\n<p><span>Support for group organizers through a </span><a href=\"https://www.facebook.com/groups/1448918765412301/\"><span>facebook group</span></a><span>, personal advice, and 1-2 in-person local group meetups per year</span></p>\n</li>\n<li>\n<p><span>A list of all </span><a href=\"https://effektiveraltruismus.de/lokalgruppen/\"><span>groups</span></a><span> and </span><a href=\"https://effektiveraltruismus.de/veranstaltungen/\"><span>events</span></a><span> in the German-speaking area</span></p>\n</li>\n</ul>\n<p>&#xA0;</p>\n<p><span>In addition, EAF supports the German-speaking EA movement with a </span><a title=\"German EA landing page\" href=\"https://effektiveraltruismus.de/\"><span>German EA landing page</span></a><span>, </span><a href=\"https://ea-stiftung.org/medien/\"><span>PR and media relations</span></a><span>, </span><a href=\"https://www.facebook.com/groups/effektiver.altruismus/\"><span>social media</span></a><span>, tax-deductible </span><a href=\"https://ea-foundation.org/donate-ea/\"><span>donation regranting</span></a><span> to all EA charities, EAGx conferences, and a </span><a href=\"https://effektiveraltruismus.de/newsletter\"><span>German EA newsletter</span></a><span>.</span></p>\n<p>&#xA0;</p>\n<p><span>You can contact EAF&#x2019;s Local Groups Coordinator at </span><a href=\"mailto:marcello.veronese@ea-stiftung.org\"><span>marcello.veronese@ea-stiftung.org</span></a><span>.</span></p>\n<h2>&#xA0;</h2>\n<h2><span>LEAN: The Local Effective Altruism Network</span></h2>\n<p><span>LEAN is a </span><a href=\"https://rtcharity.org/\"><span>Rethink Charity</span></a><span> project, originally set up by <a href=\"https://eahub.org/user/tom-ash\">Tom Ash</a></span><span> in 2014. </span><span>LEAN&#x2019;s objective is to promote Effective Altruism globally through the initiation of local EA groups, and the support of existing EA groups.</span><span> Our outreach strategy has involved </span><a href=\"https://docs.google.com/document/d/1aflaZxbGUf1ndAzpuxWXy1_J6-64hPx2CRen5IBl35c/edit\"><span>starting new EA presences</span></a><span> by getting in touch with registered EAs in locations with no known EA representation. LEAN also assumed responsibility for an older, existing network of EA groups previously served by the now-defunct THINK project. Since its inception, LEAN has directly initiated or facilitated over 200 groups and presences around the world. LEAN supports local groups by providing:</span></p>\n<p>&#xA0;</p>\n<ul>\n<li>\n<p><a href=\"https://eahub.org/groups\"><span>Public profiles</span></a><span> for Google searches, and visibility on the </span><a href=\"https://eahub.org/map\"><span>Map of EAs</span></a></p>\n</li>\n<li>\n<p><span>Free websites, EA email addresses and </span><a href=\"https://www.meetup.com/\"><span>Meetup.com</span></a><span> use</span></p>\n</li>\n<li>\n<p><span>Conference calls, bringing organisers together for knowledge transfer</span></p>\n</li>\n<li>\n<p><span>Guides, &#x2018;How-To&#x2019;s and the EA Wiki</span></p>\n</li>\n<li>\n<p><span>One-to-one feedback and support</span></p>\n</li>\n<li>\n<p><span>Regular fundraisers (such as Living on Less) for groups to participate in</span></p>\n</li>\n</ul>\n<p><span>In addition to these services, LEAN has recently launched a </span><a href=\"https://drive.google.com/open?id=1-oAMaRiTQpZY4xqZ3buIu6yO-YKQOT3zU7uTS9jt8j4\"><span>Mentoring Programme</span></a><span> in concert with CEA. LEAN will continue to build on its expertise for group management strategy, and use this to publish further guides and resources for organisers. </span></p>\n<p><span>You can contact LEAN&#x2019;s Manager at: </span><a href=\"mailto:richenda@eahub.org\"><span>richenda@eahub.org</span></a></p></body></html>", "user": {"username": "Richenda"}}, {"_id": "Co9wY9E4NykXZWguj", "title": "High Time For Drug Policy Reform. Part 3/4: Policy Suggestions, Tractability and Neglectedess", "postedAt": "2017-08-11T15:17:40.007Z", "htmlBody": "<html><body><p>This is the third of four posts on DPR. In this part I look at what a better approach to drug policy might be and then discuss how neglected and tractable this problem is as cause area of EAs to work on.</p>\n<p>Links to the&#xA0;articles in&#xA0;this series:</p>\n<p><a href=\"/ea/1d8/dpr/\">Part 1&#xA0;</a>(1,800 words): Introduction and Summary.</p>\n<p><a href=\"/ea/1df/high_time_for_drug_policy_reform_introduction_and/\">Part 2</a>&#xA0;(8,000 words): Six Ways DPR Could Do Good And Anticipating The Objections</p>\n<p><a href=\"/ea/1de/high_time_for_drug_policy_reform_policy/\">Part 3</a>&#xA0;(3,000 words): Policy Suggestions, Tractability and Neglectedess.</p>\n<p><a href=\"/ea/1dj/high_time_for_drug_policy_reform_part_44/\">Part 4</a>&#xA0;(3,500 words): Estimating Cost-Effectiveness vs Other Causes; What EA Should Do Next.</p>\n<p><!-- [if !supportLists]--><strong>3.&#xA0;What should drug policies be instead and what should we do to bring them about?</strong></p>\n<p>I hope I&#x2019;ve convinced you by this point that drug policy reform is important. Supposing I have, there are further questions to address about how the laws should change. In this section, I&#x2019;ll suggest a set of potential DPRs and what we might do to bring those about. In section 5, I discuss neglectedness and tractability and in the final section, 6, I generate some speculative cost-effectiveness estimates compared DPR to other interventions EAs might fund.</p>\n<p>Here&#x2019;s a list of options for DPR:</p>\n<p>-rescheduling: changing how easy it is to use drugs as medicines and in research</p>\n<p>-reclassifying: moving certain drugs from one class to another</p>\n<p>-decriminalising: removing the criminal sanctions associated with one or more of the production, supply or possession of drugs</p>\n<p>-depenalising: making the use of drug an administrative rather than a criminal offence (parking tickets are an administrative offense). Note: decriminalising and depenalisation aren&#x2019;t the same. In Portugal, drugs are decriminalised but not depenaliesd. You can&#x2019;t go to jail for taking drugs, but you can get a fine and be sent to a commission.</p>\n<p>-legalising: making it legal to buy and sell (certain types of) drugs.</p>\n<p>-changing the support provided to addicts. One example is &#x2018;shooting galleries&#x2019; where users can get access to heroin and clean needles, another is governments incentivising research into drug replacements to help with addiction.</p>\n<p>-changing how the &#x2018;War on Drugs&#x2019; is fought.</p>\n<p>-reforming drug education is schools so children have a better understanding of the facts as they stand.</p>\n<p>I don&#x2019;t take this list to be exhaustive; there may be options I&#x2019;ve not considered. Given the range of options, there&#x2019;s plenty of scope for disagreement, if you think there should be DPR, exactly what form it should take. I don&#x2019;t pretend to have a very considered opinion, but what follows strikes me as the obvious way to work through the problem.</p>\n<p>At the first cut, it&#x2019;s helpful to mentally separate medical drug policy reform from criminal drug policy reform. One could think we should allow any and all drugs to be use in a medical environment, supposing there is good evidence of their effectiveness, but keep the creating, trafficking, selling or using of drugs illegal. Or you could argue all drugs should be legally available to consumers, but that doctors can&#x2019;t use any of them.</p>\n<p>I&#x2019;d also point on that the different arguments I made above each suggest different sorts of policy changes. My first and second arguments, on drugs for mental health and pain, respectively, push in the direction of changing the medical schedules of those drugs so they can be used in experiments and for treatments. However, they don&#x2019;t clearly suggest any changes to the criminal rules, unless, perhaps, you think people should be able to self-administer these drugs as medicines (a separate debate I don&#x2019;t consider).</p>\n<p>The remaining four arguments more squarely fit with changing the criminal sanctions around drugs. The third argument, on health, suggests we should decriminalise drugs so people don&#x2019;t go to jail for them and can seek treatment for addiction. The health argument doesn&#x2019;t entail depenalisation or legalisation; there&#x2019;s room for debate on which policy best promotes and protects health and I don&#x2019;t offer an answer.</p>\n<p>The fourth argument, on crime, violence, corruption and instability, is partly to do with how the War on Drugs is conducted. You might think these problems are caused by trying to crack down on drug cartels with force, as that&#x2019;s what produces the instability and violence, and we should stop doing that whilst maintaining that drug production, distribution and use are illegal. This amounts to turning a blind eye to the criminal activities of cartels in perhaps the same way some developed country governments (e.g. the UK) don&#x2019;t really try prosecute possession of drugs, but keep it on the statute books. Arguing we should turn a blind eye to drug cartel is rather unsatisfactory as it still leaves lots of corruption and criminal activity unscathed and essentially abandon the inhabitants of those regions, making cartel leaders into de facto dictators. Further, it doesn&#x2019;t look like you&#x2019;d remove the problem in drug-producing countries without legalising the sale of (some) drugs. Creating a legal market would cut into the illegal market and reduce the power of cartels.</p>\n<p>At the drug-consuming-country-end, if you&#x2019;re worried about making recreational drug users into criminals, that suggests depenalisation would be sufficient but legalisation is not necessary. Where the concern is criminal groups selling illegal drugs (as I noted regarding cartels) the implication is that only legalisation would suffice to remove the economic incentives that keep them in business. The problem of addicts committing petty crime to pay for drugs could be solved by offering treatment to support addiction (which may include drugs addicts being offered drugs in &#x2018;shooting galleries&#x2019;). Governments could continue to make drug possession a criminal offence but offer opioid replacement therapy here (i.e. methadone). Countries with more progressive drug policies offer this (e.g. UK) but is unavailable in places like Russia.</p>\n<p>The fifth argument, on revenue, has two parts, one is to do with reducing the costs health and criminal costs associated with drugs (e.g. medical treatments for addicts, imprisoning drug supplies, fighting the War on Drug in drug-producing countries, etc.). I won&#x2019;t go into details but this should require different policy responses for each issue. The other is to do with raising revenue, which can only be done through legalisation. The fairness argument, because it requires drug users to pay taxes, also requires legalisation.</p>\n<p>The sixth argument, on recreation, pushes squarely towards legalisation. If you think something might make people happier and want them to be able to do it, you shouldn&#x2019;t attach criminal or civil sanctions to that activity.</p>\n<p>Where does this leave us?</p>\n<p>My thinking is governments should reschedule all those drugs that may helpful for health conditions to make it easier to conduct research on their effectiveness and use them in treatment as necessary. Specifically, this involves making psilocybin (magic mushrooms), LSD and MDMA schedule II drugs.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/drug-policy-reform-final.docx#_ftn1\"><!-- [if !supportFootnotes]-->[1]<!--[endif]--></a></p>\n<p>There&#x2019;s then the choice of decriminalisation, depenalisation and legalisation. Everything I&#x2019;ve said above speaks in favour of decriminalising all drugs, but I don&#x2019;t have a firm view over which, if any, drugs should be depenalised and which legalised. We should remember we don&#x2019;t need to make the same choice for all drugs, because different drugs have different effects.</p>\n<p>There are arguments pushing in favour of legalisation: it looks like this would have health benefits (users would get regulated substances), it would put drug cartels firmly out of business and remove their corrosive effect on development and corruption, it would raise more money in taxes and it would allow consumers freedom of choice for recreational use, if that is what they wished.</p>\n<p>Pushing against legalisation, we might worry that full legislation allows the public to access some substances, such as heroin and cocaine, that are too addictive and too harmful for their own good.</p>\n<p>The trade-off is that if we keep more apparently drugs illegal, there will presumably still be all the criminal activity that demand for such drugs trade brings. This question is complicated by the fact it&#x2019;s possible if you kept, say, heroin and cocaine, illegal, users would opt for less dangerous drugs instead &#x2013; we saw earlier when mephedrone was briefly legal that seemed to reduce deaths from cocaine use. If users switch to less dangerous legal drugs, then demand for the remaining illegal drugs may dry up and cause criminal networks to wither. It could be the case a partial legalisation is stronger than for full legalisation. Alternatively, this may just be wishful thinking on my part, and the demand for cocaine and heroin will remain undented. This takes me to the limits of my knowledge. It seems that legalising some drugs would be better than the status quo, but I&#x2019;m unsure where to draw the line. Perhaps we should legalise all those drugs up to and including cannabis on the graph of harms I used earlier, but no further. This would mean legalising everything apart from amphetamines, cocaine and heroin (and presumably keeping tobacco and alcohol legal too).<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/drug-policy-reform-final.docx#_ftn2\"><!-- [if !supportFootnotes]-->[2]<!--[endif]--></a></p>\n<p>Supposing you agree with me on some of the above, what&#x2019;s should we do?</p>\n<p>Because all the problems I&#x2019;ve discussed are to do with what governmental regulation, what&#x2019;s required is getting the laws changed. How do we do this? By convincing the public and their politicians that they should be changed. And how do we do that? Here, there&#x2019;s an indirect and a direct option.</p>\n<p>The indirect option is funding research into the effects of various drugs to develop the evidence base and hoping the results of that research will be picked up and generate publicity. There&#x2019;s certainly some useful research that could inform the public debate: what&#x2019;s happened when various states in the USA have legalised cannabis? How do various drugs impact happiness, not just mental health? What are the effects of smoking the whole cannabis plant (studies are often conducted with just one compound from it)? Do the results of psychedelics replicate with much larger groups? Etc.</p>\n<p>However, funding research doesn&#x2019;t seem very promising because, as discussed, it&#x2019;s more expensive to do this research with the current restrictions. That suggests (but doesn&#x2019;t prove) the right order is go for the direct option, campaigning for systemic policy change. Once that&#x2019;s done, then it&#x2019;ll be more cost-effective to fund the research. Campaigning for systemic policy change probably involves funding organisations that run events, generate publicity, write policy documents, meet politicians, co-ordinate with other interested groups in the area, and so on. I&#x2019;m sure lots of EAs will be very nervous about this. Despite protestations that <a href=\"https://80000hours.org/2015/07/effective-altruists-love-systemic-change/\">we love systemic change</a>, it&#x2019;s still not something EAs really do, nor are we used to thinking about it.</p>\n<p><!-- [if !supportLists]-->4.&#xA0;Tractability and neglectedness</p>\n<p>Now we come to tractability &#x2013; how easy is it to get stuff done? &#x2013; and neglectedness &#x2013; how many people are trying to solve this already? My analysis of these are quite shallow and I don&#x2019;t pretend to have all the facts here either. I&#x2019;ll tackle neglectedness first.</p>\n<p>Drug policy reform seems pretty neglected. It turns out there are organisations that work on this but I confess I hadn&#x2019;t heard of anything of them until I actively started researching this, which suggests their visibility is low. Here are some I&#x2019;ve now heard of, with their number of twitter followers in brackets to give you a sense of their digital footprint: Harm Reduction International (11.5k), Transform Drug Policy (26k), Volteface (3k), The Beckley Foundation (22.5k), Release Drugs (18k), the Drug Policy Alliance (73k). We should bear in mind there might be substantial overlap between their followers &#x2013; for instance, I follow all of them. These numbers don&#x2019;t seem high to me: as a couple of comparables, for those not familiar with twitter, Justin Bieber has 97.7m, Oxfam has 826k, Mind (a mental health charity) has 324k, Peter Singer has 77k, Will MacAskill has 13k (and <a href=\"https://twitter.com/MichaelDPlant\">I have 315</a>!).</p>\n<p>What&#x2019;s more, I&#x2019;ve not seem the set of arguments I&#x2019;ve suggested combined anywhere else. For whatever reason, the sort of people who campaign against the War on Drugs because they&#x2019;re interested in international development don&#x2019;t seem to be the same people who are interested in mental health. Mental health campaigners don&#x2019;t (yet) seem to have thought about the potential of drugs to provide new treatments. People interested in the effects of drugs tend not to be interested in the domestic crime associated with them, and so on. What seems particularly neglected, even within drug policy campaigning, is bringing a large number of people who can all agree, albeit for different reasons, DPR is important.</p>\n<p>On to tractability. The main objection I&#x2019;d had from people I&#x2019;ve discussed DPR with is &#x201C;Would be great if it happened; looks really intractable&#x201D;. It seems like a political non-starter. As an example, former Prime Minister David Cameron spoke about legalisation and regulation whilst a back bencher, then recanted on his earlier enthusiasm later on when seeking high office.<a title=\"\" href=\"file:///C:/Users/Michael/Desktop/drug-policy-reform-final.docx#_ftn3\"><!-- [if !supportFootnotes]-->[3]<!--[endif]--></a> The reason no (serious) politician will endorse DPR is because the public are against it. People have been deriding the War on Drugs for years and that hasn&#x2019;t stopped it happening, so it&#x2019;s unlikely anything will change now. As an individual, to say you&#x2019;re in favour of drug legalisation marks you out a hippie weirdo who&#x2019;s not to be taken seriously.</p>\n<p>Against this, I want to suggest both that DPR is much more promising that it seems, that low tractability is not, by itself, sufficient reason to give up on a cause, and there are further things we can do to assess the tractability. I&#x2019;ll explain these in turn.</p>\n<p>Even though DPR has not historically looked tractable, lots has changed in the last few years. The huge new thing is the evidence on the use of psychedelics to treat mental health problems, which gives a justification for DPR that didn&#x2019;t exist before. These experiments have only re-started this decade (after the moratorium that began in the 1960s) and now the public are starting to hear about them. Simultaneously, in the UK at least, mental health is rapidly becoming de-stigmatised and taken seriously in public life: witness the Royal Family&#x2019;s &#x2018;<a href=\"https://www.headstogether.org.uk/\">Heads Together&#x2019;</a> campaign that started in 2015 and the announcement from Theresa May to spend &#xA3;<a href=\"https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwje4Ya9mbTVAhWDZlAKHXs4BYgQqOcBCEEwAw&amp;url=https%3A%2F%2Fwww.theguardian.com%2Fsociety%2F2017%2Fjul%2F31%2Fmental-health-sector-gives-mixed-response-to-13bn-plan-for-better-services&amp;usg=AFQjCNHHw6JhYIk_mZ9xWyEgKfiyeChaYw\">1.3bn more on mental health</a>. When people connect the dots and realise DPR could help mental health, I imagine opinions could change rapidly.</p>\n<p>We can see attitudes to drugs themselves changing too. A number of states in America have legalised cannabis, and this seems set to continue. There seem to be a growing interest in, and acceptability of, &#x2018;micro-dosing&#x2019; LSD. For instance, the Economist&#x2019;s 1843 <a href=\"https://www.1843magazine.com/features/turn-on-tune-in-drop-by-the-office\">magazine cover story</a> in the most recent issue is on the phenomena of Silicon Valley tech companies taking small amounts of LSD to improve creativity. At a higher level, Barack Obama has <a href=\"http://www.washingtontimes.com/news/2015/apr/9/obama-blasts-war-drugs-its-been-very-unproductive/\">called the War on Drugs</a> &#x201C;unproductive&#x201D; and the head of the White House Office of National Drug Control Policy&apos;s <a href=\"https://www.vox.com/policy-and-politics/2015/12/14/10106372/drug-czar-michael-botticelli\">stated</a> &quot;We can&apos;t arrest and incarcerate addiction out of people&#x201D;. Although the current White House <a href=\"https://www.washingtonpost.com/opinions/the-new-war-on-drugs-wont-be-any-more-effective-than-the-old-one/2017/06/22/669260ee-56c3-11e7-a204-ad706461fa4f_story.html?utm_term=.604869ba8e34\">seems keen</a> on War on Drugs, it&#x2019;s worth noting the general change in attitudes. Internationally, the UN General Assembly Special Session on the World Drug Problem held in April 2016 observed a marked and widespread shift in rhetoric from it being solely a security and criminal issue to a health and social one. In the UK, the Liberal Democrats had the legalisation of cannabis in their <a href=\"http://www.telegraph.co.uk/news/2017/05/12/lib-dems-pledge-legalise-cannabis-can-sold-high-street-shops/\">manifesto for the first time</a>. The reason lots of politicians (and indeed, members of the public) have been against DPR not because they think it&#x2019;s a bad idea, but because they know it seems weird to other people (e.g. David Cameron&#x2019;s politically smart change of heart). If we get to a tipping point where it stops being weird and becomes an acceptable (or even smart) opinion to have, we could expect lots of politicians (and people) to switch sides on this issue quite quickly.</p>\n<p>Anecdotally, I&#x2019;ve found it quite easy to convince people of the value of drug policy reform, but that may be a selection bias based on the people I talk to. For instance, most people have no idea that mental health is so bad and that (some) drugs, which they&#x2019;ve always thought were bad for one&#x2019;s state of mind, may actually help treat mental health. It&#x2019;s not that they looked at the evidence and formed a strong view, they&#x2019;ve just not really thought about it and had an intuitive fear of drugs that weakened when presented with evidence. Certainly, I think the argument we should make it easier to for doctors to do research into drugs just to see if they can help miserable people, but we that shouldn&#x2019;t change the law and make it any easier for the public to gain access to drugs, seems pretty hard to object to. That&#x2019;s seems the least controversial, but not the most impactful, line to take.</p>\n<p>All this suggests DPR is far from intractable. However, even if it one were not optimistic about tractability, that wouldn&#x2019;t be sufficient reason to give up altogether. Many EAs believe X-risks, particularly AI safety, are hugely important causes, worth dedicating a lifetime to. In the case of AI safety, this is despite the fact (as I understand it) researchers have high uncertainty regarding what form the problem will take, when it will arrive, or how they will go about solving it. &#xA0;What&#x2019;s more, this low tractability needs to be consider alongside high neglectedness: the fact DPR looks intractable has caused it be very neglected, suggesting that, on the margin, a few more people working on this could have a substantial impact. Effective altruists should be risk neutral with their altruism and aim for the thing with the highest expected value, even if that thing has a low probability.</p>\n<p>I confess I don&#x2019;t know how hard DPR would be, but I can think of some ways we could find this out. We could talk to campaigners, both those campaigning directly for drug reform in particular, as well as for other issues (outsiders might engage less in wishful thinking) and ask them how much money they think they would need to be 10%, 50% and 100% confident they could organise enough people to change policy. This would mirror the strategy of asking AI researchers when they think general AI will be developed. We could also try to find comparable history policy changes and see how easy that was. Maybe the abolition of prohibition in America would be one. I haven&#x2019;t looked into this yet, but I&#x2019;d welcome help from others as well as additional suggestions.</p>\n<p>My guess is that the smart, practical strategy would be concentrate DPR campaigning in one country or region, in the hope of winning over that area and the causing a domino effect elsewhere, rather than spreading efforts thinly all over the world. This could well be the UK, but I haven&#x2019;t put much thought into this either.</p>\n<p>&#xA0;</p>\n<div><hr><!--[endif]-->\n<div>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/drug-policy-reform-final.docx#_ftnref1\"><!-- [if !supportFootnotes]-->[1]<!--[endif]--></a> There are some, fairly limited concerns that giving doctors access to these drugs would lead to them being misused or sold. A particular fear would be the theft and black market re-selling of morphine in the developing world. However, all these concerns exist with current medicines, so this seems like an argument for government oversight of doctors, rather than deny access. And even if there are some costs, presumably those are acceptable if the benefits are large enough.</p>\n</div>\n<div>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/drug-policy-reform-final.docx#_ftnref2\"><!-- [if !supportFootnotes]-->[2]<!--[endif]--></a> As Konrad Seifert notes (personally correspondence), this seems to commit me to arguing tobacco and alcohol should be banned, even as (nearly) everything else becomes legal. I don&#x2019;t think it does, if only for the practical reasoning trying to get alcohol and tobacco banned would be nearly impossible and that&#x2019;s not the important battle to fight anyway.</p>\n</div>\n<div>\n<p><a title=\"\" href=\"file:///C:/Users/Michael/Desktop/drug-policy-reform-final.docx#_ftnref3\"><!-- [if !supportFootnotes]-->[3]<!--[endif]--></a> For a summary, see <a href=\"https://www.theguardian.com/commentisfree/2015/jun/03/david-cameron-drugs-legal-highs-poppers-madness\">https://www.theguardian.com/commentisfree/2015/jun/03/david-cameron-drugs-legal-highs-poppers-madness</a></p>\n</div>\n</div></body></html>", "user": {"username": "MichaelPlant"}}, {"_id": "XvbBCP3ryme5H5RsS", "title": "High Time For Drug Policy Reform. Part 2/4: Six Ways It Could Do Good And Anticipating The Objections", "postedAt": "2017-08-10T19:34:24.567Z", "htmlBody": "<html><body><p>This is the second in a series of four posts on drug policy reform (&apos;DPR&apos;). In this post section 2 discusses six different ways DPR may do good. Section 3 anticipates several objections one might make against DPR if you doubted it would have a positive impact. I note these objections focus on whether DPR is good or bad in absolute terms, rather than whether it is better or worse than other causes EAs might consider. I compare DPR to other causes in part 4 in the series.</p>\n<p><strong>2. Six different ways drug policy reform might benefit the world&#xA0;</strong></p>\n<p>The reader should note my research into these is incomplete and of varying depths. I welcome suggestions and corrections. I&#x2019;ve left out the argument the War on Drugs causes pollution and deforestation. This is for reasons of space and because I doubt it has a large effect.[1]</p>\n<p><strong>2.1 Improved treatment for mental health</strong></p>\n<p>Mental illness is a huge cause of worldwide suffering. There are many mental illness - depression, anxiety, schizophrenia, anorexia, bulimia, etc. - but let&#x2019;s just concentrate on the two largest, depression and anxiety. These each affect over 250m people each year.[2]&#xA0;I&#x2019;ll use the term &#x2018;mental health(/illness)&#x2019; to refer just to depression and anxiety unless I specify otherwise. (I&#x2019;ve&#xA0;<a href=\"/ea/yv/is_effective_altruism_overlooking_human_happiness/\">previously argued</a>&#xA0;mental illnesses may cause more misery than poverty, although I stress nothing rests on which is bigger; we are really interested in cost-effectiveness on the margin, which I discuss later.)</p>\n<p>Treatment for mental health tends to come in two forms. You are sometimes offered psychotherapy, such as cognitive behavioural therapy (&#x2018;CBT&#x2019;), dialectical behaviour therapy, behavioural activation therapy, mindfulness-based stress reduction and so on. This involves visiting a therapist, usually weekly, for a couple of months or longer. Or you can be offered drugs, such as antidepressants (i.e. prozac). Often, people are offered both.</p>\n<p>Therapy is often, but not always, effective: CBT it works about&#xA0;50% of the time for depression (I don&#x2019;t yet have figures for anxiety).[3]&#xA0;Such interventions aren&#x2019;t cheap per person if you&#x2019;re comparing them to things like deworming pills, bednets and vaccines. In the UK, it costs the government about &#xA3;750 to provide a course of CBT.[4]&#xA0;This will be cheaper to deliver the developing world. Basic Needs, the biggest mental health charity in the developing world, claims it cost $14 per participant per month to provide their services.[5]&#xA0;Unfortunately &#x201C;hardly any published evidence exists on the cost-effectiveness of population-based or community-level strategies in or for low-income and middle-income settings&#x201D; so there&#x2019;s nothing very useful to lean on (although I suggest some simplistic estimates in section 6 which is in part 3).[6]</p>\n<p>With anti-depressants, a meta-analysis found they have minimal or no benefit over a placebo for mild to moderate depression but, for patients with very severe depression, the benefit is substantial.[7]&#xA0;&#xA0;</p>\n<p>About 20% of those treated for depression respond to neither therapy or anti-depressants, and are thus classed as having &#x2018;treatment resistant depression&#x2019; (&#x2018;TRD&#x2019;).&#xA0;[8]&#xA0;Many that have depression successfully treated also eventually relapse (29% within 1 year and 54% with 2 years).[9]</p>\n<p>Drug policy reform is important for mental health because, perhaps counter-intuitively, recent research suggests several schedule I drugs (i.e. those not allowed for use in treatment) may actually be effective&#xA0;treatments&#xA0;for mental illnesses. In several small clinical trials conducted in just the last few years, there&#x2019;s very promising evidence that LSD helps with anxiety,[10]&#xA0;that psilocybin (&#x2018;magic mushrooms&#x2019;) and ketamine alleviate depression,[11]&#xA0;and that MDMA (&#x2018;ecstasy&#x2019;) aids recovery from Post-Traumatic Stress Disorder (PTSD).[12]&#xA0;There may be other trials I&#x2019;m unaware of and I would welcome directions to things I&#x2019;ve missed.</p>\n<p>While these have been small clinical trials with just tens of participants, the results are extra-ordinary. As one example, Carhart-Harris et al. (2015) gave a single dose of psilocybin to 12 people with moderate to severe depression classified as treatment-resistant. The subjects had been depressed for a mean average of 17.8 years, a rather long-time. They found:</p>\n<p>Psilocybin was well tolerated by all of the patients, and no serious or unexpected adverse events occurred. The adverse reactions we noted were transient anxiety during drug onset (all patients), transient confusion or thought disorder (nine patients), mild and transient nausea (four patients), and transient headache (four patients). Relative to baseline, depressive symptoms were markedly reduced 1 week (mean QIDS difference &#x2212;11&#xB7;8, 95% CI &#x2212;9&#xB7;15 to &#x2212;14&#xB7;35, p=0&#xB7;002, Hedges&apos; g=3&#xB7;1) and 3 months (&#x2212;9&#xB7;2, 95% CI &#x2212;5&#xB7;69 to &#x2212;12&#xB7;71, p=0&#xB7;003, Hedges&apos; g=2) after high-dose treatment. Marked and sustained improvements in anxiety and anhedonia were also noted.[13]</p>\n<p>For those curious, the procedure was that the patients were given the psilocybin in relaxed clinical setting accompanied by 2 psychiatrists who &#x201C;adopted a non-directive, supportive approach, allowing the patient to experience a mostly uninterrupted inner &#x201C;journey&#x201D;.&#x201D;[14]</p>\n<p>This raises some questions: how do such drugs work? If they are so useful, why are they illegal? How does the current legal situation impede research?</p>\n<p>I won&#x2019;t discuss the first question. That&#x2019;s too much of a detour for this post and I invite the reader to follow my references.</p>\n<p>The short answer to the second one is largely &#x2018;politics&#x2019; and &#x2018;contingent historical facts&#x2019;. At this point it&#x2019;s important to note that, in the UK at least drugs have both a &#x2018;class&#x2019; and a &#x2018;schedule&#x2019;. Class (options: A, B, C) is a criminal classification, with class A drugs being the one you received the harshest punishment for (e.g. heroin is class A, cannabis class B). Schedule (options: 1-5) is a classification of their medical use. Schedule 1 refers to those with no perceived medical value, such as psychedelics, and their use is limited to research. Roughly, the higher on the schedule (i.e. closer to 5) a drug is, the easier it is for doctors and patients to get access to. See&#xA0;<a href=\"https://en.wikipedia.org/wiki/Controlled_Drug_in_the_United_Kingdom\">this for further explanation</a>. (The USA uses one metric, Schedule 1-5, which encompasses both legal status and medical use.)</p>\n<p>Here, I&#x2019;m just talking about scheduling (medical use); I&#x2019;ll come back to criminal classifications later. The present situation regarding scheduling is one where some psychoactive drugs, such as opiates and some stimulants (amphetamines) can be used to treat things like pain and attention-deficit disorders, respectively. Others, such as cannabis, MDMA (ecstasy) and psychedelics (e.g. LSD, magic mushrooms) are schedule one substances that are stringently controlled and not available for medical use.[15]&#xA0;As Nutt et al. explain:</p>\n<p>The reasons for decisions that were made about which drugs should be controlled under this legislation seem to be unclear and inconsistent and may have been made for political rather than health-related reasons. This is because for many drugs the decisions were made before modern scientific methods allowed a proper understanding of their pharmacology and toxicology. As a result, the decision to list MDMA, psilocybin and LSD as United Nations Schedule I drugs was not based on any consideration of their physical harms but on the&#xA0;assumption that there were no medical benefits&#xA0;[emphasis added]. Indeed, recent analyses have shown that there is no relation between the harms of a range of psychoactive drugs and their current legal status in the United Kingdom.[16]</p>\n<p>Whilst the medical benefits of, say, heroin as a painkiller have been long-known, this is not true for drugs like LSD. When the War on Drugs started in the 60s, this (now-outdated) medical understanding was ossified into law. The scheduling is now somewhat self-justifying: it&#x2019;s hard to conduct research into drugs to show they&#x2019;re useful, and because there&#x2019;s no evidence, the drugs remain in schedule 1. Further, because such drugs are in schedule 1, the public assume they must be very dangerous. In fact, population studies have found no associated link between a use of mental health problems and&#xA0;psychedelics&#xA0;(a category of hallucinogens including LSD, mescaline, psilocybin).[17]&#xA0;I haven&#x2019;t looked into to population studies of the health effects of&#xA0;other, non-psychedelic&#xA0;drugs, like amphetamines, but those aren&#x2019;t necessarily the ones that I&#x2019;m arguing should be rescheduled. I suggest those who want a fuller historical explanation should read the Nutt et al. paper cited.</p>\n<p>The current legal situation makes it much harder to do research. Quoting the same paper:</p>\n<p>For example, in the United Kingdom, it is much harder to study cannabis, MDMA and psilocybin than it is to study heroin, even though heroin is a more dangerous drug in terms of its medical and societal harms than these other drugs. However, the recognized therapeutic properties of heroin allow its medical use in the United Kingdom (although not in the United States), and hence it is placed in Schedule 2. Current UK regulations permit all hospitals to hold heroin and other opioids but require each individual hospital to obtain a licence for Schedule 1 drugs; UK Home Office data show that currently only three (out of several thousand) UK hospitals have such a licence. Applying for a licence takes about 1 year, costs many thousands of pounds and, once granted, is subject to regular police reviews. As a consequence, many researchers who would like to work on these pharmacologically fascinating substances cannot afford to do so.</p>\n<p>Drug policy reform, by rescheduling these medically promising drugs, offers enormous potential upsides. It would make it much easier to research how effective these drugs are. If they turn out to be even fractionally as good as they appear, this would provide new, cheaper, more effective treatments for a big (if not the biggest) cause of unhappiness. As a recent paper puts it:</p>\n<p>Serotonergic psychedelics operate through unique mechanisms that show promising effects for a variety of intractable, debilitating, and lethal disorders, and should be rigorously researched.[18]</p>\n<p>Whilst there is a public outrage when governments refuse to pay for a single child&#x2019;s cancer drug because they are too expensive, there is no outcry for governments to grant access to drugs that could change hundreds of millions of lives. The war on drugs is getting in the way of the war on misery.</p>\n<p>I&#x2019;d add treating mental health issues, in addition to reducing misery, could bring substantial economic gains. Evidence shows they are associated with a loss of employment, absenteeism, poor performance within the workplace and premature retirement.[19]&#xA0;Mental illnesses tend to have other co-morbidities which make them an economic cost to health systems.[20]&#xA0;</p>\n<p><strong>2.2 Better medications for pain (section written by Lee Sharkey)</strong></p>\n<p>There are several potential ways that rational reform of drug policies might benefit pain therapy. Before delineating those ways, I should note that some countries, especially the United States and others, have a problem of <em>over-accessibility</em>&#xA0;to opioid medications (e.g. morphine) resulting in an opioid crisis. There are no easy solutions to this crisis, and clinicians and policymakers are reconsidering the recommended uses of opioid pain medications. I won&#x2019;t deal further with this clinical question here, but will say that the discussion below presumes the continuing improvement of clinical practices and other measures to avoid increases in access to opioids leading to increases in their abuse.</p>\n<p>Opiates are essential medicines for the treatment of moderate to severe pain, such as in some postoperative patients, patients with significant injury, patients in need of palliative care, and more. Yet around 80% of people live in countries with low or non-existent access to these essential medicines because of overzealous restrictions on their acquisition, distribution, and use.[21]&#xA0;The International Narcotics Control Board (INCB) estimates that 92% of all morphine is consumed in America, Canada, New Zealand, Australia, and parts of western Europe&#x2014; only 17% of the world&#x2019;s population.[22]&#xA0;There appear to be no data on the total number of patients in need of pain relief who do not receive adequate therapy; indeed, such a number would be controversial as there is no global consensus on the &#x2018;correct level&#x2019; of opioid consumption on a per capita or even individual patient basis, although there is agreement that it is too high in countries like USA and too low in many low-middle income countries. A lower bound for the number of patients in preventable suffering can be found using figures for palliative care alone, in which the most important tool is opioid painkillers: Of the annual 40 million people in need of palliative care, only an estimated 14% receive it.[23]&#xA0;This rough estimate of course omits the many patients in need of strong painkillers for non-palliative care reasons. I discuss how difficult it is for many people to acquire effective pain relief, and the suffering they experience without it, in greater depth in this my&#xA0;<a href=\"/ea/16r/increasing_access_to_pain_relief_an_ea_perspective/\">EA forum article</a>&#xA0;and won&#x2019;t repeat that here in the interests of brevity.</p>\n<p>States maintain these tight restrictions with the intention of protecting their citizens against drug abuse, to ensure safe clinical use and to avoid medications entering the illicit drug supply chain. In countries with low access to controlled pain medications, appropriate liberalisation of drugs policies are needed (1) to enable trained medical staff to use existing pain medications safely and effectively while maintaining compliance with regulations and avoiding their diversion and abuse (for more detailed discussion see the afore-mentioned EA forum&#xA0;<a href=\"/ea/16r/increasing_access_to_pain_relief_an_ea_perspective/\">article</a>)[24]&#xA0;and (2) because it isn&#x2019;t clear that drug prohibition, including of opioids, is having the desired effects. The unintended consequences of prohibition, and alternatives to it, are discussed in below sections.</p>\n<p>More generally, it&#x2019;s not sufficient to impose extreme restrictions on strong painkillers on the grounds they might cause harm. Very many medical procedures, such as surgeries, have the potential to harm a patient, but go ahead because the expected benefits substantially outweigh the expected harms. In particular, the benefits and risks of painkillers are not rationally balanced in countries where there is low to zero access. which is a majority.</p>\n<p>Drug policy reform may also allow us to better understand current pain medications and develop new treatments and uses. Recent years have seen an increase in research on scheduled drugs for pain therapy, especially for cannabis and cannabinoids. Leaving aside the less substantiated claims about the powers of these drugs (of which there are many), the evidence is clear that they offer meaningful relief for patients with chronic pain, patients with nausea resulting from chemotherapy, and patient with pain and spasticity from multiple sclerosis, but significant research gaps remain.[25]&#xA0;Despite clear evidence, in the United States, cannabis remains in the schedule I drug category, and therefore has no officially accepted medical use. Contradictions between scheduling and clinical evidence ensures that research gaps remain hard to fill, including research on the cardiorespiratory, cognitive, or social effects of a widely used substance. More speculatively, there may be safe ways to use other drugs in pain management, but heavy restrictions on research prevent us from knowing. Ketamine is already a widely used painkiller (and for that reason continues to enjoy no international controls under international drug conventions); there is a small amount of evidence that LSD might be useful in pain therapy;[26]&#xA0;and psychedelic-assisted psychotherapy in pain management and palliative care is a field of great untapped potential. Lastly, there is some evidence that some schedule I drugs might be useful for the treatment of addiction which might help mitigate the risks of using existing opioid painkillers.[27]&#xA0;It is plausible that other currently-illegal drugs have therapeutic uses in pain management and palliation, but drug policy reform will be essential to scale up research.</p>\n<p><strong>2.3 Improving public health (all sections hereafter written by Michael Plant)</strong></p>\n<p>The main argument for making drugs illegal is that this protects the public from dangerous, harmful substances. Therefore, so the thought goes, it&#x2019;s necessary to criminalise drug use and even imprison users to act as a deterrent because the alternatives, decriminalisation or even legalisation, would result in more harm overall.</p>\n<p>I&#x2019;ll give five reasons why this argument is less convincing than it seems and probably false.</p>\n<p>First, making drugs illegal can make drug taking more dangerous. Governments regulate alcohol, whereas recreational and addicted drug users can&#x2019;t be sure of the purity or strength of what they&#x2019;re buying. I haven&#x2019;t found figures yet, but I assume a large proportion of drug-related deaths are accidental, rather than deliberate, and caused by people taking too much of the wrong thing, or mixing drugs out of ignorance of their combined effects (UK figures show more than 50% of drug-related death involve opiates, presumably few were intentional).[28]&#xA0;Purer, measured versions could be provided by either 1) legalisation of drug or 2) having health services give addicts what they are addicted to, or a suitable substitute (sometimes called &#x2018;shooting galleries&#x2019; in the UK).&#xA0;</p>\n<p>This still leaves the worry that decriminalising or legalising drugs, even if it makes the drugs safe(r), would be worse overall because it would cause more people to take them. This is clearly an empirical question. Useful evidence comes from Portugal, which decriminalised drugs in 2001. More precisely,&#xA0;the new law maintained the status of illegality for using or possessing any drug for personal use without authorization. However, the offense was changed from a criminal one, with prison a possible punishment, to an administrative one if the amount possessed was no more than a ten-day supply of that substance. To be clear, Portugal decriminalised drugs (no jail time), but didn&#x2019;t depenalise them (you still get a fine) or legalise them (you can&#x2019;t buy them legally in shops).</p>\n<p>What happened? From&#xA0;<a href=\"http://www.economist.com/node/14309861\">The Economist</a>:</p>\n<p>The Cato Institute, a libertarian American think-tank, published a study [in 2009] of the new policy by a lawyer, Glenn Greenwald.[29]&#xA0;In contrast to the dire consequences that critics predicted, he concluded that &#x201C;none of the nightmare scenarios&#x201D; initially painted, &#x201C;from rampant increases in drug usage among the young to the transformation of Lisbon into a haven for &#x2018;drug tourists&apos;, has occurred.&#x201D;</p>\n<p>Mr Greenwald claims that the data show that &#x201C;decriminalisation has had no adverse effect on drug usage rates in Portugal&#x201D;, which &#x201C;in numerous categories are now among the lowest in the European Union&#x201D;. This came after some rises in the 1990s, before decriminalisation. The figures reveal little evidence of drug tourism: 95% of those cited for drug misdemeanours since 2001 have been Portuguese. The level of drug trafficking, measured by numbers convicted, has also declined. And the incidence of other drug-related problems, including sexually transmitted diseases and deaths from drug overdoses, has &#x201C;decreased dramatically&#x201D;.[30]</p>\n<p>Decriminalising doesn&#x2019;t seem to make the problem worse and may make it better. This might seem surprising, but I invite the reader to reflect on how many people really want to ruin their lives with drugs and, of those that are inclined to do so, how many are really stopped from taking them by the threat of criminal sanctions.</p>\n<p>A third health benefit of decriminalisation is that addicts can seek treatment. If asking for help with your problem might land you in jail, you probably won&#x2019;t ask for help. Again, a quote from&#xA0;<a href=\"http://www.economist.com/node/14309861\">the Economist</a>&#xA0;on the Portuguese situation:</p>\n<p>Officials believe that, by lifting fears of prosecution, the policy has encouraged addicts to seek treatment. This bears out their view that criminal sanctions are not the best answer. &#x201C;Before decriminalisation, addicts were afraid to seek treatment because they feared they would be denounced to the police and arrested,&#x201D; says Manuel Cardoso, deputy director of the Institute for Drugs and Drug Addiction, Portugal&apos;s main drugs-prevention and drugs-policy agency. &#x201C;Now they know they will be treated as patients with a problem and not stigmatised as criminals.</p>\n<p>The number of addicts registered in drug-substitution programmes has risen from 6,000 in 1999 to over 24,000 in 2008, reflecting a big rise in treatment (but not in drug use). Between 2001 and 2007 the number of Portuguese who say they have taken heroin at least once in their lives increased from just 1% to 1.1%. For most other drugs, the figures have fallen: Portugal has one of Europe&apos;s lowest lifetime usage rates for cannabis.[31]</p>\n<p>Indeed, whilst people might worry DPR would lead to more addiction, the DPR may help reduce drug addictions. Studies suggest LSD may an effective be a treatment for alcoholism[32]&#xA0;and magic mushrooms for tobacco addiction.[33]&#xA0;In one study, 12 of 15 smokers (i.e. 80%) quit tobacco after 2-3 does of psilocybin (i.e. biological tests showed they had not smoked in 6 months); to the best of my knowledge, this result is unheard of in addiction treatment.[34]&#xA0;Even if some drugs are addictive (heroin), this isn&#x2019;t true across the board. We should be careful not to put all &#x2018;illegal drugs&#x2019; into a single mental category and assume they are all equally bad. This is not something people seem to do with legal drugs: we can understand aspirin, ibuprofen, alcohol, caffeine and tobacco are different substances with different effects.</p>\n<p>The fourth point is a counter-factual one. Banning dangerous substances as a precautionary principle can have perverse effects if it causes people to take a more dangerous drugs instead. A curious case is that of mephedrone. From Nutt et al. (2013) again:</p>\n<p>A more recent and equally controversial amphetamine analogue is mephedrone (also known as 4-methylmethcathinone). This drug was first synthesized in 1929, but was little used until the 2000s, when it was resurrected in Israel as an octopamine analogue to provide a biological control approach for aphids on plants (hence the slang name &#x2018;plant food&#x2019;). It became widely used in Israel by young people, and although there were no reported deaths or serious harms, it was banned by the Knesset. Soon after, it spread to the United Kingdom as a &#x2018;legal high&#x2019;, where it went by various names such as MCAT, drone and miaow-miaow. It became very popular as it was sold in pure form (in contrast to MDMA, which was often of particularly poor quality) and, being legal, could be readily ordered over the Internet. As with MDMA, many media articles claimed that mephedrone has dangerous adverse effects. Coupled with unfounded police suggestions that it had led to deaths, this resulted in mephedrone being banned despite the lack of any real evidence of harm.[35]&#xA0;It was subsequently discovered that the rise in recreational mephedrone use in the United Kingdom in fact had some unexpected benefits, particularly a spectacular fall in the number of deaths due to cocaine use by over 20% in 1 year.[36]&#xA0;This surprising finding could be explained by the fact that many cocaine users switched from cocaine to mephedrone, which is less toxic. Mephedrone thus seems to have saved more lives than it claimed, suggesting it has potential as a substitute for cocaine, like methadone is for heroin. Its illegal status and the fact that many analogues of mephedrone were banned under the same legislation means that this potential is now unlikely to be investigated, let alone realized[37]</p>\n<p>This brings me to my final point about consistency. There seems to be little obvious link between the legal classifications of drugs and how independent experts assess their harmfulness. Two large, independent studies conclude alcohol is the most dangerous when harms to others and harms to users are combined, which they rate as more dangerous even than heroin and crack cocaine.[38][39]</p>\n<p><img src=\"https://cdn.static-economist.com/sites/default/files/20101106_WOC504_0.gif\" alt=\"scoring drugs\"></p>\n<p>If we are to be consistent, we should ban alcohol and tobacco as they seem more dangerous than most(/all) illegal drugs. Smoking, as we all know, kills, which is pretty harmful. Drunkenness causes fights, injuries, stupidity and alcoholism ruins lives and leads to many deaths too. People know this, but seem not to mind, perhaps due to status quo bias (&#x201C;better the devil you know&#x2026;&#x201D;).</p>\n<p>The rejoinder to this is &#x201C;but, if we make drugs legal, people will think they&#x2019;re safe.&#x201D; Arguably, this already happens and is a substantial problem.&#xA0; People drink and smoke, rather than take other drugs, perhaps in part because they believe they are safe. If alcohol and tobacco are more harmful than the drugs people would&#xA0;otherwise have used instead, then our current drug policies foreseeably increase harm, rather than reduce it.</p>\n<p>I haven&#x2019;t yet thought much about how big the happiness gained from health might be.</p>\n<p><strong>2.4 Fuelling crime, corruption, instability and violence</strong></p>\n<p>The international War on Drugs produces crime, corruption, conflict and instability. This is different at different parts of drug trade, so I&#x2019;ll distinguish drug-producing and drug-consuming countries in turn.</p>\n<p>Here&#x2019;s a summary of the problems for drug-producing countries from the Global Commission on Drug Policy:</p>\n<p>Illegal drug profits fuel regional instability by helping to arm insurgent, paramilitary and terrorist groups. The redirection of domestic and foreign investment away from social and economic priorities toward military and policing sectors has a damaging effect on development.&#xA0;[&#x2026;] For instance, the opium trade earns paramilitary groups operating along the Pakistan-Afghanistan border up to $500 million a year [&#x2026;] Estimates of deaths from violence related to the illegal drug trade in Mexico since the war on drugs was scaled-up in 2006 range from 60,000 to more than 100,000</p>\n<p>The illicit drug trade creates a hostile environment for legitimate business interests. It deters investment and tourism, creates sector volatility and unfair competition (associated with money laundering), and distorts the macroeconomic stability of entire countries.&#xA0;</p>\n<p>In Colombia, approximately 2.6 million acres of land were aerially sprayed with toxic chemicals as part of drug crop eradication efforts between 2000 and 2007. Despite their destructive impact on livelihoods and land, the number of locations used for illicit coca cultivation actually increased during this period.</p>\n<p>The illicit drug business also corrodes governance. A 1998 study from Mexico estimated that cocaine traffickers spent as much as $500 million a year on bribes, more than the annual budget of the Mexican attorney general&#x2019;s office. As of 2011, Mexican and Colombian drug trafficking groups launder up to $39 billion a year in wholesale distribution proceeds.[40]</p>\n<p>We might think the solution to all this is simply a more aggressive War on Drugs to finally put the cartels out of business. This, however, flies in the faces of economic reality. The demand for drugs is huge and persistent:</p>\n<p>Drug&#xA0;prohibition&#xA0;has fuelled a global illegal trade estimated by the UNODC to be in the hundreds of billions. According to 2005 data, production was valued at $13 billion, the wholesale industry priced at $94 billion and retail estimated to be worth $332 billion. The wholesale valuation for the drugs market is higher than the global equivalent for cereals, wine, beer, coffee, and tobacco combined.[41]</p>\n<p>Drug enforcement can exacerbate the problem. Economic logic dictates this would raise prices and cause more criminal groups to enter the market. Successfully removing one group causes others to fight for market share.[42]&#xA0;What&#x2019;s more, the War on Drugs has not succeeded:</p>\n<p>UNODC&#x2019;s &#x2018;best estimate&#x2019; for the number of users worldwide (past year use) rose from 203 million in 2008, to 243 million in 2012 &#x2013; an 18 per cent increase, or a rise in prevalence of use from 4.6 per cent to 5.2 per cent in four years.</p>\n<p>Global illicit opium production increased by more than 380 per cent since 1980, rising from 1,000 metric tons to over 4,000 today. Meanwhile, heroin prices in Europe fell by 75 per cent since 1990 and by 80 per cent in the US since 1980, even as purity has risen[43]</p>\n<p>I don&#x2019;t have anything like a full estimate of how many people the drug trade effects and how much misery could be removed by drug policy reform, but this seems quite large in scale.</p>\n<p>I would note those who currently support charities like AMF and Give Directly because they think, amongst other things, such charities usefully contribute to economic development and human progress, should be interested in drug policy reform for these very same reasons. It looks like the international war on drugs really sets back development in a major way and stopping it would be a potentially strong candidate for a systemic change solution that would alleviate lots of poverty. It may, in fact, be better at this than either AMF or GD, but I haven&#x2019;t done any analysis on this yet.</p>\n<p>Moving to the drug-consuming countries (which can also be drug-producing countries) drug prohibition causes several problems. Much street crime is related to the drug trade: rivals gangs who fight for control of the market and robbery committed by addicts seeking money to fund their habits. I don&#x2019;t yet have good figures for either of these, but one source I found suggested about 30 percent of crimes leading to arrests in the United Kingdom had as their motive the need to find money for crack or cocaine&#xA0;[44]&#xA0;and another source claimed that in 2004, 17% of U.S. State prisoners and 18% of Federal inmates said they committed their current offense to obtain money for drugs[45]&#xA0;(I haven&#x2019;t yet established where those sources got their figures from). If drugs were legal, that would remove the financial incentives for criminal activities, and if addicts were treated, or given access to drugs (via so-called &#x2018;shooting galleries&#x2019;) that should reduce the number of drug user committing crimes in the first place.</p>\n<p>Another concern is that efforts to crack down on the trade involve turning millions of otherwise law-abiding drug users into criminals. It seems grossly disproportionate to put people behind bars, particularly as a criminal record can potentially ruins lives (such as by making it hard to find work). Spending time in jail may &#x2018;harden&#x2019; prisoners and make them more likely to commit other crimes. I don&#x2019;t yet have any data or estimates on the impact of this, but one example would be&#xA0;<a href=\"https://en.wikipedia.org/wiki/Timothy_L._Tyler\">Timothy Tyler</a>&#xA0;who was sentenced to life in prison aged 24 for selling LSD.[46]&#xA0;He was granted clemency by Barack Obama after 22 years and will be released in 2018. Pursuing drug users takes a huge amount of time and effort by police, courts and prisons that could be better redirected elsewhere. I discuss some of this further in the next section.</p>\n<p>For those motivated by social justice, an additional reason to be in favour of drug policy reform as the drug law enforce seems to disproportionally effect the world&#x2019;s poor and minorities. For instance, in the US, African Americans make up 13 per cent of the population. Yet they account for 33.6 per cent of drug arrests and 37 per cent of people sent to state prison on drug charges.[47]</p>\n<p><strong>2.5 Raising government revenue, reducing wasteful expenditure, enhancing fairness</strong></p>\n<p>At the moment, governments around the world spends lots of money fighting the war on illicit drugs through both international and domestic law enforcement. At the same time, governments gain no money from the trade of illegal drugs, because those operate on the black market. This contrasts with tobacco and alcohol, where governments don&#x2019;t fund a war against them but do raise lots of money from people buying them.</p>\n<p>If governments decriminalised drug use, they would have to spend much less money putting people in jail. Jail is expensive: UK Government spend around &#xA3;40k a year per prisoners.[48]&#xA0;If governments legalised drug use and then taxed it, they could raise lots of revenue to spend on other policies.&#xA0;<span>Current and projected tax revenues from legal cannabis in some US states is large; it California the state estimates it will raise &#xA3;1bn in <a href=\"https://www.forbes.com/sites/debraborchardt/2017/04/11/1-billion-in-marijuana-taxes-is-addicting-to-state-governors/#16974f152c3b\">revenue annually</a>.</span></p>\n<p>There&#x2019;s an additional argument to be made here on fairness grounds. Currently, at least in the UK, if you drink too much and get yourself in a fight or cause yourself health problems (e.g. cirrhosis, broken bones), it&#x2019;s the government (I.e. the taxpayer) who foots the bill, at least of health care is socialised. In some ways, this isn&#x2019;t that unfair because the government taxes alcohol consumption so alcohol drinkers, at least a group, pay for the costs of alcohol abuse. An Institute for Economic Affairs (a UK think tank) report finds:</p>\n<p>The direct costs of alcohol use to the government in England, including the NHS, police, criminal justice system and welfare system. Taken together, they amount to a gross cost of &#xA3;3.9 billion per annum (in 2015 prices) revenues from alcohol taxation in England amount to &#xA3;10.4 billion, leaving an annual net benefit to the government of &#xA3;6.5 billion.[49]</p>\n<p>Whereas the same does not hold for drug abusers. The government picks up the costs for their activities, but because drugs are illegal, drug users (as a group) contribute nothing in taxes. If drugs were legalised and taxed, this would seem fairer. Again, I don&#x2019;t have estimates of the figures involved. I also note governments that wish to legalise some drugs do not need to legalise all of them. Different drugs have different effects and should be assessed by their individual effects. There&#x2019;s nothing inconsistent in legalising cannabis but deciding heroin is too addictive and dangerous to be legal, though in both cases policy decisions ought to be based on the evidence.</p>\n<p><strong>2.6 Recreational benefits and a liberty argument</strong></p>\n<p>The final argument I want to suggest is also quite obvious. One reason to make drugs more available is because people might enjoy them. Many people seem to enjoy alcohol and find nothing problematic about using it. Unless we can identify particular reasons to make a drug illegal, such as it being highly addictive and harmful, the obvious thought is that we should give people the choice and let them decide for themselves if they want to use them or not.</p>\n<p>An objection to this line of reasoning is that drugs are potentially dangerous and we should instead apply a precautionary principle. However, I&#x2019;m not sure what this precautionary principle would be. I can&#x2019;t be &#x2018;ban things that could be dangerous&#x2019;[50]&#xA0;because many things, like driving, riding horses, extreme sports and alcohol are dangerous and we think it&#x2019;s better to let people do them anyway at their own risk.</p>\n<p>It can&#x2019;t be &#x2018;ban things until we have academic studies showing they increase happiness overall&#x2019; either.[51]&#xA0;That&#x2019;s also too strong. Do we have evidence horse-riding or parachutes increase happiness?[52]&#xA0;We don&#x2019;t, nor do we think we need it. If we require evidence of the positive effects of drugs, there is some available: in a trial, 14 months after taking psilocybin over 50% of the subjects rated the experience as among the top 5 most meaningful and significant of their lives.[53]</p>\n<p>Another option would be &#x2018;ban&#xA0;drugs&#xA0;until we have evidence they increase happiness overall&#x2019;. This is slightly narrower, but it&#x2019;s problematically&#xA0;ad hoc. Why should this principle apply just to drugs? Even we did use this principle, I note it implies we should also ban tobacco and alcohol.</p>\n<p>I&#x2019;m not sure what a good precautionary principle would look like and I leave this here.</p>\n<p>The other argument in this category is a liberty-based one. Simply, the thought is we should allow people to do what they want unless it harms other people; hence unless we can show a type of drug use harms others, it should be legal. Here we would need to discriminate based on the drug and, as I suggested before, it&#x2019;s not obvious alcohol or smoking would remain legal on this basis.</p>\n<p>I note two possible objections to the recreational argument here. First, that making drugs legal would remove the excitement recreational experience have from doing something illicit. This is possibly true, although it seems very trivial. If we buy it, we may also want consider making other things, such as sex, illegal so that people find it more exciting too.</p>\n<p>Second, there&#x2019;s something apparently contradictory in claiming DPR wouldn&#x2019;t be bad because it wouldn&#x2019;t increase drug use (as I suggested above in the context of health), whilst also claiming DPR would be good because it would increase drug use (in a recreational context). This is only apparently contradictory. The thought is DPR would lessen the problems associated with drug addiction by treating addicts, but could increase non-harmful recreational use of safer, regulated drugs. The position is really no more contradictory than claiming alcoholism is bad, but moderate drinking is harmless or even good for happiness.</p>\n<p><strong>3. Objections &#x2013; who should oppose drug policy reform?</strong></p>\n<p>Drug policy looks very promising. This finding has surprised me, so I&#x2019;ll anticipate five objections against DPR here before getting into discussion of its comparative tractability and neglectedness in the following sections. These objections are not in any particular order if importance.</p>\n<p>First, you could just think concerns about something else, such as the far future or animals, dominates. I produce some cost-effectiveness estimates on DPR in section 6 and I&#x2019;ll delay discussion this problem until then.</p>\n<p>Second, you might be really concerned the health risks from decriminalising or legalising drugs are much greater than I&#x2019;ve suggested. It&#x2019;s possible I&#x2019;ve missed something and I would welcome further evidence. This shouldn&#x2019;t put you off the whole area instantly; you&#x2019;d still need to weight up the costs against the benefits. This wouldn&#x2019;t give you a reason to object to the rescheduling of drugs for research purposes though.</p>\n<p>Third, we might wonder how sensitive DPR is to your moral views: are there some positions that, if you endorse them, would lead you to think DPR is uninteresting or objectionable? I&#x2019;ll run through some options and suggest DPR should be widely acceptable. It&#x2019;s not sensitive to your account of well-being, what you think ultimately makes someone&#x2019;s life go well for them. Whilst hedonists think well-being consists only in happiness, all plausible accounts of well-being will value happiness; it&#x2019;s not clear I&#x2019;m suggesting anything non-hedonists will find objectionable. I am not, for instance, suggesting we force feed members of the public drugs against their knowledge (nor, for what it&#x2019;s worth, does it look like this would increase happiness anyway, so the hedonist would also object to this too). My conclusions don&#x2019;t change if you&#x2019;re a prioritarian (you believe happiness for each person has diminishing moral value) because presumably the worst off in terms of happiness are those in great emotional or physical pain.[54]&#xA0;There&#x2019;s no objection on Scanlonian contractualist grounds that the gain to each individual would be trivial; alleviating emotional and physical pain would be large for each individual considered separately.[55]&#xA0;Negative utilitarians, those who value decreasing unhappiness more than increase happiness, should find much to like as DPR is likely to decrease lots of unhappiness.[56]&#xA0;And my suggestions don&#x2019;t contravene any obvious non-welfarist principles I can think of, such as equality or preserving the environment; indeed, use of psychedelics predicts pro-environmental behavioural.[57]&#xA0;Those motivated by fairness, social justice or protecting rights have things to get their teeth stuck into: making users pay for their costs, reducing the criminalisation of minorities, and protecting the human rights of the mentally ill, respectively.[58]</p>\n<p>However, I suppose one could make sort of purity-based argument that drug use is itself morally bad even if it doesn&#x2019;t harm anyone. The difficulty with this would be finding a non-ad hoc&#xA0;justification for claiming some drugs (e.g. heroin) and not others (e.g. caffeine) are bad. Or, again for purity reasons, would might think drug liberalisation, of one form or another, is still bad even if it reduces harm. This seems pretty implausible as a moral principle and faces the levelling-down objection.[59]</p>\n<p>Fourth, there&#x2019;s the concern that, if drugs were legalised, this would give corporations strong incentives to get people addicted to drugs in a similar way to how &#x2018;Big Tobacco&#x2019; do in many parts of the world. My suggestion here is to roll out the same sort of regulatory regime there is the UK, where you have heavy taxes, age restrictions, a ban on advertising and plain packaging of cigarette packs. Companies would only be allowed to sell particular chemical compounds, thus mitigating the potential fear they would develop more addictive versions of existing drugs.[60]</p>\n<p>Fifth and finally, you might find yourself in a state of deep scepticism with regards to everything I&#x2019;ve said, even if you aren&#x2019;t sure where the argument has gone wrong. To some extent, I share this feeling: I&#x2019;d thought drugs were dangerous and therefore keeping them illegal was the best option. It&#x2019;s therefore surprising to reach the conclusion drug policy reform might a huge opportunity to improve happiness. However, I now think my earlier beliefs were lazy assumptions produced by internalising societal fears about drugs, rather than something based on any evidence or serious consideration. I would encourage those who are sceptical of my conclusions to also question how evidence-based their reaction is. I would very much welcome counter-arguments to everything I&#x2019;ve said: if I&#x2019;m missed important facts or reasoning and drug policy reform is a terrible idea then I don&#x2019;t want to be advocating for it.</p>\n<p>&#xA0;</p>\n<p>Links to the&#xA0;articles in&#xA0;this series:</p>\n<p><a href=\"/ea/1d8/dpr/\">Part 1&#xA0;</a>(1,800 words): Introduction and Summary.</p>\n<p><a href=\"/ea/1df/high_time_for_drug_policy_reform_introduction_and/\">Part 2</a>&#xA0;(8,000 words): Six Ways DPR Could Do Good And Anticipating The Objections</p>\n<p><a href=\"/ea/1de/high_time_for_drug_policy_reform_policy/\">Part 3</a>&#xA0;(3,000 words): Policy Suggestions, Tractability and Neglectedess.</p>\n<p><a href=\"/ea/1dj/high_time_for_drug_policy_reform_part_44/\">Part 4</a>&#xA0;(3,500 words): Estimating Cost-Effectiveness vs Other Causes; What EA Should Do Next.</p>\n<hr>\n<div>&#xA0;</div>\n<div>&#xA0;</div>\n<div>[1]&#xA0;For more, see&#xA0;Count The Costs, &#x201C;The Alternative World Drug Report,&#x201D; 2016, http://www.countthecosts.org/alternative-world-drug-report.</div>\n<div>&#xA0;</div>\n<div>[2]&#xA0;Theo Vos et al., &#x201C;Global, Regional, and National Incidence, Prevalence, and Years Lived with Disability for 301 Acute and Chronic Diseases and Injuries in 188 Countries, 1990&#x2013;2013: A Systematic Analysis for the Global Burden of Disease Study 2013,&#x201D;&#xA0;The Lancet&#xA0;386, no. 9995 (2015): 743&#x2013;800, doi:10.1016/S0140-6736(15)60692-4.</div>\n<div>&#xA0;</div>\n<div>[3]&#xA0;Keith S. Dobson et al., &#x201C;Randomized Trial of Behavioral Activation, Cognitive Therapy, and Antidepressant Medication in the Prevention of Relapse and Recurrence in Major Depression.,&#x201D;&#xA0;Journal of Consulting and Clinical Psychology&#xA0;76, no. 3 (June 2008): 468&#x2013;77, doi:10.1037/0022-006X.76.3.468.</div>\n<div>&#xA0;</div>\n<div>[4]&#xA0;Muralikrishnan Radhakrishnan et al., &#x201C;Cost of Improving Access to Psychological Therapies (IAPT) Programme: An Analysis of Cost of Session, Treatment and Recovery in Selected Primary Care Trusts in the East of England Region,&#x201D;&#xA0;Behaviour Research and Therapy&#xA0;51, no. 1 (January 2013): 37&#x2013;45, doi:10.1016/j.brat.2012.10.001.</div>\n<div>&#xA0;</div>\n<div>[5]&#xA0;Basic Needs, &#x201C;Basic Need Annual Report,&#x201D; 2016, http://www.basicneeds.org/what-we-do/our-impact/.</div>\n<div>&#xA0;</div>\n<div>[6]&#xA0;Patel, V et al. (2015). Addressing the burden of mental, neurological, and substance use disorders: key messages from Disease Control Priorities.&#xA0;The Lancet.&#xA0;These figures should&#xA0;be treated with caution. As the authors note, p1681,&#xA0;Vikram Patel et al., &#x201C;Efficacy and Cost-Effectiveness of Drug and Psychological Treatments for Common Mental Disorders in General Health Care in Goa, India: A Randomised, Controlled Trial,&#x201D;&#xA0;The Lancet&#xA0;361, no. 9351 (2003): 33&#x2013;39.</div>\n<div>&#xA0;</div>\n<div>[7]&#xA0;Jay C Fournier et al., &#x201C;Antidepressant Drug Effects and Depression Severity: A Patient-Level Meta-Analysis.,&#x201D;&#xA0;JAMA&#xA0;303, no. 1 (January 6, 2010): 47&#x2013;53, doi:10.1001/jama.2009.1943.</div>\n<div>&#xA0;</div>\n<div>[8]&#xA0;Jambur Ananth, &#x201C;Treatment-Resistant Depression,&#x201D;&#xA0;Psychotherapy and Psychosomatics&#xA0;67, no. 2 (March 13, 1998): 61&#x2013;70, doi:10.1159/000012261; Bradley N Gaynes, &#x201C;Identifying Difficult-to-Treat Depression: Differential Diagnosis, Subtypes, and Comorbidities.,&#x201D;&#xA0;The Journal of Clinical Psychiatry&#xA0;70 Suppl 6, no. SUPPL. 6 (2009): 10&#x2013;15, doi:10.4088/JCP.8133su1c.02.</div>\n<div>&#xA0;</div>\n<div>[9]&#xA0;Jeffrey R. Vittengl et al., &#x201C;Reducing Relapse and Recurrence in Unipolar Depression: A Comparative Meta-Analysis of Cognitive-Behavioral Therapy&#x2019;s Effects.,&#x201D;&#xA0;Journal of Consulting and Clinical Psychology&#xA0;75, no. 3 (2007): 475&#x2013;88, doi:10.1037/0022-006X.75.3.475.</div>\n<div>&#xA0;</div>\n<div>[10]&#xA0;Peter Gasser, Katharina Kirchner, and Torsten Passie, &#x201C;LSD-Assisted Psychotherapy for Anxiety Associated with a Life-Threatening Disease: A Qualitative Study of Acute and Sustained Subjective Effects,&#x201D;&#xA0;Journal of Psychopharmacology&#xA0;29, no. 1 (January 2015): 57&#x2013;68, doi:10.1177/0269881114555249.</div>\n<div>&#xA0;</div>\n<div>[11]&#xA0;See e.g.&#xA0;Robin L Carhart-Harris et al., &#x201C;Psilocybin with Psychological Support for Treatment-Resistant Depression: An Open-Label Feasibility Study,&#x201D;&#xA0;The Lancet Psychiatry&#xA0;3, no. 7 (July 2016): 619&#x2013;27, doi:10.1016/S2215-0366(16)30065-7; Stephen Ross et al., &#x201C;Rapid and Sustained Symptom Reduction Following Psilocybin Treatment for Anxiety and Depression in Patients with Life-Threatening Cancer: A Randomized Controlled Trial,&#x201D;&#xA0;Journal of Psychopharmacology&#xA0;30, no. 12 (December 30, 2016): 1165&#x2013;80, doi:10.1177/0269881116675512; Roland R Griffiths et al., &#x201C;Psilocybin Produces Substantial and Sustained Decreases in Depression and Anxiety in Patients with Life-Threatening Cancer: A Randomized Double-Blind Trial,&#x201D;&#xA0;Journal of Psychopharmacology&#xA0;30, no. 12 (December 30, 2016): 1181&#x2013;97, doi:10.1177/0269881116675513.on psilocybin and&#xA0;&#xA0;Polly Taylor et al., &#x201C;Ketamine&#x2014;the Real Perspective,&#x201D;&#xA0;The Lancet&#xA0;387, no. 10025 (2016): 1271&#x2013;72, doi:10.1016/S0140-6736(16)00681-4; Rebecca B. Price et al., &#x201C;Effects of Intravenous Ketamine on Explicit and Implicit Measures of Suicidality in Treatment-Resistant Depression,&#x201D;&#xA0;Biological Psychiatry, vol. 66, 2009, doi:10.1016/j.biopsych.2009.04.029; Olivia F O&#x2019;Leary, Timothy G Dinan, and John F Cryan, &#x201C;Faster, Better, Stronger: Towards New Antidepressant Therapeutic Strategies,&#x201D;&#xA0;European Journal of Pharmacology&#xA0;753 (April 2015): 32&#x2013;50, doi:10.1016/j.ejphar.2014.07.046.&#xA0;on ketamine.</div>\n<div>&#xA0;</div>\n<div>[12]&#xA0;Peter Oehen et al., &#x201C;A Randomized, Controlled Pilot Study of MDMA (&#xB1;3,4-Methylenedioxymethamphetamine)-Assisted Psychotherapy for Treatment of Resistant, Chronic Post-Traumatic Stress Disorder (PTSD),&#x201D;&#xA0;Journal of Psychopharmacology&#xA0;27, no. 1 (January 2013): 40&#x2013;52, doi:10.1177/0269881112464827.. I note PTSD is distinct from either depression or anxiety. PTSD seems to affect about 3.6% of people in a given year, retrieved from&#xA0;<a href=\"http://www.who.int/mediacentre/news/releases/2013/trauma_mental_health_20130806/en/\">http://www.who.int/mediacentre/news/releases/2013/trauma_mental_health_20130806/en/</a></div>\n<div>&#xA0;</div>\n<div>[13]&#xA0;Carhart-Harris et al., &#x201C;Psilocybin with Psychological Support for Treatment-Resistant Depression: An Open-Label Feasibility Study.&#x201D;</div>\n<div>&#xA0;</div>\n<div>[14]&#xA0;Ibid.</div>\n<div>&#xA0;</div>\n<div>[15]&#xA0;David J. Nutt, Leslie A. King, and David E. Nichols, &#x201C;Effects of Schedule I Drug Laws on Neuroscience Research and Treatment Innovation,&#x201D;&#xA0;Nature Reviews Neuroscience&#xA0;14, no. 8 (June 12, 2013): 577&#x2013;85, doi:10.1038/nrn3530.</div>\n<div>&#xA0;</div>\n<div>[16]&#xA0;Ibid.&#xA0;In support of the last claim about the mismatch between harm and legal status, the author cites the following two studies:&#xA0;David Nutt et al., &#x201C;Development of a Rational Scale to Assess the Harm of Drugs of Potential Misuse.,&#x201D;&#xA0;Lancet (London, England)&#xA0;369, no. 9566 (March 24, 2007): 1047&#x2013;53, doi:10.1016/S0140-6736(07)60464-4; David J Nutt et al., &#x201C;Drug Harms in the UK: A Multicriteria Decision Analysis.,&#x201D;&#xA0;Lancet (London, England)&#xA0;376, no. 9752 (November 6, 2010): 1558&#x2013;65, doi:10.1016/S0140-6736(10)61462-6.</div>\n<div>&#xA0;</div>\n<div>[17]&#xA0;P&#xE5;l-&#xD8;rjan Johansen and Teri Suzanne Krebs, &#x201C;Psychedelics Not Linked to Mental Health Problems or Suicidal Behavior: A Population Study,&#x201D;&#xA0;Journal of Psychopharmacology&#xA0;29, no. 3 (March 5, 2015): 270&#x2013;79, doi:10.1177/0269881114568039; Zoe Cormier, &#x201C;No Link Found between Psychedelics and Psychosis,&#x201D;&#xA0;Nature, March 4, 2015, doi:10.1038/nature.2015.16968.</div>\n<div>&#xA0;</div>\n<div>[18]&#xA0;D. E. Nichols, M. W. Johnson, and C. D. Nichols, &#x201C;Psychedelics as Medicines: An Emerging New Paradigm,&#x201D;&#xA0;Clinical Pharmacology and Therapeutics&#xA0;101, no. 2 (2017), doi:10.1002/cpt.557.</div>\n<div>&#xA0;</div>\n<div>[19]&#xA0;D McDaid, M Knapp, and C Curran, &#x201C;Mental Health III: Funding Mental Health in Europe,&#x201D; 2005, http://apps.who.int/iris/bitstream/10665/107633/1/E85489.pdf.</div>\n<div>&#xA0;</div>\n<div>[20]&#xA0;BJ Miller, CB Paschall III, and DP Svendsen, &#x201C;Mortality and Medical Comorbidity among Patients with Serious Mental Illness,&#x201D;&#xA0;Focus, 2008, http://focus.psychiatryonline.org/doi/abs/10.1176/foc.6.2.foc239.</div>\n<div>&#xA0;</div>\n<div>[21]&#xA0;Marie-Josephine Seya et al., &#x201C;A First Comparison Between the Consumption of and the Need for Opioid Analgesics at Country, Regional, and Global Levels,&#x201D;&#xA0;Journal of Pain &amp; Palliative Care Pharmacotherapy&#xA0;25, no. 1 (March 15, 2011): 6&#x2013;18, doi:10.3109/15360288.2010.536307.</div>\n<div>&#xA0;</div>\n<div>[22]&#xA0;International Narcotics Control Board, &#x201C;Annual Report 2014,&#x201D; 2014, https://www.incb.org/incb/en/publications/annual-reports/annual-report-2014.html.</div>\n<div>&#xA0;</div>\n<div>[23]&#xA0;&#x201C;Palliative Care,&#x201D;&#xA0;WHO&#xA0;(World Health Organization, 2016), http://www.who.int/mediacentre/factsheets/fs402/en/.</div>\n<div>&#xA0;</div>\n<div>[24]&#xA0;This problem is not just restricted to the developing world, e.g.&#xA0;&#x201C;Doctor Arrested For Illegal Distribution Of More Than Ten Thousand Oxycodone Pills, Resulting In One Known Death | USAO-SDNY | Department of Justice,&#x201D; accessed July 17, 2017,</div>\n<div>https://www.justice.gov/usao-sdny/pr/doctor-arrested-illegal-distribution-more-ten-thousand-oxycodone-pills-resulting-one.</div>\n<div>&#xA0;</div>\n<div>[25]&#xA0;Engineering, and Medicine National Academies of Sciences,&#xA0;The Health Effects of Cannabis and Cannabinoids&#xA0;(Washington, D.C.: National Academies Press, 2017), doi:10.17226/24625.</div>\n<div>&#xA0;</div>\n<div>[26]&#xA0;E C KAST and V J COLLINS, &#x201C;STUDY OF LYSERGIC ACID DIETHYLAMIDE AS AN ANALGESIC AGENT.,&#x201D;&#xA0;Anesthesia and Analgesia&#xA0;43: 285&#x2013;91, accessed July 17, 2017, http://www.ncbi.nlm.nih.gov/pubmed/14169837.</div>\n<div>&#xA0;</div>\n<div>[27]&#xA0;Roni Jacobson, &#x201C;A One-Dose Psychedelic Fix for Addiction?,&#x201D;&#xA0;Scientific American Mind&#xA0;28, no. 1 (December 8, 2016): 10&#x2013;11, doi:10.1038/scientificamericanmind0117-10.</div>\n<div>&#xA0;</div>\n<div>[28]<a href=\"https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/bulletins/deathsrelatedtodrugpoisoninginenglandandwales/2015-09-03#deaths-from-all-drug-poisonings\">https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/bulletins/deathsrelatedtodrugpoisoninginenglandandwales/2015-09-03#deaths-from-all-drug-poisonings</a></div>\n<div>&#xA0;</div>\n<div>[29]&#xA0;Glenn Greenwald, &#x201C;Drug Decriminalization in Portugal: Lessons for Creating Fair and Successful Drug Policies | Cato Institute,&#x201D; 2009, https://www.cato.org/publications/white-paper/drug-decriminalization-portugal-lessons-creating-fair-successful-drug-policies.</div>\n<div>&#xA0;</div>\n<div>[30]&#xA0;Economist, &#x201C;Treating, Not Punishing | The Economist,&#x201D;&#xA0;The Economist, 2009, http://www.economist.com/node/14309861.</div>\n<div>&#xA0;</div>\n<div>[31]&#xA0;Ibid.</div>\n<div>&#xA0;</div>\n<div>[32]&#xA0;Teri S Krebs and P&#xE5;l-&#xD8;rjan Johansen, &#x201C;Lysergic Acid Diethylamide (LSD) for Alcoholism: Meta-Analysis of Randomized Controlled Trials.,&#x201D;&#xA0;Journal of Psychopharmacology (Oxford, England)&#xA0;26, no. 7 (July 2012): 994&#x2013;1002, doi:10.1177/0269881112439253.</div>\n<div>&#xA0;</div>\n<div>[33]&#xA0;Matthew W Johnson et al., &#x201C;Pilot Study of the 5-HT2AR Agonist Psilocybin in the Treatment of Tobacco Addiction.,&#x201D;&#xA0;Journal of Psychopharmacology (Oxford, England)&#xA0;28, no. 11 (November 2014): 983&#x2013;92, doi:10.1177/0269881114548296.</div>\n<div>&#xA0;</div>\n<div>[34]&#xA0;Albert Garcia-Romeu, Roland R Griffiths, and Matthew W Johnson, &#x201C;Psilocybin-Occasioned Mystical Experiences in the Treatment of Tobacco Addiction.,&#x201D;&#xA0;Current Drug Abuse Reviews, 2014, doi:10.2174/1874473708666150107121331.&#xA0;I thank Aaron Nesmith-Beck for informing me of this study.</div>\n<div>&#xA0;</div>\n<div>[35]&#xA0;David Nutt, &#x201C;Perverse Effects of the Precautionary Principle: How Banning Mephedrone Has Unexpected Implications for Pharmaceutical Discovery.,&#x201D;&#xA0;Therapeutic Advances in Psychopharmacology&#xA0;1, no. 2 (April 2011): 35&#x2013;36, doi:10.1177/2045125311406958.</div>\n<div>&#xA0;</div>\n<div>[36]&#xA0;S Bird, &#x201C;Mephedron and Cocaine: Clues from Army Testing,&#x201D;&#xA0;Straight Statistics [Online], 2011, http://www.straightstatistics.org/article/mephedrone-and-cocaine-clues-army-testing.</div>\n<div>&#xA0;</div>\n<div>[37]&#xA0;Nutt, King, and Nichols, &#x201C;Effects of Schedule I Drug Laws on Neuroscience Research and Treatment Innovation.&#x201D;</div>\n<div>&#xA0;</div>\n<div>[38]&#xA0;Nutt et al., &#x201C;Development of a Rational Scale to Assess the Harm of Drugs of Potential Misuse.&#x201D;; Nutt et al., &#x201C;Drug Harms in the UK: A Multicriteria Decision \\Analysis.&#x201D;</div>\n<div>&#xA0;</div>\n<div>[39]&#xA0;Source:&#xA0;<a href=\"https://www.economist.com/blogs/dailychart/2010/11/drugs_cause_most_harm\">https://www.economist.com/blogs/dailychart/2010/11/drugs_cause_most_harm</a></div>\n<div>&#xA0;</div>\n<div>[40]&#xA0;&#x201C;Global Commission on Drugs Policy,&#x201D; 2014, http://www.gcdpsummary2014.com/.</div>\n<div>&#xA0;</div>\n<div>[41]&#xA0;Ibid.</div>\n<div>&#xA0;</div>\n<div>[42]&#xA0;Ibid.</div>\n<div>&#xA0;</div>\n<div>[43]&#xA0;Ibid.</div>\n<div>&#xA0;</div>\n<div>[44]&#xA0;J. F. Rischard,&#xA0;High Noon?: Twenty Global Problems, Twenty Years to Solve Them&#xA0;(Basic Books, 2002).</div>\n<div>&#xA0;</div>\n<div>[45]&#xA0;From&#xA0;<a href=\"https://en.wikipedia.org/wiki/Drug-related_crime\">https://en.wikipedia.org/wiki/Drug-related_crime</a></div>\n<div>&#xA0;</div>\n<div>[46]&#xA0;I thank Eli Nathan for this story.</div>\n<div>&#xA0;</div>\n<div>[47]&#xA0;R Allen, &#x201C;Global Prison Trends 2015. Penal Reform International,&#x201D; 2014, http://www.unodc.org/documents/ungass2016//Contributions/Civil/PenalReform/Drugs_and_imprisonment_PRI_submission_UNGASS.pdf.. Further, globally, more women are imprisoned for drug offences than for any other crime. One in four women in prison across Europe and Central Asia are incarcerated for drug offences, while in many Latin American countries such as Argentina (68.2 per cent), Costa Rica (70 per cent) and Peru (66.38 per cent) the rates are higher still.&#xA0;</div>\n<div>&#xA0;</div>\n<div>[48]&#xA0;K Marsh, &#x201C;The Real Cost of Prison | Opinion | The Guardian,&#x201D;&#xA0;The Guardian, 2008, https://www.theguardian.com/commentisfree/2008/jul/28/justice.prisonsandprobation.</div>\n<div>&#xA0;</div>\n<div>[49]&#xA0;C Snowden, &#x201C;Alcohol and the Public Purse: Do Drinkers Pay Their Way? &#x2013; Institute of Economic Affairs,&#x201D; 2015, https://iea.org.uk/publications/research/alcohol-and-the-public-purse-do-drinkers-pay-their-way.</div>\n<div>&#xA0;</div>\n<div>[50]&#xA0;Arguably, this principle is dangerous and should itself be banned.</div>\n<div>&#xA0;</div>\n<div>[51]&#xA0;I note this principle is also self-refuting. Someone who wanted to impose it would need to show, in advance, the principle increase happiness overall, which he couldn&#x2019;t do until she&#x2019;s imposed it and collected some results.</div>\n<div>&#xA0;</div>\n<div>[52]&#xA0;Indeed, Nutt argues &#x2018;equasy&#x2019; (horse-riding) is a dangerous addiction.&#xA0;D J Nutt, &#x201C;Equasy-- an Overlooked Addiction with Implications for the Current Debate on Drug Harms.,&#x201D;&#xA0;Journal of Psychopharmacology (Oxford, England)&#xA0;23, no. 1 (January 2009): 3&#x2013;5, doi:10.1177/0269881108099672.</div>\n<div>&#xA0;</div>\n<div>[53]&#xA0;R. R. Griffiths et al., &#x201C;Psilocybin Can Occasion Mystical-Type Experiences Having Substantial and Sustained Personal Meaning and Spiritual Significance,&#x201D;&#xA0;Psychopharmacology&#xA0;187, no. 3 (August 7, 2006): 268&#x2013;83, doi:10.1007/s00213-006-0457-5.</div>\n<div>&#xA0;</div>\n<div>[54]&#xA0;D Parfit, &#x201C;Equality and Priority,&#x201D;&#xA0;Ratio, 1997.</div>\n<div>&#xA0;</div>\n<div>[55]&#xA0;T Scanlon,&#xA0;What We Owe to Each Other, 1998.</div>\n<div>&#xA0;</div>\n<div>[56]&#xA0;Toby Ord, &#x201C;Why I Am Not a Negative Utilitarian,&#x201D; 2013, http://www.amirrorclear.net/academic/ideas/negative-utilitarianism/.</div>\n<div>&#xA0;</div>\n<div>[57]&#xA0;Matthias Forstmann and Christina Sagioglou, &#x201C;Lifetime Experience with (Classic) Psychedelics Predicts pro-Environmental Behavior through an Increase in Nature Relatedness,&#x201D;&#xA0;Journal of Psychopharmacology, June 20, 2017, 26988111771404, doi:10.1177/0269881117714049.Matthew M. Nour, Lisa Evans, and Robin L. Carhart-Harris, &#x201C;Psychedelics, Personality and Political Perspectives,&#x201D;&#xA0;Journal of Psychoactive Drugs, April 26, 2017, 1&#x2013;10, doi:10.1080/02791072.2017.1312643.</div>\n<div>&#xA0;</div>\n<div>[58]&#xA0;I note Gabriel discusses whether effective altruism will satisfy advocates of rights, justice and priority in&#xA0;&#x201C;Effective Altruism and Its Critics,&#x201D;&#xA0;Journal of Applied Philosophy, 2016.&#xA0;I haven&#x2019;t tried to show DPR is the best cause area if you value those things, only that DPR doesn&#x2019;t appear bad by such lights.</div>\n<div>&#xA0;</div>\n<div>[59]&#xA0;Larry Temkin, &#x201C;Equality, Priority, and the Levelling down Objection,&#x201D;&#xA0;The Ideal of Equality, 2000, 126&#x2013;61.</div>\n<div>&#xA0;</div>\n<div>[60]&#xA0;I thank Sam Hilton for this point (personal correspondence).</div></body></html>", "user": {"username": "MichaelPlant"}}, {"_id": "NkJv2F4zfrnnjTEtF", "title": "Introducing Improving autonomy ", "postedAt": "2017-08-10T08:00:04.207Z", "htmlBody": "<html><body><p>Following on from an attempt to define a <a href=\"/ea/1cm/autonomy_a_search_for_a_measure/\">measure for autonomy</a>, I&apos;ve decided to try and build a community around <a href=\"/ea/170/ea_should_invest_more_in_exploration/\">exploring</a> the idea of improving autonomy for humanity.</p>\n<p>I&apos;ve created a <a href=\"https://improvingautonomy.wordpress.com/\">main site</a>&#xA0;and a <a href=\"https://www.reddit.com/r/improvingautonomy/\">subreddit</a>&#xA0;for discussions.</p>\n<p>&#xA0;</p>\n<p>My plans are to write more on the ideas of intelligence augmentation and how it might be approached&#xA0;to minimise the risks associated with it. Then I will continue to work on the <a href=\"https://github.com/eb4890/agorint/\">market-based resource allocation in computers</a>&#xA0;and blog about it to see if it is appropriate for intelligence augmentation.</p>\n<p>If you think it important that humans in the future have more autonomy please get in touch.</p></body></html>", "user": {"username": "WillPearson"}}, {"_id": "wu9nEXWtvhEnYQTxG", "title": "High Time For Drug Policy Reform. Part 1/4: Introduction and Cause Summary", "postedAt": "2017-08-09T13:17:20.012Z", "htmlBody": "<p>In the last 4 months, I\u2019ve come to believe drug policy reform, changing the laws on currently illegal psychoactive substances, may offer a substantial, if not the most substantial, opportunity to increase the happiness of humans alive today. I consider this result very surprising. I\u2019ve been researching how best to improve happiness (i.e. increase happiness whilst people are alive) for nearly 2 years. When I argued effective altruism (\u2018EA\u2019) was\u00a0<a href=\"/ea/yv/is_effective_altruism_overlooking_human_happiness/\">overlooking mental health and happiness</a>\u00a0a year ago, drug reform featured nowhere in my analysis, nor had I heard anyone (inside or outside EA) suggest it was seriously important. This is the first in a series of four posts for the EA forum where I examine drug policy reform (\u2018DPR\u2019) and explain why I think it should be taken seriously. I don\u2019t claim to know all the empirical facts, nor do I claim to have a fully considered set of suggestions for what to do next. I wanted to share the problem with others as I\u2019ve reached the limits of my knowledge and expertise on the topic and hope other brains can help.[1]</p>\n<p>I want to thank Lee Sharkey for his assistance. We started collaborating on this before work commitments dragged him elsewhere. He wrote section 2.2 (on pain) and I wrote the rest with him providing helpful comments. Many of the ideas are his.[2]\u00a0Because the entire analysis came to over 15,000 words, which seems unreasonably long, even for EA standards, we decided to split the essay into four parts I\u2019ll upload on consecutive days [edit: have uploaded over consecutive days]. The motivation for this unusual move is to make the topic more readable and to keep the discussion of the different parts of the problem into separate places.</p>\n<p>Here are the lengths and contents of the four parts:-</p>\n<p>Part 1 (1,800 words): introduction and cause summary.</p>\n<p><a href=\"/ea/1df/high_time_for_drug_policy_reform_introduction_and/\">Part 2</a>\u00a0(8,000 words): I make the case for DPR, explain why it might do a lot of good and anticipate objections.</p>\n<p><a href=\"/ea/1de/high_time_for_drug_policy_reform_policy/\">Part 3</a> (3,000 words): I\u2019ll discuss how drug policy could and should be reformed and take a stab and its tractability and neglectedness.</p>\n<p><a href=\"/ea/1dj/high_time_for_drug_policy_reform_part_44/\">Part 4</a> (3,500 words): I provide some simplistic but illustrative cost-effectiveness estimates comparing an imaginary campaign for DPR against current interventions for poverty, physical health and mental health; I also consider what EAs should do next.</p>\n<p>[Edit 10/08/2017: this forum post originally contained what I above call parts 1 &amp; 2 because Lee and I thought we should\u00a0divide\u00a0this\u00a0topic into three posts. A commentator below suggested this was still too big and we should split this four ways; Lee and I agreed, so we've moved part 2 into a another forum post.\u00a0This may make the comments from before 10/08/2017 slightly confusing, sorry.]</p>\n<p>What follows is a summary of DPR as a cause area.</p>\n<p><strong>1. Summary</strong></p>\n<p>All around the world, governments restrict access to psychoactive drugs, drugs that change how you think or feel, because they deem them unsafe to the public. These are the sorts of things your parents probably told you to stay away from when you were growing up: LSD, magic mushrooms, cocaine, cannabis, ecstasy and so on. While a drug is anything that has a physiological effect on the body, by \u2018drugs\u2019 I am more narrowly referring to those that are both psychoactive and illegal. This rules out things like antibiotics (not psychoactive) and alcohol and caffeine (not typically illegal). Legality may change from country to country,\u00a0though all countries nominally subscribe to the international scheduling and control of certain drugs under UN treaties.\u00a0A full list of the drugs controlled by the UK government can be found\u00a0<a href=\"https://www.gov.uk/government/publications/controlled-drugs-list--2/list-of-most-commonly-encountered-drugs-currently-controlled-under-the-misuse-of-drugs-legislation\">here</a>.\u00a0</p>\n<p>Drug prohibition causes two sets of effects, each of I\u2019ll claim causes unnecessary misery. The first comes from the fact these bans are (at least somewhat) successful in restricting access to drugs. They make it harder for people to use drugs for medical or recreational purposes or to conduct research into their impact.</p>\n<p>The second surround the efforts required to enforce these bans. Governments worldwide have been a fighting a \u2018War or Drugs\u2019 since the 1960s, punishing those who try to create, transport, distribute or use drugs, often by putting them in prison, sometimes by killing them.</p>\n<p>I think we can identify six separate pathways drug reform, either by changing what\u2019s banned, or how those bans are enforced, could improve happiness. I\u2019ll list these briefly and explain them in more detail in the next section, provide numbers and citations.\u00a0</p>\n<ol>\n<li>Fighting mental illness. Many currently illegal drugs, such as LSD and magic mushrooms, have shown great promise for treating mental health conditions in recent small-scale trials. The ability to conduct research or provide these to patients is greatly restricted by drug policies.</li>\n<li>Reducing pain. Many people in physical pain (and often dying from terminal illnesses) lack access to effective pain relief, such as morphine, again because governments restrict access. This is primarily a problem in the developing, rather than developed, world.</li>\n<li>Improving public health. Perversely, prohibition may cause more harm by pushing people towards more harmful drugs. Where addicts fear punishment, they don\u2019t seek help.</li>\n<li>Reducing crime, violence, corruption and instability. The drug trade can destabilise entire drug-producing countries (e.g. Afghanistan, Columbia). In drug-consuming countries it creates a demand for criminal gangs to distribute drugs, turns recreational users into criminals, leads individual addicts to engage in petty crime (e.g. theft) to fund their habit and takes up the time and focus of police, courts, prisons and politicians.</li>\n<li>Raising revenue for governments. Governments around the world spend billions (unsuccessfully) fighting the war on drugs, both at home and abroad. If they stopped, they would save money. If they legalised (some) drugs and taxed them, governments would have much more money to spend on other policies.</li>\n<li>Recreational use. Many people unproblematically consume alcohol, a legal recreational drug, and think it improves their enjoyment of life. It\u2019s possible (although uncertain) other drugs could increase happiness too.</li>\n</ol>\n<p>As we can see, there are several different ways drug reform might do considerable good. Those who wish to claim we should maintain the status quo approach have a considerable task. It\u2019s not sufficient to show one or two of above avenues fail. What\u2019s needed is to show DPR is, on balance, bad. I\u2019ll explore each of the six avenues in the next section and suggest the case for DPR seems strongly positive overall.</p>\n<p>For those who think DPR is promising, there is still a very open question about what the best policies would be. There are a range of options here and the legal situation is slightly complicated, so I won\u2019t explain this in any depth until part 2 in the series. For those what want some concrete suggestions to chew on, my current, tentative view is we should:</p>\n<ul>\n<li>change the medical classification of several drugs, such as LSD, MDMA (\u2018ecstasy\u2019), psilocybin (\u2018magic mushrooms\u2019), so it\u2019s much easier to conduct research on their effects and use them in treatment of mental illnesses.</li>\n<li>improve access to morphine worldwide so it can be used to treat pain.</li>\n<li>decriminalise, but not necessarily depenalise, all drugs and offering addiction treatment, rather than prison cells, to users.</li>\n<li>legalise the less dangerous common recreational drugs, perhaps cannabis and ecstasy, and treating them like alcohol and tobacco; putting a minimum age limit on them and taxing them heavily.</li>\n</ul>\n<p>In terms of neglectedness and tractability, I think this is (highly) neglected, both within effective altruism and the world as a whole. In contrast, tractability is up for debate. What\u2019s ultimately required is changing the law, which requires changing public opinion. It\u2019s not clear how much public opinion would need to change, what the best ways are to change it, or much this would cost (assuming money does help). Attitudes towards drugs are changing: many states in the USA are legalising weed, and there seems to be growing agreement the War on Drugs is a failure. Once we add to that the sudden de-stigmatisation of mental health and the realisation certain drugs, such as LSD, may offer new, cheaper, more effective treatments for mental health, we can suddenly see a few reasons to believe drugs policy reform is tractable now whilst it wasn\u2019t before. There are still open questions on whether the best thing to do would be to fund a public advocacy group (e.g. the ACLU), a drug policy reform group (e.g. the Beckley Foundation), start a new EA organisation or fund clinical trials testing the safety and efficacy of drugs, or something else. I don\u2019t have a firm view on this yet.</p>\n<p>The final question is whether DPR does more good than other interventions EAs might support. I assume DPR would be of most interest to EAs who focus on benefiting humans alive today and therefore support poverty and physical health interventions (e.g. Give Directly, SCI and AMF). I produce some cost-effectiveness numbers which indicate funding a campaign for drug policy reform in the UK, if it costs less than \u00a310bn, would be more cost-effective than giving money to AMF. This is on the assumption roughly $9,000 to AMF caused 45 years of happy life to be lived. Roughly then, if you think we could successful pull off a DPR campaign in the UK for less than \u00a310bn (which, for what it\u2019s worth, I do), you should think DPR is a more cost-effective than AMF. I stress this requires several assumptions I won\u2019t discuss here but set out in part 3. Those with different assumptions will reach alternative conclusions and I encourage others to run their own numbers.</p>\n<p>I\u2019m uncertain whether those who currently believe X-risk or animal welfare are the top causes should believe DPR would do more good (in expectation) than their present priorities. This is because I don\u2019t know how many times more cost-effective such people believe their causes are than near-term, human-focused alternatives. Hence, I\u2019m unsure how many more times effective I would need to show DPR is than current options, such as AMF, before, say animal welfare advocates, would switch from animal welfare to DPR. I don\u2019t touch on how DPR might help animals, but I do identify some ways DPR could benefit humanity over the long run in addition to how it should increase the happiness of humans alive today. In reality, I don\u2019t expect to convert people who aren\u2019t already focused on near-term human interventions, but I would invite such people to offer me some simplistic estimates explaining how much more effective their top causes are to improve the debate.</p>\n<p>Links to the\u00a0articles in\u00a0this series:</p>\n<p><a href=\"/ea/1d8/dpr/\">Part 1\u00a0</a>(1,800 words): Introduction and Summary.</p>\n<p><a href=\"/ea/1df/high_time_for_drug_policy_reform_introduction_and/\">Part 2</a>\u00a0(8,000 words): Six Ways DPR Could Do Good And Anticipating The Objections</p>\n<p><a href=\"/ea/1de/high_time_for_drug_policy_reform_policy/\">Part 3</a>\u00a0(3,000 words): Policy Suggestions, Tractability and Neglectedess.</p>\n<p><a href=\"/ea/1dj/high_time_for_drug_policy_reform_part_44/\">Part 4</a>\u00a0(3,500 words): Estimating Cost-Effectiveness vs Other Causes; What EA Should Do Next.</p>\n<hr />\n<div>[1]\u00a0I\u2019d like to thank to Sam Hilton, Eli Nathan, Konrad Seifert and Aaron Nesmith-Beck for their substantial written feedback and I\u2019d like to thank Haydn Belfield and a contributor who wishes to remain anonymous for their comments on the argument overall.</div>\n<div>\u00a0</div>\n<div>[2]\u00a0All the bad ones, obviously.</div>", "user": {"username": "MichaelPlant"}}, {"_id": "Tqr8urzr87SeMvzY3", "title": "Effective Altruism as a Market in Moral Goods \u2013 Introduction", "postedAt": "2017-08-06T02:29:28.683Z", "htmlBody": "<html><body><p><em><span>This is post 1 of a 5-part series, where I tentatively apply the market and network concepts in the table below (links to definitions/examples) to hopefully gain a better applied understanding of how the EA community operates. I welcome your feedback in the comments section throughout.</span></em></p>\n<p>&#xA0;</p>\n<div>\n<table><colgroup><col><col><col><col></colgroup>\n<tbody>\n<tr>\n<td>\n<p><span>1)</span></p>\n</td>\n<td colspan=\"3\">\n<p><span>Introduction</span></p>\n</td>\n</tr>\n<tr>\n<td rowspan=\"5\">\n<p><span>2)</span></p>\n</td>\n<td>\n<p><a href=\"https://en.wikipedia.org/wiki/Market_(economics)\"><span>Markets</span></a><span> &#xA0;</span></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"/ea/ky/introducing_moral_economics/\"><span>Markets in Moral Goods</span></a></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism/#the-community\"><span>EA Market in Moral Goods</span><span><br></span></a><span> &#xA0;&#xA0;&#xA0;(</span><a href=\"https://www.effectivealtruism.org/articles/introduction-to-effective-altruism/#the-community\"><span>EA community</span></a><span>)</span></p>\n</td>\n</tr>\n<tr>\n<td colspan=\"3\">\n<p><a href=\"https://en.wikipedia.org/wiki/Agent_(economics)\"><span>Agents</span></a></p>\n</td>\n</tr>\n<tr>\n<td colspan=\"3\">\n<p><a href=\"https://en.wikipedia.org/wiki/Capital_(economics)\"><span>Capital</span></a></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"https://en.wikipedia.org/wiki/Value_theory#Economics\"><span>Goods</span></a></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"https://en.wikipedia.org/wiki/Value_theory#Ethics_and_axiology\"><span>Moral goods</span></a></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"https://concepts.effectivealtruism.org/concepts/accounts-of-well-being/\"><span>&#x2019;Impact&#x2019; goods</span></a></p>\n</td>\n</tr>\n<tr>\n<td colspan=\"3\">\n<p><a href=\"https://www.merriam-webster.com/dictionary/exchange\"><span>Exchange</span></a></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>3) </span></p>\n</td>\n<td>\n<p><a href=\"http://www.investopedia.com/terms/v/value-network.asp\"><span>Networks</span></a></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"http://essay.utwente.nl/61302/1/Horstink_Tim_-s_0125792_scriptie.pdf\"><span>Moral networks</span></a></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"/ea/1c8/testing_an_ea_networkbuilding_strategy_in_the/\"><span>EA networks</span></a></p>\n</td>\n</tr>\n<tr>\n<td rowspan=\"3\">\n<p><span>4)</span></p>\n</td>\n<td>\n<p><a href=\"http://dictionary.cambridge.org/dictionary/english/consumer-preference\"><span>Preferences</span></a></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"http://econfaculty.gmu.edu/pboettke/workshop/archives/spring07/EconMoralPref.pdf\"><span>Moral preferences</span></a></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"https://concepts.effectivealtruism.org/concepts/accounts-of-well-being/\"><span>Consequentialist preferences</span></a></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"https://plato.stanford.edu/entries/preferences/#PreCri\"><span>Improving intrinsic</span><span><br></span><span>preferences</span></a></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"http://webspace.ship.edu/cgboer/genpsymoraldev.html\"><span>Moral development</span></a><span> &#xA0;</span><span><br></span></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"https://wiki.lesswrong.com/wiki/Metaethics_sequence\"><span>Rationality-guided<br> (meta-)ethics</span></a></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><a href=\"https://en.wikipedia.org/wiki/Consumer_education\"><span>Improving revealed</span></a></p>\n<p><a href=\"https://en.wikipedia.org/wiki/Consumer_education\"><span>preferences</span></a></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"https://en.wikipedia.org/wiki/Applied_ethics\"><span>Applied ethics</span></a></p>\n</td>\n<td>\n<p><span>&#x2794; </span><a href=\"https://concepts.effectivealtruism.org/concepts/improving-the-accuracy-of-beliefs/\"><span>Improving the accuracy of <br> beliefs</span></a></p>\n</td>\n</tr>\n<tr>\n<td>\n<p><span>5)</span></p>\n</td>\n<td colspan=\"3\">\n<p><a href=\"http://www.econlib.org/library/Enc/DivisionofLabor.html\"><span>Division of labour</span></a></p>\n</td>\n</tr>\n</tbody>\n</table>\n</div>\n<p><strong>&#xA0;</strong></p>\n<h2><span>1. Introduction</span></h2>\n<p><a href=\"https://en.wikipedia.org/wiki/Free_market\"><span>Free markets</span></a><span> are social systems that create powerful forces towards </span><a href=\"http://slatestarcodex.com/2013/12/08/a-something-sort-of-like-left-libertarianism-ist-manifesto/]\"><span>decentralised productivity</span></a><span> (I&#x2019;ll also cover downsides in my next post). Likewise, </span><a href=\"https://en.wikipedia.org/wiki/Social_network_analysis\"><span>social network analysis</span></a><span> is a powerful lens through which to analyse </span><a href=\"http://barabasi.com/networksciencebook/\"><span>community interactions</span></a><span> (see book chapter 9).</span></p>\n<p>&#xA0;</p>\n<p><span>In the coming weeks, I will tentatively apply market and network concepts in the hope of gaining a better understanding of how the EA community works and how its collective impact can be more reliably improved. At the end of each post, I will speculate on its implications and the possible actions that flow from those.</span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p>\n<p><span><span><span>Credits</span><span>: I want to thank Matthijs Maas, Dora Zupka and Victor Sint Nicolaas, Chris van Merwijk and Max Dalton for their valuable feedback and contributions so far, making me look less of an amateur. </span></span></span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "remmelt"}}, {"_id": "tBH9txe2TLhH2acGo", "title": "Medical research: cancer is hugely overfunded; here's what to choose instead", "postedAt": "2017-08-05T15:41:06.692Z", "htmlBody": "<html><body><p><span>&#xA0;</span></p>\n<p><em><span>Note: the below was posted to my <a href=\"http://thinkingaboutcharity.blogspot.co.uk/2017/08/donating-to-medical-research-heres-why.html\">blog</a></span><span><span>; I expect&#xA0;formatting etc will be rendered better there. Also some important appendices and footnotes are available there, but not here. Apologies to international readers for the UK slant on this article, although I suspect that a similar story would apply in other developed&#xA0;countries.</span></span></em></p>\n<p><span>-----------------------------------------------------------------------</span></p>\n<p><span>If you&apos;re going to donate to medical research, don&apos;t donate to cancer -- there are better choices. Cancer accounts for 43% of the research spend, but only 9% of the disease burden, and you can see in the chart below that it&apos;s an outlier.</span><br><br><span>Having said this, it may still be a better choice than some other (non-medical-research) charities.</span><br><br><br></p>\n<div><a href=\"https://2.bp.blogspot.com/-MXOYCHRlbdA/WXx4wkrF54I/AAAAAAAAF2w/CV6uY5yoQUMzJX_lD7DhCEHBUFfvEPl1wCLcBGAs/s1600/Global%2Bdisease%2Bburden%2Bv%2BUK%2Bresearch%2Bspend.PNG\"><img src=\"https://2.bp.blogspot.com/-MXOYCHRlbdA/WXx4wkrF54I/AAAAAAAAF2w/CV6uY5yoQUMzJX_lD7DhCEHBUFfvEPl1wCLcBGAs/s640/Global%2Bdisease%2Bburden%2Bv%2BUK%2Bresearch%2Bspend.PNG\"></a></div>\n<p><br><br><span>This chart seems to lead to the conclusion that you should pick other areas of research to fund, rather than cancer, on the grounds that it&apos;s already a crowded funding area. Better areas appear to be&#xA0;</span><strong>reproductive health &amp; childbirth</strong><span>&#xA0;and&#xA0;</span><strong>infectious disease</strong><span>. If you want to see a table of the data from this chart, together with the figures that show the gap between the disease burden and the research spend (this gap being proportional to the distance of the point from the red line on the above chart) check out the footnote</span><a href=\"https://www.blogger.com/blogger.g?blogID=1644983150805984491#1\"><sup>1</sup></a><span>. (reminder -- for footnotes, see my <a href=\"http://thinkingaboutcharity.blogspot.co.uk/2017/08/donating-to-medical-research-heres-why.html\">blog</a>)</span><br><br></p>\n<hr>\n<p><br><strong>Reasons why you might disagree with this conclusion</strong><br><br><strong>1. Tractability -&#xA0;scale and neglectedness aren&apos;t the only things that matter</strong><br><strong>2. You may disagree with the DALY weights</strong><br><strong>3. The research spend figures are only showing part of the picture</strong><br><strong>4. Some categories may be being researched, but aren&apos;t captured in these stats properly</strong><br><strong>5. This chart uses global disease burden -- you may care about the local disease burden</strong><br><strong>6. These categories are too crude</strong><br><br><br><strong>1. Tractability - scale and neglectedness aren&apos;t the only things that matter</strong><br><span>The chart above may seem to suggest that the scale of the problem is the only factor to consider. This is not the case. We may choose to fund something because we think that more progress will be made if we fund that area; in this scenario, we may say that this is a more tractable (or &quot;easy&quot;) area of research. In general tractability is good, because funding something with a higher chance of success is better than funding something with a lower chance of success, all other things being equal.</span><br><br><span>However, we know that the funds *aren&apos;t* being allocated to cancer as opposed to other areas because donors have taken this into account and consider cancer to be more tractable.</span><br><br><span>So should we expect better tractability, or better progress, from well-researched areas (such as cancer) or less-well-researched areas (such as mental health)?</span></p>\n<div>&#xA0;</div>\n<p><em>Some examples to illustrate what I mean:</em><br><em>- Cancer: Cancer Research UK talk about the current cancer survival rate having reached 50%, and their aim to reach 75% by 2034; my reading of their language is that they are optimistic that cancer (an area which already has a track record of progress) is a good tractable area to fund.</em><br><em>- Mental Health: MQ (a mental health research research charity) seem to consider funding under-funded research to be an opportunity; as far as I can tell, they have made no explicit claims that (for example) mental health may have lower hanging research fruit, so to speak. However implicit in their language is the belief that tractability for mental health is at least as good as that of cancer.</em><br><br><span>My -- admittedly cursory -- conversations with researchers suggest that there are arguments both ways as the above examples suggest. Sometimes an under-explored area can yield low-hanging fruits; sometimes an area being under-explored means that researchers haven&apos;t quite worked out the best research questions to ask yet, which might mean that research in that area is leading nowhere.</span><br><br><span>To my mind, both arguments seem reasonable, so I&apos;m inclined to assume that we can treat tractability as roughly equal across health categories. If anyone can give a better, and evidence-based, indication of how to treat this, I would love to take this into account.</span><br><br><strong>2. You may disagree with the DALY weights</strong><br><span>Determining the disease burden involves using a measure commonly used by the WHO called the&#xA0;</span><a href=\"http://www.who.int/healthinfo/global_burden_disease/metrics_daly/en/\">DALY</a><span>, which counts how many years have been lost from people dying together with the number of years spent in disability. DALYs generally assume that being disabled is better than being dead, but how much better depends on the disability; a weighting factor is assigned to each type of disability</span><a href=\"https://www.blogger.com/blogger.g?blogID=1644983150805984491#2\"><sup>2</sup></a><span>. However getting these right is hard, and you may disagree with the WHO on this.</span><br><br><strong>3. The research spend figures are only showing part of the picture</strong><br><span>It&apos;s true that the figures focus just on UK spend, and also ignore government expenditure. It could be that, for example, that UK charitable cancer research spend is really high to make up for a lack of government spend on cancer research, or a lack of spend by other developed nations. I suspect that this isn&apos;t true, however I haven&apos;t done the digging to check.</span><br><br><span>The reason I suspect that this isn&apos;t true is (to echo an earlier comment): we know that the funds *aren&apos;t* being allocated to cancer as opposed to other areas because donors have taken this into account and are making up for a lack of government or international research spend.</span><br><br><strong>4. Some categories may be being researched, but aren&apos;t captured in these stats properly</strong><br><span>For example, 2.7% of the global morbidity burden comes from road injuries. It could be that research is happening on road injuries, but (perhaps) it&apos;s not captured in the statistics because the source of the data (the Association of Medical Research Charities, or AMRC) used data that didn&apos;t treat this as medical research.</span><br><br><span>Overall, I don&apos;t think this is likely to be a concern, except in &quot;Other&quot;, which in any case is a fairly unhelpful category.</span><br><br><strong>5. This chart uses global disease burden -- you may care about the local disease burden</strong><br><span>Interestingly, I had assumed that changing the global disease burden to the UK-specific disease burden would account for the apparent over-allocation to cancer, however it appears that this only goes part of the way towards doing this -- there is still a material over-allocation to cancer based on the UK prevalence of cancer. I&apos;ve created a copy of the above chart, but replaced the global disease burden with the UK-specific disease burden -- you can find the chart below</span><a href=\"https://www.blogger.com/blogger.g?blogID=1644983150805984491#3\"><sup>3</sup></a><span>.&#xA0;</span><span>(reminder -- for footnotes, see my&#xA0;</span><a href=\"http://thinkingaboutcharity.blogspot.co.uk/2017/08/donating-to-medical-research-heres-why.html\">blog</a><span>)</span><br><br><strong>6. These categories are too crude</strong><br><span>To a certain extent, this is a fair criticism -- for example, lumping together several different metabolic and endocrine diseases leaves open some of the uncertainties that arise from lumping things together; e.g. maybe the proportion spent on diabetes research is (perhaps) higher than the proportion of disability burden attributable to diabetes, but we aren&apos;t seeing it because the disease burden for non-diabetes conditions within the metabolic/endocrine category is also quite high. (I haven&apos;t checked whether this is the case.) As a later piece of work, I may see whether it&apos;s possible break those categories down further.</span><br><br><span>In particular, &quot;Other&quot; is a notably crude and unhelpful category; it is likely that it includes some sub-categories which are more neglected than reproductive health or infectious disease, so seeing this breakdown could be useful.</span><br><br><span>However I don&apos;t think it detracts from the conclusion about cancer -- it&apos;s still true that the UK allocates 43% of its research funding to cancer, whereas the only 9% of the global disability burden is for cancer.</span><br><br><strong><em>A minor note...</em></strong><br><span>In case you&apos;re feeling like you&apos;ve seen something like this before... after the ice bucket challenge there was an infographic doing the rounds that compared how much money research charities got with how much they kill people. I would suggest that this chart is in fact much better than that infographic -- if you would like to know more, I&apos;ve expanded on this in a footnote.</span><sup><a href=\"https://www.blogger.com/blogger.g?blogID=1644983150805984491#4\">4</a>&#xA0;</sup><span>(reminder -- for footnotes, see my&#xA0;</span><a href=\"http://thinkingaboutcharity.blogspot.co.uk/2017/08/donating-to-medical-research-heres-why.html\">blog</a><span>)</span></p>\n<p><br><span>And lastly, apologies to international readers for the UK focus of this piece. I suspect that its conclusions would still apply in most other countries in the developed world, but have not checked this.</span><br><br></p>\n<p><span>For sources, appendices, and footnotes, please see <a href=\"http://thinkingaboutcharity.blogspot.co.uk/2017/08/donating-to-medical-research-heres-why.html\">http://thinkingaboutcharity.blogspot.co.uk/2017/08/donating-to-medical-research-heres-why.html</a>.</span></p>\n<p>&#xA0;</p>\n<p>&#xA0;</p></body></html>", "user": {"username": "Sanjay"}}, {"_id": "vbbWtTBpzjxwZowaF", "title": "How We Banned Fur in Berkeley", "postedAt": "2017-08-05T14:53:47.331Z", "htmlBody": "<p><i>Cross-posted from </i><a href=\"https://medium.com/@jayquigley/how-we-banned-fur-in-berkeley-8bcf19284efd\"><i>Medium</i></a><i>.</i></p><p>Late last month, the City Council of Berkeley, California banned the sale of new fur apparel items within city limits. The ban is the second of its kind in the United States, following a 2011 ban in West Hollywood, California.&nbsp;</p><p>The group responsible for the ban is&nbsp;<a href=\"https://web.archive.org/web/20170519214825/http://www.berkeleycoalition.org/\">Berkeley Coalition for Animals</a>&nbsp;(BCA), an all-volunteer legislative advocacy group of which I am secretary.</p><p>Many animal rights advocates have asked how a small all-volunteer group with zero funding achieved success with the&nbsp;<a href=\"https://web.archive.org/web/20180827022737/https://furfreeberkeley.com/\">Fur Free Berkeley</a>&nbsp;campaign. This case study can help provide a template for those wishing to sponsor animal rights or other altruistic legislation in their jurisdictions.</p><h2>tl;dr:&nbsp;</h2><p>Key components were</p><ul><li>cultivating relationships with sympathetic council members we thought would be sympathetic to animal rights legislation, and</li><li>utilizing a proven template for the bill.</li></ul><h2>Background</h2><ul><li>Fur-bearing animals suffer horrendously.&nbsp;You can learn about these atrocities, and appreciate animals\u2019 perspectives on the conditions they are forced to live in, by visiting the webpages of<a href=\"https://web.archive.org/web/20180827022737/https://furfreeberkeley.com/\">&nbsp;Fur Free Berkeley</a>&nbsp;and&nbsp;<a href=\"https://www.peta.org/issues/animals-used-for-clothing/fur/\">PETA</a>, among others.</li><li>Very few stores in Berkeley&nbsp;sell fur apparel of any sort. This meant that we faced little opposition\u200a\u2014\u200aa very different situation than that in West Hollywood, which had numerous high-price venders of new fur apparel.</li><li>West Hollywood, California passed a&nbsp;<a href=\"https://www.animallaw.info/local/ca-west-hollywood-chapter-948-animal-control-regulations#s020\">ban</a>&nbsp;on&nbsp;the sale of new fur apparel in 2011 (effective 2013). Since WeHo\u2019s law had both been written with careful scrutiny and successfully challenged in court, we needed not agonize over language nor recruit a legal team to finalize or defend the bill.</li><li>The&nbsp;<a href=\"http://www.seraphonline.com/furfreeweho/\">Fur Free WeHo</a>&nbsp;campaign&nbsp;was a many-months-long effort and very contentious, due to the plethora of fur vendors there, according to West Hollywood Campaigner Ed Buck. The ban became a campaign item for John D\u2019Amico\u2019s successful 2011 bid for City Council. (More details on the vote&nbsp;<a href=\"https://web.archive.org/web/20181212145827/https://lcanimal.org/index.php/campaigns/fur/fur-free-weho\">here</a>.) The West Hollywood law was successfully&nbsp;<a href=\"http://www.fashionapparellawblog.com/2014/07/articles/changes-in-law/fur-flies-and-west-hollywood-weho-fur-ban-is-upheld-by-federal-court/\">defended in federal court</a>&nbsp;in 2014.</li><li>Similar campaigns&nbsp;are ongoing in other places, including&nbsp;<a href=\"http://www.furfreela.com/\">Los Angeles</a>&nbsp;and&nbsp;<a href=\"http://www.haaretz.com/israel-news/.premium-1.769135?v=583063C6430F9C1606C45BC2B8BB5716\">Israel</a>. Several&nbsp;<a href=\"http://www.furfreealliance.com/fur-bans/\">European</a>&nbsp;countries including the UK, Austria, the Netherlands, Croatia, Slovenia, Bosnia and Herzegovina, Serbia, and Macedonia have banned fur production.</li><li>The Berkeley bill&nbsp;tracked the West Hollywood law almost verbatim. However, during the first hearing, City Council members made two changes, one that strengthened the ban and one that marginally limited its application. First, whereas in West Hollywood&nbsp;nonprofits&nbsp;are exempted from the ban, in Berkeley they are not. Second, Berkeley council members insisted on an exempting&nbsp;sheep and cow&nbsp;furs.&nbsp;<i>We encourage would-be followers to use the West Hollywood definitions</i>.</li><li>The sheep fleece and cowhide exemptions&nbsp;in Berkeley were squeezed into the definitions provided in its bill, complicating the original definition, which came from the US Federal Trade Commission\u2019s<a href=\"https://www.ftc.gov/node/119458\">&nbsp;Fur Products Labeling Act</a>. This was a last-minute surprise pushed in by Councilmember Sophie Hahn. At nearly midnight during the bill\u2019s first reading, enough of the councilmembers were persuaded and/or untroubled by her exceptions to vote for her amended version of the bill. Failed attempts by both BCA and sponsoring Councilmember Worthington to negotiate these exceptions away gave way to the bill\u2019s being eventually moved to the consent calendar just before the Council\u2019s summer recess. We decided to move on to larger battles.&nbsp;<i>The lesson: don\u2019t take representatives\u2019 votes for granted, even if their support seems like a sure bet.</i></li><li>(Hahn introduced the exceptions for basically two reasons. First, she held that cows and sheep were already being farmed for reasons other than their hides, making sale of their skins acceptable. Our dissenting view is that two forms of exploitation don\u2019t make a right; moreover, the sale of skin-and-hair as a byproduct still does plausibly increase overall demand. Second, Hahn maintained that sheep fleece is one of a few \u201cpure\u201d materials to which certain people, including babies, would not be allergic, and that it should stay legal for that reason. We dispute both this claim\u2019s factuality and its relevance.)</li><li>Public support&nbsp;for the bill was substantial. Our Change.org&nbsp;<a href=\"https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.change.org%2Fp%2Fberkeley-city-council-ban-fur-in-berkeley&amp;h=ATOvXNh3RhPtTDm56vZTQ4ulxlMkry5Ci-OxCQULzx7IwbEVe1dVrMbrNWGO1ag_fj2yn-NU0kg-dmVcup3fdSxUQlzzOQdmvQ1XdBVFbmYIHcrT_b5O8RXphptzzL3jlwqaY_B2KRchfq9UqXXo9q0JcJkIt7yD\">petition</a>&nbsp;garnered over 5,000 signatures from around the world, and at least 60 people from around the Bay Area wrote to the city council in favor of the fur ban, thanks to alerts from PETA\u2019s mailing list. One councilmember&nbsp;<a href=\"https://l.facebook.com/l.php?u=http%3A%2F%2Fwww.dailycal.org%2F2017%2F03%2F14%2Fknow-council-cheryl-davila-wants-advance-civil-rights-berkeley%2F&amp;h=ATOvXNh3RhPtTDm56vZTQ4ulxlMkry5Ci-OxCQULzx7IwbEVe1dVrMbrNWGO1ag_fj2yn-NU0kg-dmVcup3fdSxUQlzzOQdmvQ1XdBVFbmYIHcrT_b5O8RXphptzzL3jlwqaY_B2KRchfq9UqXXo9q0JcJkIt7yD\">mentioned</a>&nbsp;being overwhelmed by the number of emails she got.</li><li>Berkeley\u2019s ban passed its&nbsp;<a href=\"https://www.cityofberkeley.info/Clerk/City_Council/2017/03_Mar/Documents/2017-03-28_Item_24_Ordinance_to_Ban_the_Sale.aspx\">first reading</a>&nbsp;on March 28, 2017, and became <a href=\"http://www.codepublishing.com/CA/Berkeley/cgi/NewSmartCompile.pl?path=Berkeley09/Berkeley0922/Berkeley0922020.html\">law</a> after the&nbsp;<a href=\"https://www.cityofberkeley.info/Clerk/City_Council/2017/07_Jul/Documents/2017-07-25_Item_46_Ordinance_to_Ban.aspx\">second reading</a>&nbsp;on July 25.</li></ul><h3>Key Strategies</h3><ul><li>Building relationships&nbsp;with key councilmembers and then-candidates was the most important factor in both introducing and passing the law. Through the personal relationships we cultivated with Councilmembers Worthington, Harrison, Bartlett, etc., we not only brought the issue to their attention but demonstrated that thoughtful, connected residents of their city and districts were concerned about making progress for animals.</li><li>Berkeley&nbsp;is an above-average place to attempt pathbreaking legislation for several reasons. First, because there a political history and climate of trend-setting movements and laws (from the Free Speech and Anti-Vietnam War movements to the first curbside recycling program to 2014\u2019s soda tax). Second, it among the very most left-leaning cities in&nbsp;<a href=\"http://www.dailycal.org/2014/07/02/berkeley-voted-liberal-city-california/\">California</a>&nbsp;and the&nbsp;<a href=\"https://thetab.com/us/uc-berkeley/2016/11/29/berkeley-voted-78-clinton-14-trump-2780\">USA</a>. Third, the 2016 election saw a major gain for progressives on Berkeley\u2019s City Council, with progressives gaining 6 of 9 seats including the mayorship. It helped that our impression was they would vote as a bloc (which turned out to be only partially true for this bill).</li></ul><h3>Advice</h3><p>What should you do if you want to pass similar animal protection legislation in another jurisdiction in California, a different U.S. state, or elsewhere?</p><ul><li>Develop relationships with local representatives.&nbsp;Representatives tend to be responsive to their community members\u2019 concerns. Dialogue is key. As your concerned constituent, they want to hear your agenda. They also have an agenda of their own; take interest in it. Sign up for their mailing lists; take note of when they have office hours and show up at community events; call their offices and ask for appointments for you to present animal protection legislation.</li><li>Seek out a solid team.&nbsp;You don\u2019t need to be a superhero to work on these things. Anyone passionate about justice for animals who can reasonably read and communicate can develop the needed allies and skills. That said, it can be helpful for you or those you know to have or develop certain further skills. Legal and/or legislative know-how is helpful for knowing what an adequate bill should look like, as well as for understanding the process of how a city council meeting works. Internet skills can also be helpful for mobilizing people and communicating about animals\u2019 perspectives. Think email lists, social media, and websites (e.g. content management systems such as Squarespace or WordPress). Finally, the habit of showing gratitude to others sustains a campaign through thick and thin.</li><li>Get solid advice.&nbsp;The Berkeley Coalition for Animals (info@berkeleycoalition.org) stands by ready to help with ideas and support for similar initiatives. You can also&nbsp;<a href=\"http://www.seraphonline.com/furfreeweho/contact.php\">contact</a>&nbsp;Fur Free WeHo for their advice, which would be especially relevant if you\u2019re working in a large metro area or one with plenty of fur retailers.</li><li>Build coalitions.&nbsp;The more contentious your issue\u200a\u2014\u200athe more real opposition you face\u200a\u2014\u200athe more important it is to have key allies in your local community. Think of local groups, especially ones influential in local politics, as well as notable individuals in your community (professors, entrepreneurs, lawyers, minsters, etc., etc.). Beyond your local community, international animal advocacy organizations will be ready and willing to help; see Fur Free Berkeley\u2019s list of&nbsp;<a href=\"https://furfreeberkeley.com/#supporters\">supporters</a>&nbsp;for ideas.</li><li>Consider economic impact assessment.&nbsp;If there are dedicated fur boutiques in your community, lawmakers will be interested in the tax ramifications of their being unable to sell fur apparel. Attaining an economic impact assessment could be necessary in that case. Economic consulting firms near you may be able to help. They may do&nbsp;<i>pro bono&nbsp;</i>work for nonprofits at a reduced or free rate; in any event, it is worth taking into account early as for whether and how much you need to fundraise. If you do need funding, asking advice from organizations far and wide should be a priority; often these kinds of efforts are more achievable than they first seem.</li><li>Stay focused on farmed animals\u2019 perspectives.&nbsp;Ultimately we\u2019re pushing for a world in which no animal is used or abused for any purpose\u200a\u2014\u200aclothing, entertainment, food, or anything. Focusing on the&nbsp;<a href=\"https://furfreeberkeley.com/\">millions</a>&nbsp;and&nbsp;<a href=\"http://www.humanesociety.org/news/resources/research/stats_slaughter_totals.html\">billions</a>&nbsp;of animals who need their voices amplified can keep you motivated to persevere.</li></ul>", "user": {"username": "jayquigley"}}, {"_id": "jqCCM3NvrtCYK3uaB", "title": "Blood Donation: (Generally) Not That Effective on the Margin", "postedAt": "2017-08-05T03:56:12.114Z", "htmlBody": "<html><body><p>[Note: This originally came out of a&#xA0;<a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1485445588178460/\">post</a>&#xA0;on the Facebook group, which was then reversed in light of Gregory Lewis&apos;s expert&#xA0;<a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1485445588178460/?comment_id=1486210914768594&amp;comment_tracking=%7B%22tn%22%3A%22R0%22%7D\">comments</a>. The substance of this article&#xA0;is his; any errors are mine.]</p>\n<p>If no one donated blood, a lot of trauma/hemorrhage victims would die, and the world would be a lot worse off. The average unit of blood donated goes pretty far, in terms of the expected value of the good it does. However, when considering whether or not to donate, we need to evaluate the counterfactual difference our actions make. That is, rather than looking at the average donation, the relevant measurement is&#xA0;of the&#xA0;<em>marginal</em>&#xA0;donation. This brings up the following considerations:</p>\n<p>1) As it stands, a substantial number of people already donate regularly, and will continue to do so whether or not the (comparatively tiny) EA community does&#xA0;too.</p>\n<p>2)&#xA0;Truly life-or-death situations are a minority of transfusions, and these&#xA0;are pretty much already covered by the existing supply. In fact, hospitals almost always keep an emergency reserve of O- specifically for these cases, so it&apos;s&#xA0;<em>very</em>&#xA0;rare that someone directly&#xA0;dies for lack of compatible blood. Because a large number of transfusions/donations happen each day, and blood product can often be transported to different hospitals to meet local shortages, projected supplies are relatively easy to forecast within a given margin of error, so it is possible for hospitals to maintain this emergency supply to handle urgent cases.</p>\n<p>Thus, the effect of an&#xA0;<em>additional</em>&#xA0;donation to the existing supply is to help cases where the patient wouldn&apos;t be directly saved from death, but a transfusion&#xA0;would improve the quality of their recovery. Nailing down exactly how many QALYs this typically adds is very difficult to track, and probably hasn&apos;t been done in a rigorous way. However, there is&#xA0;reason to believe this number is not that high.</p>\n<p>In the UK, a unit of red blood cells (RBCs) costs about 120 pounds. While financial incentives don&apos;t translate seamlessly into extra donations, this is&#xA0;roughly this price at which more supply can be obtained, so it roughly reflects the medical field&apos;s impression of how valuable it would be to do more outreach per unit. Furthermore, the typical cutoff for whether to fund treatment is ~20,000 pounds/QALY, which is much less efficient than the ~130 pounds/QALY one can get by donating to the AMF. (For more detail on these numbers, see&#xA0;<a href=\"https://www.getguesstimate.com/models/9301\">this guesstimate</a>&#xA0;and&#xA0;<a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1485445588178460/?comment_id=1486210914768594&amp;reply_comment_id=1486386644751021&amp;comment_tracking=%7B%22tn%22%3A%22R0%22%7D\">this explanation</a>&#xA0;of it.)</p>\n<p>Thus, for blood donation to be anywhere near as effective as the AMF (in terms of paying 120 pounds/unit for more product), the medical field would have to be undervaluing the effectiveness of blood donations by 2 orders of magnitude. Despite the lack of rigorous calculations done in the literature, a collective miscalculation of this magnitude seems implausible given the feedback mechanisms which exist in medicine, not to mention&#xA0;the tacit knowledge hematologists have developed from making these tradeoffs.</p>\n<p>The role of effective altruism is to look for, and seize upon, moral opportunities that have been unfairly passed over by society at large. GiveWell-recommended charities, for instance, may sometimes&#xA0;get positive comments from economists, but receive insufficient funding to fully exploit the ethical gold mine that is their cause area.&#xA0;In the case of blood donations, the medical field generally has ways of spotting and filling in the cheap and obvious ways to save more lives, so our time is better spent on causes that&#xA0;aren&apos;t being watched over as carefully.<br><br>That said, there are occasional cases where emergency supplies dwindle. When this happens, specific appeals are made, and in these cases it&#xA0;probably is effective to lend some helping&#xA0;hemoglobin. Less crucially, regular donors often drop out on holidays and during the winter (due to colds/flu), so if one is inclined to donate, those&#xA0;are the best times to do so.</p></body></html>", "user": {"username": "Grue_Slinky"}}, {"_id": "7AjqtQFDLjnnKgdhM", "title": "Paper Ballots in the 2020 US Election", "postedAt": "2017-08-02T19:50:54.269Z", "htmlBody": "<html><body><p>&#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;</p>\n<p>Here&#x2019;s a cause which may be worthy of EA attention: trying to ensure that all U.S. states must use paper ballots in the 2020 Presidential election.</p>\n<p>In the 2016 election, most voters <a href=\"http://www.pewresearch.org/fact-tank/2016/11/08/on-election-day-most-voters-use-electronic-or-optical-scan-ballots/\">did not use paper ballots</a>. And although it is true that voting machines are not directly connected to the internet, <a href=\"http://www.npr.org/2017/06/14/532824432/if-voting-machines-were-hacked-would-anyone-know\">they are not as secure as you might think</a>. Also, voting machines were <a href=\"https://blog.horner.tj/post/hacking-voting-machines-def-con-25\">compromised within 2 hours at DEF CON 25</a>.</p>\n<p>Furthermore, and in my view most importantly, even the appearance or not-totally-obviously-unfounded allegations of an unfair election can be very dangerous. Particularly when the current president will almost certainly run for reelection conditional on not being impeached, and it is not unfounded to suspect his campaign may resort to intentional attempts to compromise the ballots.</p>\n<p>There are other reasons to think the integrity of the 2020 election will be at risk . Russia will almost certainly be at full force attempting to undermine our democratic process (having succeeded this time), and possibly North Korea as well. Furthermore, if you thought Fake News was an epistemic nightmare last year, imagine <a href=\"http://www.businessinsider.com/ai-video-editing-fake-obama-talking-doctored-footage-2017-7\">when Fake Video technology is perfected</a>. &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;</p>\n<p>&#xA0;</p>\n<p>I believe this cause is tractable but not easy. A few people have created a petition to enforce paper ballots on change.org in the past, without success, which makes me think most people don&#x2019;t think this is important. However, my intuition says this is something that relatively few Americans would actively oppose. Therefore I think a concerted effort to start early spreading the worth is likely worth of the time spent.&#xA0;</p></body></html>", "user": {"username": "purplepeople"}}, {"_id": "4NnvA2sXbB87CqArF", "title": "Reading recommendations for the problem of consequentialist scope?", "postedAt": "2017-08-02T02:07:46.769Z", "htmlBody": "<html><body><p>Determining which&#xA0;scope of outcomes to consider when making a decision seems like a difficult problem for consequentialism. By &quot;scope of outcomes&quot; I mean how far into the future and how many links in the causal chain to incorporate into decision-making. For example, if I&apos;m assessing the comparative goodness of two charities, I&apos;ll need to have some method of comparing&#xA0;future impacts (perhaps &quot;consider impacts that occur in the next 20 years&quot;) and flow-through contemporaneous impacts (perhaps &quot;consider the actions of the charitable recipient, but not the actions of those they interact with&quot;).<br><br>I&apos;m using&#xA0;&quot;consequentialist scope&quot; as a shorthand for this type of determination because I&apos;m not aware of a common-usage word for it.<br><br>Consequentialist scope seems both (a) important and (b) difficult to think about clearly, so I want to learn more about it. <br><br>Does anyone have&#xA0;reading recommendations for this? Philosophy papers, blog posts, books, whatever.&#xA0;I didn&apos;t encounter it in <em>Reasons and Persons</em>, but I&apos;ve only read the first third so far.</p></body></html>", "user": {"username": "Milan_Griffes"}}]