[{"_id": "poDvi54vaFhze4Xvd", "postedAt": "2023-12-10T00:42:00.369Z", "postId": "Xq3ALk5LHkak2cKag", "htmlBody": "<p>Some previous discussion:</p><ul><li><a href=\"https://www.lesswrong.com/posts/A5YQqDEz9QKGAZvn6/agi-is-easier-than-robotaxis\">AGI is easier than robotaxis</a></li><li><a href=\"https://www.lesswrong.com/posts/Mtmq827AE7PCdHzef/why-don-t-we-have-self-driving-cars-yet\">Why don't we have self-driving cars yet?</a></li></ul>", "parentCommentId": null, "user": {"username": "Ryan Greenblatt"}}, {"_id": "ZusWub5oaLhZ4dAtP", "postedAt": "2023-12-10T01:22:40.623Z", "postId": "Xq3ALk5LHkak2cKag", "htmlBody": "<p>This kinda overlaps with (2), but the end of 2035 is 12 years away. A lot can happen in 12 years! If we look back to 12 years ago, it was December 2011. AlexNet had not come out yet, neural nets were a backwater within AI, a neural network with 10 layers and 60M parameters was considered groundbreakingly deep and massive, the idea of using GPUs in AI was revolutionary, tensorflow was still years away, doing even very simple image classification tasks would continue to be treated as a funny joke for several more years (literally\u2014<a href=\"https://www.explainxkcd.com/wiki/index.php/1425:_Tasks\">this comic</a> is from 2014!), I don\u2019t think anyone was dreaming of AI that could pass a 2nd-grade science quiz or draw a recognizable picture without handholding, GANs had not been invented, nor transformers, nor deep RL, etc. etc., I think.</p><p>So \u201cAGI by 2035\u201d isn\u2019t like \u201cwow that could only happen if we\u2019re already almost there\u201d, instead it leaves tons of time for like a whole different subfield of AI to develop from almost nothing.&nbsp;</p><p>(I'm making a case against being confidently skeptical about AGI by 2035, not a case for confidently expecting AGI by 2035.)</p>", "parentCommentId": null, "user": {"username": "steve2152"}}, {"_id": "bAyjaundkZ9SoHnAX", "postedAt": "2023-12-10T03:11:14.280Z", "postId": "Xq3ALk5LHkak2cKag", "htmlBody": "<p>Great question!</p><p>My understanding was that self-driving cars are already less likely to get into accidents than humans are.</p><p>However, they certainly can't \"drive autonomously wherever and whenever a typical human driver could\", requiring a costly process to adapt current self-driving technology to each city one at a time.</p><p>What does this tell us about how far from AGI? In particular, should this make us less enthusiastic about the generative AI direction than we might otherwise be? If it's so powerful, shouldn't we be able to use it to solve self-driving?</p><p>I guess it doesn't feel to me that we should make a huge update on this because anyone who is at all familiar with generative AI should already know it is incredibly unreliable without having to bring self-driving cars into the equation.</p><p>The question then becomes how insurmountable the unreliability problem is. There are certainly challenges here, but it's not clear that it is insurmountable. The short-timelines scenarios are pretty much always contingent on us discovering some kind of self-reinforcing loops. Is this likely? It's hard to tell, but there are already very basic techniques like self-consistency or reinforcement learning from AI feedback, so it isn't completely implausible. And it's not really clear to me why the lack of self-driving cars at present is a strong reason to believe that attempts to set up such a loop will fail.</p>", "parentCommentId": null, "user": {"username": "casebash"}}, {"_id": "uAz7BAcwW5T4EJL7A", "postedAt": "2023-12-10T20:01:02.828Z", "postId": "Xq3ALk5LHkak2cKag", "htmlBody": "<p>Great comment!</p>\n", "parentCommentId": "ZusWub5oaLhZ4dAtP", "user": {"username": "Yarrow Bouchard"}}]