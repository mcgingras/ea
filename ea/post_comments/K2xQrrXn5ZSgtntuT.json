[{"_id": "oymaETLEBeSBkm6rq", "postedAt": "2023-07-19T09:25:54.135Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>Thanks for this - super interesting! One thing I hadn't caught before is how much the estimates reduce for domain experts in the top quintile for reciprocal scoring - in many cases an order of magnitude lower than that of the main domain expert group!</p>", "parentCommentId": null, "user": {"username": "BenStewart"}}, {"_id": "ZbwGABiYELGEs8D5N", "postedAt": "2023-07-19T09:29:35.467Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>I think the differing estimates between domain experts and superforecasters is very interesting. Two factors I think might contribute to this are selection effects, and anchoring effects.&nbsp;</p><p>For selection effects, we know that 42% of the selected domain experts had attended EA meetups, whereas only 9% of the superforecasters had (page 9 of report). I assume the oversampling of EA members among experts may cause some systematic shift. The same applies to previous surveys of AI experts: for example the <a href=\"https://wiki.aiimpacts.org/doku.php?id=ai_timelines:predictions_of_human-level_ai_timelines:ai_timeline_surveys:2022_expert_survey_on_progress_in_ai\">2022 survey</a> was voluntary had only a 17% response rate. It's possible (I would even say likely) that if you had managed to survey 100% of the experts, the median probability of AI doom would drop, as AI concerned people are probably more likely to answer surveys.&nbsp;</p><p>The other cause &nbsp;could be differing susceptibility to anchoring bias. We know (from page 9) that the general public is extremely susceptible to anchoring here: &nbsp;the median public estimate of x-risk dropped six orders of magnitude from 5% to 1 in 15 million depending on the phrasing of the question (with the difference cause either by the example probabilities presented, or whether they asked in terms of odds instead of percentages).&nbsp;</p><p>If the public is susceptible to anchoring, experts probably will be as well. If you look at the resources given to the survey participants on AI risk (page 132), they gave a list of previous forecasts and expert surveys, which were in turn:</p><p>&nbsp;5%, 1 in 10, 5%, &nbsp;5%, \u201c0 to 10%\u201d, 0.05%, and 5%</p><p>Literally 4 out of the 7 forecast give the <i>exact same number</i>. &nbsp;Are we sure that it\u2019s a coincidence that the final forecast median ends up around this same number?&nbsp;</p><p>My view is that the domain experts are subconsciously afraid to stray too far from their anchor point, whereas the superforecasters are more adept at resisting such biases, and noticing that the estimates come mostly from EA sources, which may have correlated bias on this question. &nbsp;</p><p>Of course, there could be plenty of other reasons as well, but I thought these two were interesting to highlight.&nbsp;</p>", "parentCommentId": null, "user": {"username": "titotal"}}, {"_id": "t5CNwxK8NyZGrXe8C", "postedAt": "2023-07-19T10:35:58.346Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>Thanks for providing the arguments commonly given for and against various cruxes, that's super interesting.</p><p>These two arguments for why extinction would be unlikely</p><blockquote><ul><li>The logistics would be extremely challenging.</li><li>Millions of people live very remotely, and AI would have little incentive to pay the high costs of killing them.</li></ul></blockquote><p>make me wonder what the forecasters would've estimated for <i>existential</i> risk rather than extinction risk (i.e. we lose control over our future / are permanently disempowered, even if not literally everyone dies this century). (Estimates would presumably be somewhere between the ones for catastrophe and extinction.)</p><p>I'm also curious about why the tournament chose to focus on extinction/catastrophe rather than existential risks (especially given that its called the Existential Risk Persuasion Tournament). Maybe those two were easier to operationalize?</p>", "parentCommentId": null, "user": {"username": "Erich_Grunewald"}}, {"_id": "TC6ELRrtikNBTGasJ", "postedAt": "2023-07-19T10:37:37.959Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<blockquote><p>Of course, there could be plenty of other reasons as well, but I thought these two were interesting to highlight.</p></blockquote><p>Yes, for example maybe the AI experts benefited from their expertise on AI.</p>", "parentCommentId": "ZbwGABiYELGEs8D5N", "user": {"username": "Erich_Grunewald"}}, {"_id": "B9sexvf3PyzvWtQ3J", "postedAt": "2023-07-19T11:09:10.721Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>I wasn't around when the XPT questions were being set, but I'd guess that you're right that extinction/catastrophe were chosen because they are easier to operationalise.</p><p>On your question about what forecasts on existential risk would have been: I think this is a great question.</p><p>FRI actually ran a follow-up project after the XPT to dig into the AI results. One of the things we did in this follow-up project was elicit forecasts on a broader range of outcomes, including some approximations of existential risk. I don't think I can share the results yet, but we're aiming to publish them in August!</p>", "parentCommentId": "t5CNwxK8NyZGrXe8C", "user": {"username": "rosehadshar"}}, {"_id": "5WaFenuECuEekoQjh", "postedAt": "2023-07-19T14:12:42.694Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>One could say the CIA benefited from their expertise in geopolitics, and yet the superforecasters still <a href=\"https://en.wikipedia.org/wiki/The_Good_Judgment_Project\">beat them</a>. Superforecasters perform well because they are good at synthesising diverse opinions and expert opinions: they had access to the same survey of AI experts that everyone else did.&nbsp;</p><p>Saying the AI experts had more expertise in AI is obviously true, but it doesn't explain the discrepancy in forecasts. Why were superforecasters <i>unconvinced</i> by the AI experts?&nbsp;</p>", "parentCommentId": "TC6ELRrtikNBTGasJ", "user": {"username": "titotal"}}, {"_id": "dtwYC4mJpRt5Z5NNE", "postedAt": "2023-07-19T16:58:39.169Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>My guess is that the crowds are similar and thus the surveys and the initial forecasts were  also similar.</p>\n<p>Iirc(?) the report states that there wasn't much updating of forecasts, so the final and initial average also are naturally close.</p>\n<p>Besides that, there was also some deference to literature/group averages, and also some participants imitated e.g. the Carlsmith forecast but with their own numbers (I think it was 1/8th of my group, but I'd need to check my notes).</p>\n<p>I kinda speculate that Carlsmith's model may be biased towards producing numbers around ~5% (sth about how making long chains of conditional probabilities doesn't work because humans fail to imagine each step correctly and thus end up biased towards default probabilities closer to 50% at each step).</p>\n", "parentCommentId": "ZbwGABiYELGEs8D5N", "user": {"username": "J\u00f6rn St\u00f6hler"}}, {"_id": "iZ3dWdrvqa9z3jX8H", "postedAt": "2023-07-20T08:49:46.844Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>I think there are a bunch of things that make expertise more valuable in AI forecasting than in geopolitical forecasting:</p><ul><li>We've all grown up with geopolitics and read about it in the news, in books and so on, so most of us non-experts already have passable models of it. That's not true with AI (until maybe one year ago, but even now news don't report on technical safety).</li><li>Geopolitical events have fairly clear reference classes that can give you base rates and so on (and this is a tool available to both experts and non-experts) -- this is much harder with AI. That means the outside view is less valuable for AI forecasting.</li><li>I think AI is really complex and technical, and especially hard given that we're dealing with systems that don't yet exist. Geopolitics is also complex, and geopolitical futures will be different from now, but the basic elements are the same. I think this also favors non-experts when forecasting on geopolitics.</li></ul><p>And quoting <a href=\"https://www.lesswrong.com/posts/YTPtjExcwpii6NikG/existential-risk-persuasion-tournament\">Peter McCluskey</a>, a participating superforecaster:</p><blockquote><p>The initial round of persuasion was likely moderately productive. The persuasion phases dragged on for nearly 3 months. We mostly reached drastically diminishing returns on discussion after a couple of weeks.</p><p>[...]</p><p>The persuasion seemed to be spread too thinly over 59 questions. In hindsight, I would have preferred to focus on core cruxes, such as when AGI would become dangerous if not aligned, and how suddenly AGI would transition from human levels to superhuman levels. That would have required ignoring the vast majority of those 59 questions during the persuasion stages. But the organizers asked us to focus on at least 15 questions that we were each assigned, and encouraged us to spread our attention to even more of the questions.</p><p>[...]</p><p>Many superforecasters suspected that recent progress in AI was the same kind of hype that led to prior disappointments with AI. I didn't find a way to get them to look closely enough to understand why I disagreed.</p><p>My main success in that area was with someone who thought there was a big mystery about how an AI could understand causality. I pointed him to <a href=\"https://bayesianinvestor.com/blog/index.php/2018/07/06/pearls-book-of-why/\">Pearl</a>, which led him to imagine that problem might be solvable. But he likely had other similar cruxes which he didn't get around to describing.</p><p>That left us with large disagreements about whether AI will have a big impact this century.</p><p>I'm guessing that something like half of that was due to a large disagreement about how powerful AI will be this century.</p><p>I find it easy to understand how someone who gets their information about AI from news headlines, or from laymen-oriented academic reports, would see a fair steady pattern of AI being overhyped for 75 years, with it always looking like AI was about 30 years in the future. It's unusual for an industry to quickly switch from decades of overstating progress, to underhyping progress. Yet that's what I'm saying has happened.</p><p>[...]</p><p>That superforecaster trend seems to be clear evidence for AI skepticism. How much should I update on it? I don't know. I didn't see much evidence that either group knew much about the subject that I didn't already know. So maybe most of the updates during the tournament were instances of the blind leading the blind.</p></blockquote><p>Scott Alexander <a href=\"https://astralcodexten.substack.com/p/the-extinction-tournament\">points out</a> that the superforecasters have likely already gotten one question pretty wrong, having a median prediction of the most expensive training run for 2024 of $35M (experts had a median of $65M by 2024) whereas GPT-4 seems to have been ~$60M, though with ample uncertainty. But bearish predictions will tend to fail earlier than bullish predictions, so we'll see how the two groups compare in the next years, I guess.</p>", "parentCommentId": "5WaFenuECuEekoQjh", "user": {"username": "Erich_Grunewald"}}, {"_id": "zR3jwcgnTFidTztHD", "postedAt": "2023-07-20T16:19:18.320Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>By the way, I feel now that my first reply in this thread was needlessly snarky, and am sorry about that.</p>", "parentCommentId": "5WaFenuECuEekoQjh", "user": {"username": "Erich_Grunewald"}}, {"_id": "sE8WyJhGNGjdR992n", "postedAt": "2023-07-20T18:09:18.862Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<blockquote><p>Forecasters stopped updating their forecasts on 31st October 2022</p></blockquote><p>Would be interested to see an update post-chatGPT (esp. GPT-4). I know a lot of people who have reduced their timelines/increased their p(doom) in the last few months.</p>", "parentCommentId": null, "user": {"username": "Greg_Colbourn"}}, {"_id": "cF5QoEhCdDxDFahhi", "postedAt": "2023-07-21T09:35:53.311Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>From <a href=\"https://astralcodexten.substack.com/p/the-extinction-tournament\">Astral Codex Ten</a></p><blockquote><p>FRI called back a few XPT forecasters in May 2023 to see if any of them wanted to change their minds, but they mostly didn\u2019t.</p></blockquote><p><br>&nbsp;</p>", "parentCommentId": "sE8WyJhGNGjdR992n", "user": {"username": "ChanaMessinger"}}, {"_id": "agXzeDah9Kyx8wZjr", "postedAt": "2023-07-21T10:15:36.474Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>Weird. Does this mean they predicted GPT-4's performance in advance (and also didn't let that update them toward doom)!?</p>", "parentCommentId": "cF5QoEhCdDxDFahhi", "user": {"username": "Greg_Colbourn"}}, {"_id": "GEnhvMYs7eLk4y5B5", "postedAt": "2023-07-21T12:29:22.281Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>I think you make good points in favour of the AI expert side of the equation. To balance that out, I want to offer one more point in favour of the superforecasters, in addition to my earlier points about anchoring and selection bias (we don't actually know what the true median of AI expert opinion is or would be if questions were phrased differently).&nbsp;</p><p>The primary point I want to make is that Ai x-risk forecasting is, at least partly, a geopolitical forecast. Extinction from rogue AI requires some form of war or struggle between humanity. You have to estimate the probability that that struggle ends with humanity losing.&nbsp;</p><p>An AI expert is an expert in software development, not in geopolitical threat management. Neither are they experts in potential future weapon technology. If someone has worked on the latest bombshell LLM model, I will take their predictions about specific AI development seriously, but if they tell me an AI will be able to build omnipotent nanomachines that take over the planet in a month, I have no hesitations in telling them they're wrong, because I have more expertise in that realm than they do.&nbsp;</p><p>I think the superforecasters have superior geopolitical knowledge than the AI experts, and that is reflected in these estimates.&nbsp;</p>", "parentCommentId": "iZ3dWdrvqa9z3jX8H", "user": {"username": "titotal"}}, {"_id": "aSrsMNpYhNoYjTPjd", "postedAt": "2023-07-22T11:49:08.318Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<blockquote><p>we're aiming to publish them in August!</p></blockquote><p>&nbsp;</p><p>Please do! And if possible, one small request from me would be if any insight on extinction vs existential risk for AI can be transferred to bio and nuclear - e.g. might there be some general amount of population decline (e.g. 70%) that seems to be able to trigger long-term/permanent civilizational collapse.</p>", "parentCommentId": "B9sexvf3PyzvWtQ3J", "user": {"username": "Ulrik Horn"}}, {"_id": "87xsiJPJmQ7c3wTb9", "postedAt": "2023-07-23T06:15:58.859Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>Here are a couple of excerpts from relevant comments from the <a href=\"https://astralcodexten.substack.com/p/the-extinction-tournament\">Astral Codex Ten post about the tournament</a>. From the anecdotes, it seems as though this tournament had some flaws in execution, namely that the \"superforcasters\" weren't all that. But I want to see more context if anyone has it.</p>\n<p><a href=\"https://astralcodexten.substack.com/p/the-extinction-tournament/comment/21052188\">From Jacob:</a></p>\n<blockquote>\n<p>I  signed up for this tournament (I think? My emails related to a Hybrid Forecasting-Persuasion tournament that at the very least shares many authors), was selected, and partially participated. I found this tournament from it being referenced on ACX and am not an academic, superforecaster, or in any way involved or qualified whatsoever. I got the Stage 1 email on June 15.</p>\n</blockquote>\n<p><a href=\"https://astralcodexten.substack.com/p/the-extinction-tournament/comment/21052708\">From magic9mushroom:</a></p>\n<blockquote>\n<p>I participated and AIUI got counted as a superforecaster, but I'm really not. There was one guy in my group (I don't know what happened in other groups) who said X-risk can't happen unless God decides to end the world. And in general the discourse was barely above \"normal Internet person\" level, and only about a third of us even participated in said discourse. Like I said, haven't read the full paper so there <em>might</em> have been some technique to fix this, but overall I wasn't impressed.</p>\n</blockquote>\n", "parentCommentId": null, "user": {"username": "romanhauksson"}}, {"_id": "smcnGkJmeupMkD6cZ", "postedAt": "2023-07-24T09:32:01.706Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>Perhaps it's less surprising given who counted as 'superforecasters', cf magic9mushroom's comment <a href=\"https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1?commentId=87xsiJPJmQ7c3wTb9\">here</a>? I'm not sure how much their personal anecdote as participant generalizes though.</p>", "parentCommentId": "agXzeDah9Kyx8wZjr", "user": {"username": "Mo Nastri"}}, {"_id": "FkELhMsH5yfJDXjRT", "postedAt": "2023-07-24T10:34:08.787Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>The follow-up project was on AI specifically, so we don't currently have any data that would allow us to transfer directly to bio and nuclear, alas.</p>\n", "parentCommentId": "aSrsMNpYhNoYjTPjd", "user": {"username": "rosehadshar"}}, {"_id": "abpnznBtrwYq5scbD", "postedAt": "2023-07-24T15:18:27.806Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>See <a href=\"https://forum.effectivealtruism.org/posts/EHTynQaSN8ubjCbm9/how-much-is-reducing-catastrophic-and-extinction-risk-worth\">here</a> for a mash up of XPT forecasts on catastrophic and extinction risk, with <a href=\"https://philpapers.org/archive/SHUHMS.pdf\">Shulman and Thornley</a>'s paper on how much governments should pay to prevent catastrophes.</p>", "parentCommentId": null, "user": {"username": "rosehadshar"}}, {"_id": "KvC3SaJntjobxveqC", "postedAt": "2024-03-07T22:33:45.073Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>I'm going through the <a href=\"https://static1.squarespace.com/static/635693acf15a3e2a14a56a4a/t/64f0a7838ccbf43b6b5ee40c/1693493128111/XPT.pdf\">hosted paper</a> (\"Forecasting Existential Risks\") and making some comments in hypothes.is (see <a href=\"https://hypothes.is/users/daaronr?q=URL%3Astatic1.squarespace.com%2F*\">here</a>).&nbsp;<br><br>I first thought I saw something off, but now see that it's because of the difference between &nbsp;&nbsp;<i>total</i> <i>extinction </i>risk vs <i>catastrophic</i> risk. For the latter, the superforecasters are not so different from the domain experts (about 2:1). Perhaps this could be emphasized more.&nbsp;<br><br>Putting this in a 'data notebook/dashboard' presentation could be helpful in seeing these distinctions.</p>", "parentCommentId": null, "user": {"username": "david_reinstein"}}, {"_id": "P5twDxX5viyfgkjzf", "postedAt": "2024-03-13T22:34:48.209Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<p>Replies to those comments mostly concur:</p><blockquote><p>(Replies to Jacob)</p><p>(AdamB) I had almost exactly the same experience.</p><p>(sclmlw) I'm sorry you didn't get into the weeds of the tournament. My experience was that most of the best discussions came at later stages of the tournament. [...]&nbsp;</p><p>(Replies to magic9mushroom)</p><p>(Dogiv) I agree, unfortunately there was a lot of low effort participation, and a shocking number of really dumb answers, like putting the probability that something will happen by 2030 higher than the probability it will happen by 2050. In one memorable case a forecaster was answering the \"number of future humans who will ever live\" and put a number less than 100. I hope these people were filtered out and not included in the final results, but I don't know.</p><p>I also recommend taking a look at <a href=\"https://damienlaird.substack.com/p/post-mortem-2022-hybrid-forecasting\">Damien Laird's post-mortem</a>.</p><p>Damien and I were in the same group and he wrote it up much better than I could.</p><p>FWIW I had AI extinction risk at 22% during the tournament and I would put it significantly higher now (probably in the 30s, though I haven't built an actual model lately). Seeing the tournament results hardly affects my prediction at all. I think a lot of people in the tournament may have anchored on Ord's estimate of 10% and Joe Carlsmith's similar prediction, which were both mentioned in the question documentation, as the \"doomer\" opinion and didn't want to go above it and be even crazier.</p><p>&gt; (Sergio) I don\u2019t think we were on the same team (based on your AI extinction forecast), but I also encountered several instances of low-effort participation and answers which were as baffling as those you mention at the beginning (or worse). One of my resulting impressions was that the selection process for superforecasters had not been very strict.</p></blockquote>", "parentCommentId": "87xsiJPJmQ7c3wTb9", "user": {"username": "dpiepgrass"}}, {"_id": "ff7BLkdEgpaQwq8mS", "postedAt": "2024-03-13T23:12:38.002Z", "postId": "K2xQrrXn5ZSgtntuT", "htmlBody": "<blockquote><p>Superforecasters more than quadruple their extinction risk forecasts by 2100 if conditioned on AGI or TAI by 2070.</p></blockquote><ul><li>The data on this table is strange! Originally Superforecasters' gave 0.38% for extinction by 2100 (though 0.088% for RS top quintile) but on this survey it's 0.225%. Why? Also, somehow the first number has 3 digits of precision while the second number is \"1%\" which is maximally lacking in significant digits (like, if you were rounding off, 0.55% ends up as 1%).</li><li>The implied result is strange! How could participants' AGI timelines possibly be so long? I notice <a href=\"https://forum.effectivealtruism.org/posts/K2xQrrXn5ZSgtntuT/what-do-xpt-forecasts-tell-us-about-ai-risk-1?commentId=87xsiJPJmQ7c3wTb9\">ACX comments</a> may explain this as a poor process of classifying people as \"superforecasters\" and/or \"experts\".</li></ul><p>I'd strongly like to see three other kinds of outcome analyzed in future tournaments, especially in the context of AI:</p><ol><li><strong>Authoritarian takeover</strong>: how likely is it that events in the next few decades weaken the US/EU and/or strengthen China (or another dictatorship), eventually leading to world takeover by dictatorship(s)? How likely is it that AGIs either (i) bestow upon a few people or a single person (*cough*Sam Altman) dictatorial powers or (ii) strengthen the power of existing dictators, either in their own country and/or by enabling territorial and/or soft-power expansion?</li><li><strong>Dystopia</strong>: what's the chance of some kind of AGI-induced hellscape in which life is worse for most people than today, with little chance of improvement? (This may overlap with other outcomes, of course)</li><li><strong>Permanent loss of control</strong>: fully autonomous ASIs (genius-level and smarter) would likely take control of the world, such that humans no longer have influence. If this happens and leads to catastrophe (or utopia, for that matter), then it's arguably more important to estimate when <i>loss of control</i> occurs than when the catastrophe itself occurs (and in general it seems like \"date of the point of no return on the path to X\" is more important than \"date of X\", though the concept is fuzzier). Besides, I am very skeptical of any human's ability to predict what will happen after a loss of control event. I'm inclined to think of such an event almost like an event horizon, which is a second reason that forecasting the event itself is more important than forecasting the eventual outcome.</li></ol>", "parentCommentId": null, "user": {"username": "dpiepgrass"}}]