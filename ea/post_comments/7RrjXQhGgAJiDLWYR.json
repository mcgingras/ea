[{"_id": "tBepWs98snNMeWd7K", "postedAt": "2023-08-10T21:55:42.251Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>On mobile, \"roughly \u2155 -\u2159 of our past spending rate\" doesn't display correctly -- that's one fifth to one sixth for my fellow mobile users.</p>\n<p>[Edit for Forum management: these images displayed as white Xs in a black box on my Android / Samsung S22; I saved a screenshot if helpful. It looks like they have been edited to 1/5 and 1/6 in ordinary characters now.]</p>\n", "parentCommentId": null, "user": {"username": "Jason"}}, {"_id": "jFdnwuTniBbw62mWj", "postedAt": "2023-08-10T22:40:47.610Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>It\u2019s working on mobile for me (iPhone - safari)</p>\n", "parentCommentId": "tBepWs98snNMeWd7K", "user": {"username": "calebp"}}, {"_id": "ddzQvXJJoSTRG8mi5", "postedAt": "2023-08-10T23:16:16.430Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>Thank you! I believe it should be fixed now! (I changed \u2155 to 1/5).</p>", "parentCommentId": "tBepWs98snNMeWd7K", "user": {"username": "Linch"}}, {"_id": "Qw4ipyYHxJLtNJjzy", "postedAt": "2023-08-11T00:40:20.018Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>Would donors/other members of the community find it helpful if I were to repeat this process and write such a post for EAIF? Note that as I do not do grantmaking for EAIF, my attempts at doing the analagous \"modifying and blending grants to form representative fictitious grants\" might be missing some key nuances.</p><p>\"Agree\"-vote if helpful relative to the counterfactual, \"Disagree\" if not helpful; assume my nearest counterfactual is writing some other posts drawn from the same distribution as my past posts or comments, particularly LTFF-related ones.</p>", "parentCommentId": null, "user": {"username": "Linch"}}, {"_id": "mNL5nrkoQxykK53kh", "postedAt": "2023-08-11T01:09:45.706Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<blockquote><p>The PI scores the application from -5 to +5.&nbsp;</p></blockquote><p>Does the zero point have any specific meaning? Specifically, does a negative score convey a belief that the proposal has net-negative EV?</p>", "parentCommentId": null, "user": {"username": "Jason"}}, {"_id": "xSCdrpehfPCpeKfzf", "postedAt": "2023-08-11T01:30:47.556Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>In principle, the zero point is supposed to signify equivalent to burning the money, and negative signifies net-negative EV (neglecting financial cost of the grant). In practice, speaking personally, if I weakly think a grant is a bit net negative, but it's not particularly worrying nor something I feel confident about, I usually give it a score that's well below the funding threshold, but still positive (so that if other grantmakers are more confidently in favor of the grant, they can more likely outvote me here). If I were to <i>confidently</i> believe that a grant was of zero net value, I would give it a vote of zero.</p>", "parentCommentId": "mNL5nrkoQxykK53kh", "user": {"username": "Daniel_Eth"}}, {"_id": "bmqAuYYEfCYjY9Zsd", "postedAt": "2023-08-11T03:02:34.496Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>I personally give a negative value and (when I have low certainty) flag that I'm willing to change/delete my votes if other people feel strongly, so as to not unduly tank the results. I think LTFF briefly experimented with weighted voting in the past but we've moved against it (I forgot why).&nbsp;</p>", "parentCommentId": "xSCdrpehfPCpeKfzf", "user": {"username": "Linch"}}, {"_id": "qv4aWQCgKBh7rqft8", "postedAt": "2023-08-11T07:16:56.241Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>This change also helps with text-to-speach</p>\n", "parentCommentId": "ddzQvXJJoSTRG8mi5", "user": {"username": "JJ Hepburn"}}, {"_id": "KmaJKKX4iLekezxJA", "postedAt": "2023-08-12T03:27:08.280Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>Yay!</p>", "parentCommentId": "qv4aWQCgKBh7rqft8", "user": {"username": "Linch"}}, {"_id": "ScPKdETngu99snimL", "postedAt": "2023-08-13T09:41:22.054Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>I really liked this post, and specifically the framing of \"what will a marginal donation be\" (as opposed to \"what's the best thing we ever did\" or so).&nbsp;</p><p>&nbsp;</p><p>[ramblings from my subjective view point of EA-software]</p><ol><li>It reminds me of how developers consider joining an EA org, and think \"well, seems like all your stuff is already built, no?\". I think writing about marginal things the org wants to build and needs help with - would go a long way for many job posts</li><li>This somewhat updated me towards \"it's a bad idea to fund me, my work isn't as important as all this\" and also towards \"maybe I better do some E2G so you can fund more things like this\"</li></ol>", "parentCommentId": null, "user": {"username": "hibukki"}}, {"_id": "nDKQvB7cFsKgD93HK", "postedAt": "2023-08-13T19:09:26.195Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>I really appreciate your transparency about how you allocate funding! Thank you for this post!</p>\n", "parentCommentId": null, "user": {"username": "vin"}}, {"_id": "AokgxuKAjqNtnnFiX", "postedAt": "2023-08-13T22:08:20.267Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>This is hard to answer without knowing the exact counterfactual. I'd value you going deeper on topics you have the most information on, and my guess is EAIF is not your comparative advantage, but if there isn't a specific other post you're excited about I'd much rather have EAIF than nothing. I thought it might be helpful to give ideas of &nbsp;posts I'd be interested in from you, specifically:</p><ul><li>what do you want to see in the impact or theories of change section? (<a href=\"https://forum.effectivealtruism.org/posts/eLe8YG2AdQ3W9YtJY/grant-applications-and-grand-narratives\">related</a>)</li><li>the practicalities of living off of grants as an independent. do people ask for enough? how bad is it if you ask for too much? how do you structure work to avoid gaps between grants?&nbsp;</li><li>how do you evaluate results from independent researchers?</li><li>how do you evaluate the success of grants for upskilling or exploration?</li><li>how do you evaluate work from other kinds of independent grant recipients (AXRP and Rob Miles's youtube channel come to mind, but probably there are more grants that are even harder to categorize)?&nbsp;</li><li>what do you regret not funding?</li></ul>", "parentCommentId": "Qw4ipyYHxJLtNJjzy", "user": {"username": "Elizabeth"}}, {"_id": "yjJ8DCk8waDJKbbrJ", "postedAt": "2023-08-17T16:49:18.784Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>Thanks for sharing! I confess I had been wondering about moving my donations elsewhere due to lack of knowledge about LTFF's processes, but this and other recent posts will probably imply that I will continue donating to LTFF in the near future.</p><blockquote><p>We are committed to improving the long-term trajectory of civilization, with a particular focus on reducing global catastrophic risks.</p></blockquote><p>Which definition of global catastrophic risks are you considering? I think global catastrophes were originally defined by Nick Bostrom and Milan \u0106irkovi\u0107 <a href=\"https://web.archive.org/web/20161005155815/http://sethbaum.com/ac/2009_Rev-GCR.pdf\">as</a> \"events that cause roughly 10 million deaths or $10 trillion in damages or more\". Maybe it would be better to be explicit about the severity of the events in the website?</p><blockquote><p>Note that this grant [in bio] would be controversial within the fund at a $100k funding bar, as some fund managers and advisers would say we shouldn\u2019t fund any biosecurity grants at that level of funding.</p></blockquote><p>I would be curious to know how you compare grants in different areas. For example, could you share which fraction of grants in each area (e.g. AI, bio, nuclear, or other) are successful? I understand <a href=\"https://funds.effectivealtruism.org/funds/far-future\">you consider</a> AI and bio to be the most pressing areas (emphasis mine):</p><blockquote><p>The Long-Term Future Fund aims to positively influence the long-term trajectory of civilization by making grants that address global catastrophic risks, <strong>especially potential risks from advanced artificial intelligence and pandemics</strong>. In addition, we seek to promote, implement, and advocate for longtermist ideas, and to otherwise increase the likelihood that future generations will flourish.</p></blockquote><p>You also only mentioned grants in AI and bio in the OP. However, even if applications in other areas were as likely as those in AI to be funded, they would still not be (randomly) selected to be in the OP, because applications outside of AI and bio only represent a small fraction of the total.</p>", "parentCommentId": null, "user": {"username": "vascoamaralgrilo"}}, {"_id": "5WcmovizqKt2DG7Sw", "postedAt": "2023-08-17T17:45:11.964Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<blockquote>\n<p>Which definition of global catastrophic risks are you considering? I think global catastrophes were originally defined by Nick Bostrom and Milan \u0106irkovi\u0107 as \"events that cause roughly 10 million deaths or $10 trillion in damages or more\". Maybe it would be better to be explicit about the severity of the events in the website?</p>\n</blockquote>\n<p>I don\u2019t think that as an organisation we have a specific definition in mind. I think it\u2019s still worth saying we are most focussed in reducing global catastrophic risks as opposed to pursuing other goals like instilling caring about future generations as a value in society or economic growth.</p>\n<p>In practice we direct funding towards activities that we think reduce catastrophic risks, but are most focussed on existential risks.</p>\n", "parentCommentId": "yjJ8DCk8waDJKbbrJ", "user": {"username": "calebp"}}, {"_id": "nEvHRzQndnee8o5hk", "postedAt": "2023-08-18T20:48:51.019Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>Thanks for the post. Until now, I used to learn about what LTFF funds by manually reading through its grants database. It's helpful to know what the funding bar looks like and how it would change with additional funding.</p><p>I think increased transparency is helpful because it's valuable for people to have some idea of how likely their applications are to be funded if they're thinking of making major life decisions (e.g. relocating) based on them. More transparency is also valuable for funders who want to know how their money would be used.</p>", "parentCommentId": null, "user": {"username": "Stephen McAleese"}}, {"_id": "ia7wANSuENftxBPTm", "postedAt": "2023-08-24T15:20:06.274Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>Writing such a post for EAIF (even a 5x shorter version) would help me get an idea on what's the bar for a community project to be ~worthwhile, and especially to easily say \"no, this isn't worthwhile\".</p><p>I'm saying this because even this LTFF post <a href=\"https://forum.effectivealtruism.org/posts/7RrjXQhGgAJiDLWYR/?commentId=ScPKdETngu99snimL\">updated</a> my opinion about that.</p>", "parentCommentId": "Qw4ipyYHxJLtNJjzy", "user": {"username": "hibukki"}}, {"_id": "khizLeEGnegKGBeh6", "postedAt": "2023-08-25T13:41:25.646Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>Thanks for the post!</p><p>A related question: Is LTFF more likely to fund a small AI safety research group than to fund individual independent AI Safety researchers?</p><p>So could we see a scenario where, if person A, B or C apply individually for an independent research grant, they might not meet your funding bar. But where, if similarly impressive people with a similarly good research agenda applied as a research group, they would be a more attractive funding opportunity for you?</p>", "parentCommentId": null, "user": {"username": "Alexandra Bos"}}, {"_id": "srcxKcBrXGu7CBu3m", "postedAt": "2023-08-25T23:06:55.453Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>(Giving my own professional opinion, not speaking for anybody else/employers) This seems unlikely to me, unless there's a different substantive reason to believe that the research group is better for either research qua research or upskilling. Eg having access to better mentors, or demonstrated evidence that the group is better at keeping each other on track.&nbsp;</p><p>Plausibly I'm wrong here. Being an independent researcher kinda sucks in a variety of ways, and I can imagine having a group to work with to be good even if you can't point to a specific reason. But I don't currently think we have a bias towards groups and against independent researchers, and if anything I'd guess our revealed preferences are a bit in the other direction.&nbsp;</p>", "parentCommentId": "khizLeEGnegKGBeh6", "user": {"username": "Linch"}}, {"_id": "T4DKH82KHXAtG6FFs", "postedAt": "2023-08-28T17:34:46.769Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>I'm curating this. Along with other commenters, I really like the focus on the marginal grant. If I were to write a post that would help donors understand the impact of their donations to the Long Term Future Fund, it would look a lot like this.&nbsp;</p><p>While I'm sympathetic to the reasoning, I was sad to hear that EA Funds would stop sharing publicly all its grants. &nbsp;To my mind to this post goes a long way towards remedying that, and makes me much more likely to recommend the Long Term Future Fund to others. (That strikes me as a surprisingly large update, but I stand by it.)</p><p>Thanks a bunch for writing this!</p>", "parentCommentId": null, "user": {"username": "jpaddison"}}, {"_id": "57A3LTeBFjWCSD2kQ", "postedAt": "2023-08-28T23:18:44.358Z", "postId": "7RrjXQhGgAJiDLWYR", "htmlBody": "<p>Thanks for curating it :)</p>\n", "parentCommentId": "T4DKH82KHXAtG6FFs", "user": {"username": "calebp"}}]